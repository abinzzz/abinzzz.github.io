
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        BLIP-2 x STA |
        
        blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   本文目的 BLIP-2的思想：对于多模态大模型，同时训练Image Encoder和LLM的计算成本实在是太大。该文提出了一种新的思路，使用Q-former这个模块来对齐Image Encoder和LLM之间的gap。">
<meta property="og:type" content="article">
<meta property="og:title" content="BLIP-2 x STA">
<meta property="og:url" content="https://abinzzz.github.io/2024/03/08/BLIP-2-x-STA/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   本文目的 BLIP-2的思想：对于多模态大模型，同时训练Image Encoder和LLM的计算成本实在是太大。该文提出了一种新的思路，使用Q-former这个模块来对齐Image Encoder和LLM之间的gap。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/GIHizQpaYAAYalO?format=jpg&amp;name=large">
<meta property="og:image" content="https://pbs.twimg.com/media/GIHlyKobMAAzvJ6?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GJf0gokXYAAiQas?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GJf1BBbXEAAZ-W7?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GJf5tJYXIAAKfRQ?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GJf6Q0BXcAEtiQI?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/GJf6aIBWYAATpKZ?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GJf6e9VWgAA74jX?format=png&amp;name=900x900">
<meta property="article:published_time" content="2024-03-08T03:09:39.000Z">
<meta property="article:modified_time" content="2024-03-28T10:36:01.878Z">
<meta property="article:author" content="Jerome">
<meta property="article:tag" content="internship">
<meta property="article:tag" content="blip2">
<meta property="article:tag" content="sta">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/GIHizQpaYAAYalO?format=jpg&amp;name=large">
    
        <link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://pbs.twimg.com/media/GHVZM28XUAEbhJW?format=jpg&amp;name=medium" data-sizes="auto" alt="BLIP-2 x STA" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>BLIP-2 x STA</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-BLIP-2-x-STA" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2024/03/08/BLIP-2-x-STA/" class="article-date-link">
        <time datetime="2024-03-08T03:09:39.000Z"
              itemprop="datePublished">2024-03-08</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/internship/">internship</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h2 id="本文目的"><a class="markdownIt-Anchor" href="#本文目的"></a> 本文目的</h2>
<p><strong>BLIP-2的思想</strong>：对于多模态大模型，同时训练Image Encoder和LLM的计算成本实在是太大。该文提出了一种新的思路，使用Q-former这个模块来对齐Image Encoder和LLM之间的gap。<br />
<strong>STA的思想</strong>：尽管现有的ANN2SNN转换方法适用于卷积网络，新兴的Transformer模型引入了自注意力和测试时标准化等独特机制，导致当前SNNs难以实现的非因果非线性交互。文章提出了一种无需训练的ANN到SNN的转换方法，该方法通过时空近似（STA）将ANN激活转换为时间尖峰序列，几乎保留了源模型的所有功能。<br />
<strong>本文思想</strong>：BLIP-2多模态对齐非常好用但是它是是现在ANN上面的，我们现在要使用STA的思想将BLIP-2的Image Encoder和LLM部分有ANN表示转换为SNN表示的</p>
<h2 id="blip-2"><a class="markdownIt-Anchor" href="#blip-2"></a> BLIP-2</h2>
<p><img src="https://pbs.twimg.com/media/GIHizQpaYAAYalO?format=jpg&amp;name=large" alt="" /><br />
<img src="https://pbs.twimg.com/media/GIHlyKobMAAzvJ6?format=jpg&amp;name=medium" alt="" /></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── Qformer.py</span><br><span class="line">├── __init__.py</span><br><span class="line">├── __pycache__</span><br><span class="line">├── blip2.py</span><br><span class="line">├── blip2_image_text_matching.py</span><br><span class="line">├── blip2_opt.py</span><br><span class="line">├── blip2_qformer.py</span><br><span class="line">├── blip2_t5.py</span><br><span class="line">├── blip2_t5_instruct.py</span><br><span class="line">├── blip2_vicuna_instruct.py</span><br><span class="line">├── modeling_llama.py</span><br><span class="line">├── modeling_opt.py</span><br><span class="line">└── modeling_t5.py</span><br></pre></td></tr></table></figure>
<p>由于我只需要改LLM部分，那么涉及到的应该是opt和t5</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br></pre></td><td class="code"><pre><span class="line">OPTForCausalLM(</span><br><span class="line">  (model): OPTModel(</span><br><span class="line">    (decoder): OPTDecoder(</span><br><span class="line">      (embed_tokens): Embedding(<span class="number">50272</span>, <span class="number">2560</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">      (embed_positions): OPTLearnedPositionalEmbedding(<span class="number">2050</span>, <span class="number">2560</span>)</span><br><span class="line">      (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (layers): ModuleList(</span><br><span class="line">        (<span class="number">0</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">2</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">3</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">4</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">5</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">6</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">7</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">8</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">9</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">10</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">11</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">12</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">13</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">14</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">15</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">16</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">17</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">18</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">19</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">20</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">21</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">22</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">23</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">24</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">25</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">26</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">27</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">28</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">29</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">30</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">31</span>): OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (lm_head): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">50272</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">VisionTransformer(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">768</span>, kernel_size=(<span class="number">32</span>, <span class="number">32</span>), stride=(<span class="number">32</span>, <span class="number">32</span>), bias=<span class="literal">False</span>)</span><br><span class="line">  (ln_pre): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">  (transformer): Transformer(</span><br><span class="line">    (resblocks): Sequential(</span><br><span class="line">      (<span class="number">0</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">1</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">2</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">3</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">4</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">5</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">6</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">7</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">8</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">9</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">10</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (<span class="number">11</span>): ResidualAttentionBlock(</span><br><span class="line">        (attn): MultiheadAttention(</span><br><span class="line">          (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (mlp): Sequential(</span><br><span class="line">          (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (gelu): QuickGELU()</span><br><span class="line">          (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (ln_post): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<br>
<p>OPTDecoder:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(model): OPTModel(</span><br><span class="line">  (decoder): OPTDecoder(</span><br><span class="line">    (embed_tokens): Embedding(<span class="number">50272</span>, <span class="number">2560</span>, padding_idx=<span class="number">1</span>)</span><br><span class="line">    (embed_positions): OPTLearnedPositionalEmbedding(<span class="number">2050</span>, <span class="number">2560</span>)</span><br><span class="line">    (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): OPTDecoderLayer(</span><br><span class="line">        (self_attn): OPTAttention(</span><br><span class="line">          (k_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (v_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (q_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          (out_proj): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        (activation_fn): ReLU()</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">        (fc1): Linear(in_features=<span class="number">2560</span>, out_features=<span class="number">10240</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (fc2): Linear(in_features=<span class="number">10240</span>, out_features=<span class="number">2560</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (final_layer_norm): LayerNorm((<span class="number">2560</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>
<ul>
<li>linear</li>
<li>ReLU</li>
</ul>
<br>
<p>vit:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">(transformer): Transformer(</span><br><span class="line">  (resblocks): Sequential(</span><br><span class="line">    (<span class="number">0</span>): ResidualAttentionBlock(</span><br><span class="line">      (attn): MultiheadAttention(</span><br><span class="line">        (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Sequential(</span><br><span class="line">        (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (gelu): QuickGELU()</span><br><span class="line">        (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<ul>
<li>Linear</li>
<li>LN</li>
<li>GELU</li>
</ul>
<p>gnn:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(transformer): Transformer(</span><br><span class="line">  (resblocks): Sequential(</span><br><span class="line">    (<span class="number">0</span>): ResidualAttentionBlock(</span><br><span class="line">      (attn): MultiheadAttention(</span><br><span class="line">        (out_proj): NonDynamicallyQuantizableLinear(in_features=<span class="number">768</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (ln_1): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">      (mlp): Sequential(</span><br><span class="line">        (c_fc): Linear(in_features=<span class="number">768</span>, out_features=<span class="number">3072</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (gelu): Distilled_GELU(</span><br><span class="line">          (approximator): Sequential(</span><br><span class="line">            (<span class="number">0</span>): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">64</span>, bias=<span class="literal">True</span>)</span><br><span class="line">            (<span class="number">1</span>): ReLU()</span><br><span class="line">            (<span class="number">2</span>): Linear(in_features=<span class="number">64</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">        (c_proj): Linear(in_features=<span class="number">3072</span>, out_features=<span class="number">768</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (ln_2): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>snn:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">2</span>): ResidualAttentionBlock(</span><br><span class="line">  (attn): SpikeAttention(</span><br><span class="line">    (product): SpikeProduct()</span><br><span class="line">    (spike_x2x): X2X(</span><br><span class="line">      (approximator): Sequential(</span><br><span class="line">        (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): ReLU()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): StraightThrough()</span><br><span class="line">        (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): StraightThrough()</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (spike_x2x_pos): X2X_POS(</span><br><span class="line">      (approximator): Sequential(</span><br><span class="line">        (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): ReLU()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): StraightThrough()</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (ln_1): SpikeLN(</span><br><span class="line">    (module): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (spike_sqrtinv): Distilled_SQRTINV(</span><br><span class="line">      (approximator): Sequential(</span><br><span class="line">        (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): ReLU()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): StraightThrough()</span><br><span class="line">        (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): StraightThrough()</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (spike_x2x): X2X(</span><br><span class="line">      (approximator): Sequential(</span><br><span class="line">        (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): ReLU()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): StraightThrough()</span><br><span class="line">        (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): StraightThrough()</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (mlp): Sequential(</span><br><span class="line">    (c_fc): SpikeLinear_ReLU(</span><br><span class="line">      (relu): StraightThrough()</span><br><span class="line">    )</span><br><span class="line">    (gelu): Distilled_GELU(</span><br><span class="line">      (approximator): Sequential(</span><br><span class="line">        (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): ReLU()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): StraightThrough()</span><br><span class="line">        (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): StraightThrough()</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (c_proj): SpikeLinear_ReLU(</span><br><span class="line">      (relu): StraightThrough()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (ln_2): SpikeLN(</span><br><span class="line">    (module): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (spike_sqrtinv): Distilled_SQRTINV(</span><br><span class="line">      (approximator): Sequential(</span><br><span class="line">        (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): ReLU()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): StraightThrough()</span><br><span class="line">        (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): StraightThrough()</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (spike_x2x): X2X(</span><br><span class="line">      (approximator): Sequential(</span><br><span class="line">        (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): ReLU()</span><br><span class="line">        )</span><br><span class="line">        (<span class="number">1</span>): StraightThrough()</span><br><span class="line">        (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">          (relu): StraightThrough()</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<br>
<p>下面是代码是经过如下操作后的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mse = <span class="literal">False</span> <span class="keyword">if</span> args.method ==<span class="string">&#x27;normal&#x27;</span> <span class="keyword">else</span> <span class="literal">True</span></span><br><span class="line">    get_maximum_activation(train_loader, model=snn, momentum=<span class="number">0.9</span>, iters=args.iters, mse=mse, percentile=args.percentile, T=args.T, neuron_wise=args.neuron_wise)</span><br><span class="line"></span><br><span class="line">    torch.set_num_threads(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    snn.set_spike_state(use_spike=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">2</span>): ResidualAttentionBlock(</span><br><span class="line">          (attn): SpikeAttention(</span><br><span class="line">            (product): SpikeProduct()</span><br><span class="line">            (spike_x2x): X2X(</span><br><span class="line">              (approximator): Sequential(</span><br><span class="line">                (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): ReLU()</span><br><span class="line">                )</span><br><span class="line">                (<span class="number">1</span>): StraightThrough()</span><br><span class="line">                (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): StraightThrough()</span><br><span class="line">                )</span><br><span class="line">              )</span><br><span class="line">            )</span><br><span class="line">            (spike_x2x_pos): X2X_POS(</span><br><span class="line">              (approximator): Sequential(</span><br><span class="line">                (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): ReLU()</span><br><span class="line">                )</span><br><span class="line">                (<span class="number">1</span>): StraightThrough()</span><br><span class="line">              )</span><br><span class="line">            )</span><br><span class="line">          )</span><br><span class="line">          (ln_1): SpikeLN(</span><br><span class="line">            (module): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (spike_sqrtinv): Distilled_SQRTINV(</span><br><span class="line">              (approximator): Sequential(</span><br><span class="line">                (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): ReLU()</span><br><span class="line">                )</span><br><span class="line">                (<span class="number">1</span>): StraightThrough()</span><br><span class="line">                (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): StraightThrough()</span><br><span class="line">                )</span><br><span class="line">              )</span><br><span class="line">            )</span><br><span class="line">            (spike_x2x): X2X(</span><br><span class="line">              (approximator): Sequential(</span><br><span class="line">                (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): ReLU()</span><br><span class="line">                )</span><br><span class="line">                (<span class="number">1</span>): StraightThrough()</span><br><span class="line">                (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): StraightThrough()</span><br><span class="line">                )</span><br><span class="line">              )</span><br><span class="line">            )</span><br><span class="line">          )</span><br><span class="line">          (mlp): Sequential(</span><br><span class="line">            (c_fc): SpikeLinear_ReLU(</span><br><span class="line">              (relu): StraightThrough()</span><br><span class="line">            )</span><br><span class="line">            (gelu): Distilled_GELU(</span><br><span class="line">              (approximator): Sequential(</span><br><span class="line">                (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): ReLU()</span><br><span class="line">                )</span><br><span class="line">                (<span class="number">1</span>): StraightThrough()</span><br><span class="line">                (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): StraightThrough()</span><br><span class="line">                )</span><br><span class="line">              )</span><br><span class="line">            )</span><br><span class="line">            (c_proj): SpikeLinear_ReLU(</span><br><span class="line">              (relu): StraightThrough()</span><br><span class="line">            )</span><br><span class="line">          )</span><br><span class="line">          (ln_2): SpikeLN(</span><br><span class="line">            (module): LayerNorm((<span class="number">768</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">            (spike_sqrtinv): Distilled_SQRTINV(</span><br><span class="line">              (approximator): Sequential(</span><br><span class="line">                (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): ReLU()</span><br><span class="line">                )</span><br><span class="line">                (<span class="number">1</span>): StraightThrough()</span><br><span class="line">                (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): StraightThrough()</span><br><span class="line">                )</span><br><span class="line">              )</span><br><span class="line">            )</span><br><span class="line">            (spike_x2x): X2X(</span><br><span class="line">              (approximator): Sequential(</span><br><span class="line">                (<span class="number">0</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): ReLU()</span><br><span class="line">                )</span><br><span class="line">                (<span class="number">1</span>): StraightThrough()</span><br><span class="line">                (<span class="number">2</span>): SpikeLinear_ReLU(</span><br><span class="line">                  (relu): StraightThrough()</span><br><span class="line">                )</span><br><span class="line">              )</span><br><span class="line">            )</span><br><span class="line">          )</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<br>
<h2 id="实验部分"><a class="markdownIt-Anchor" href="#实验部分"></a> 实验部分</h2>
<p><img src="https://pbs.twimg.com/media/GJf0gokXYAAiQas?format=jpg&amp;name=medium" alt="" /></p>
<p><strong>BLIP-2 在各种零样本视觉语言任务上的结果概述。与以前最先进的模型相比。 BLIP-2 实现了最高的零样本性能，同时在视觉语言预训练期间需要最少数量的可训练参数。</strong></p>
<p><img src="https://pbs.twimg.com/media/GJf1BBbXEAAZ-W7?format=jpg&amp;name=medium" alt="" /></p>
<p><strong>零样本视觉问答与最先进方法的比较。</strong></p>
<p>零样本 VQA。我们对零样本视觉问答任务进行定量评估。对于OPT模型，我们使用提示“问题：{}答案：”。对于 FlanT5 型号，我们使用提示“问题：{}简答：”。在生成过程中，我们使用波束宽度为 <strong>5</strong> 的波束搜索。我们还将长度惩罚设置为 <strong>-1</strong>，这鼓励更短的答案，更好地与人工注释保持一致。</p>
<p>如表 <strong>2</strong> 所示。<strong>BLIP-2 在 VQAv2 (Goyal et al., <strong>2017</strong>) 和 GQA (Hudson &amp; Manning, <strong>2019</strong>) 数据集上取得了最先进的结果。</strong> 尽管可训练参数少了 <strong>54</strong> 倍，但它在 VQAv2 上的性能比 Flamingo80B 高出 <strong>8.7%</strong>。在 OK-VQA（Marino 等人，<strong>2019</strong>）数据集上，BLIP-2 次于 Flamingo80B。我们假设这是因为 OK-VQA 更注重开放世界知识而不是视觉理解，而来自 Flamingo80B 的 <strong>70B</strong> Chinchilla (Hoffmann et al., <strong>2022</strong>) 语言模型比 <strong>11B</strong> FlanT5XXL 拥有更多的知识。</p>
<p>我们从表 <strong>2</strong> 中得出了一个有希望的观察结果：更强的图像编码器或更强的 LLM 都会带来更好的性能。这一观察结果得到了几个事实的支持：(1) ViT-g 在 OPT 和 FlanT5 方面均优于 ViT-L。 (2) 在同一LLM系列中，较大的模型优于较小的模型。 (3) FlanT5，一种指令调整的 LLM，在 VQA 上优于无监督训练的 OPT。这一观察结果验证了 BLIP-2 作为一种通用的视觉语言预训练方法，可以有效地收获视觉和自然语言社区的快速进步。</p>
<p><img src="https://pbs.twimg.com/media/GJf5tJYXIAAKfRQ?format=jpg&amp;name=medium" alt="" /></p>
<p><strong>表 <strong>3</strong>：NoCaps 和 COCO Caption 上最先进的图像字幕方法的比较。所有方法都优化微调过程中的交叉熵损失。 C: CIDEr, S: SPICE, B@4: BLEU@4</strong></p>
<p>我们针对图像字幕任务对 BLIP-2 模型进行了微调，该任务要求模型为图像的视觉内容生成文本描述。我们使用提示“a photo of”作为 LLM 的初始输入，并训练模型生成具有语言建模损失的标题。我们在微调期间保持 LLM 冻结，并与图像编码器一起更新 Q-Former 的参数。我们用 ViT-g 和各种 LLMs 进行实验。详细的超参数可以在附录中找到。我们对 COCO 进行微调，并对 COCO 测试集和零样本转移到 NoCaps（Agrawal 等人，<strong>2019</strong>）验证集进行评估。</p>
<p>结果如表 <strong>3</strong> 所示。<strong>BLIP-2 实现了最先进的性能，与现有方法相比，NoCap 有了显着改进，展示了对外域图像的强大泛化能力。</strong></p>
<p><img src="https://pbs.twimg.com/media/GJf6Q0BXcAEtiQI?format=png&amp;name=900x900" alt="" /></p>
<p><strong>表 <strong>4</strong>：与针对视觉问答进行微调的最先进模型的比较。</strong></p>
<p>给定带注释的 VQA 数据，我们微调 Q-Former 和图像编码器的参数，同时保持 LLM 冻结。我们对开放式答案生成损失进行微调，其中 LLM 接收 Q-Former 的输出和问题作为输入，并被要求生成答案。为了提取与问题更相关的图像特征，我们还针对问题设置了 Q-Former 条件。具体来说，问题标记作为 Q-Former 的输入给出，并通过自注意力层与查询进行交互，这可以引导 Q-Former 的交叉注意力层关注信息更丰富的图像区域。</p>
<p>遵循 BLIP，我们的 VQA 数据包括来自 VQAv2 的训练和验证分割，以及来自 Visual Genome 的训练样本。<strong>表 <strong>4</strong> 显示了 BLIP-2 在开放式生成模型中的最新结果。</strong></p>
<p><img src="https://pbs.twimg.com/media/GJf6aIBWYAATpKZ?format=jpg&amp;name=medium" alt="" /></p>
<p><strong>表 <strong>5</strong>：与最先进的图像文本检索方法的比较，在 COCO 上进行微调，并零样本传输到 Flickr30K。</strong></p>
<p>由于图像文本检索不涉及语言生成，因此我们直接对第一阶段预训练模型进行微调，无需LLM。具体来说，我们使用与预训练相同的目标（即 ITC、ITM 和 ITG）在 COCO 上与 Q-Former 一起微调图像编码器。然后，我们在 CO 和 Flickr30K 数据集上评估图像到文本检索和文本到图像检索的模型。在推理过程中，我们遵循 Li 等人的观点。 (<strong>2021</strong>, <strong>2022</strong>) 首先根据图像文本特征相似性选择 k=<strong>128</strong> 候选者，然后根据成对的 ITM 分数重新排名。我们尝试使用 ViT-L 和 ViT-g 作为图像编码器。详细的超参数可以在附录中找到。</p>
<p><img src="https://pbs.twimg.com/media/GJf6e9VWgAA74jX?format=png&amp;name=900x900" alt="" /></p>
<p><strong>表 <strong>6</strong>：基于图像的文本生成 (ITG) 损失通过强制查询提取与语言相关的视觉特征来提高图像文本检索性能。</strong></p>
<p>TC 和 ITM 损失对于图像文本检索至关重要，因为它们直接学习图像文本相似性。在表 <strong>6</strong> 中，我们表明 ITG（基于图像的文本生成）损失也有利于图像文本检索。这一结果支持了我们设计表示学习目标的直觉：ITG 损失强制查询提取与文本最相关的视觉特征，从而改善视觉语言对齐。</p>
<br>
<h2 id="transformers导入与modeling_optpy性能差别"><a class="markdownIt-Anchor" href="#transformers导入与modeling_optpy性能差别"></a> transformers导入与modeling_opt.py性能差别</h2>
<p>整体来看，其实差不多</p>
<p>transforemrs导入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">11</span>,<span class="number">260</span> [INFO] Start training epoch <span class="number">0</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">/cyb/LAVIS/lavis/processors/randaugment.py:<span class="number">40</span>: RuntimeWarning: overflow encountered <span class="keyword">in</span> scalar negative</span><br><span class="line">  offset = -low * scale</span><br><span class="line">Train: data epoch: [<span class="number">0</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:<span class="number">11</span>  lr: <span class="number">0.000001</span>  loss: <span class="number">4.7803</span>  time: <span class="number">5.9693</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">8321</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">17</span>,<span class="number">694</span> [INFO] Reducer buckets have been rebuilt <span class="keyword">in</span> this iteration.</span><br><span class="line">Train: data epoch: [<span class="number">0</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:03  lr: <span class="number">0.000001</span>  loss: <span class="number">5.7962</span>  time: <span class="number">3.3131</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">8714</span></span><br><span class="line">Train: data epoch: [<span class="number">0</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:06 (<span class="number">3.3137</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">17</span>,<span class="number">888</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">5.2883</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">17</span>,<span class="number">891</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">17</span>,<span class="number">921</span> [INFO] Saving checkpoint at epoch <span class="number">0</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_0.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">18</span>,<span class="number">958</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">19</span>,008 [INFO] Start training epoch <span class="number">1</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">1</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000098</span>  loss: <span class="number">4.2522</span>  time: <span class="number">3.2473</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">8731</span></span><br><span class="line">Train: data epoch: [<span class="number">1</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:01  lr: <span class="number">0.000098</span>  loss: <span class="number">6.3714</span>  time: <span class="number">1.8830</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">8736</span></span><br><span class="line">Train: data epoch: [<span class="number">1</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:03 (<span class="number">1.8835</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">22</span>,<span class="number">776</span> [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">5.3118</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">22</span>,<span class="number">778</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">22</span>,<span class="number">808</span> [INFO] Saving checkpoint at epoch <span class="number">1</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_1.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">23</span>,<span class="number">789</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">23</span>,<span class="number">824</span> [INFO] Start training epoch <span class="number">2</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">2</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000091</span>  loss: <span class="number">5.5883</span>  time: <span class="number">3.3091</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9036</span></span><br><span class="line">Train: data epoch: [<span class="number">2</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:01  lr: <span class="number">0.000091</span>  loss: <span class="number">6.0959</span>  time: <span class="number">1.9160</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9533</span></span><br><span class="line">Train: data epoch: [<span class="number">2</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:03 (<span class="number">1.9168</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">27</span>,<span class="number">658</span> [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">5.8421</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">27</span>,<span class="number">661</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">27</span>,<span class="number">694</span> [INFO] Saving checkpoint at epoch <span class="number">2</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_2.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">29</span>,<span class="number">868</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">29</span>,<span class="number">904</span> [INFO] Start training epoch <span class="number">3</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">/cyb/LAVIS/lavis/processors/randaugment.py:<span class="number">40</span>: RuntimeWarning: overflow encountered <span class="keyword">in</span> scalar negative</span><br><span class="line">  offset = -low * scale</span><br><span class="line">Train: data epoch: [<span class="number">3</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000081</span>  loss: <span class="number">5.6558</span>  time: <span class="number">3.4074</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9534</span></span><br><span class="line">Train: data epoch: [<span class="number">3</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000081</span>  loss: <span class="number">6.4650</span>  time: <span class="number">2.0847</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">3</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.0856</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">34</span>,076 [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">6.0604</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">34</span>,079 [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">34</span>,<span class="number">111</span> [INFO] Saving checkpoint at epoch <span class="number">3</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_3.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">36</span>,<span class="number">305</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">36</span>,<span class="number">340</span> [INFO] Start training epoch <span class="number">4</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">4</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:07  lr: <span class="number">0.000069</span>  loss: <span class="number">4.9748</span>  time: <span class="number">3.5716</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">4</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000069</span>  loss: <span class="number">6.2966</span>  time: <span class="number">2.1039</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">4</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1049</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">40</span>,<span class="number">552</span> [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">5.6357</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">40</span>,<span class="number">562</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">40</span>,<span class="number">647</span> [INFO] Saving checkpoint at epoch <span class="number">4</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_4.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">43</span>,<span class="number">149</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">43</span>,<span class="number">184</span> [INFO] Start training epoch <span class="number">5</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">/cyb/LAVIS/lavis/processors/randaugment.py:<span class="number">40</span>: RuntimeWarning: overflow encountered <span class="keyword">in</span> scalar negative</span><br><span class="line">  offset = -low * scale</span><br><span class="line">Train: data epoch: [<span class="number">5</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000055</span>  loss: <span class="number">4.6983</span>  time: <span class="number">3.4429</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">5</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000055</span>  loss: <span class="number">7.9794</span>  time: <span class="number">2.1261</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">5</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1273</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">47</span>,<span class="number">440</span> [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">6.3388</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">47</span>,<span class="number">443</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">47</span>,<span class="number">475</span> [INFO] Saving checkpoint at epoch <span class="number">5</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_5.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">49</span>,<span class="number">912</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">49</span>,<span class="number">948</span> [INFO] Start training epoch <span class="number">6</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">6</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:07  lr: <span class="number">0.000041</span>  loss: <span class="number">4.7854</span>  time: <span class="number">3.5252</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">6</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000041</span>  loss: <span class="number">5.9835</span>  time: <span class="number">2.1675</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">6</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1683</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">54</span>,<span class="number">286</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">5.3845</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">54</span>,<span class="number">289</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">54</span>,<span class="number">332</span> [INFO] Saving checkpoint at epoch <span class="number">6</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_6.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">56</span>,<span class="number">880</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">49</span>:<span class="number">56</span>,<span class="number">914</span> [INFO] Start training epoch <span class="number">7</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">7</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000029</span>  loss: <span class="number">4.6165</span>  time: <span class="number">3.4250</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">7</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:01  lr: <span class="number">0.000029</span>  loss: <span class="number">6.2248</span>  time: <span class="number">1.9936</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">7</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:03 (<span class="number">1.9946</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">00</span>,<span class="number">904</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">5.4207</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">00</span>,<span class="number">908</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">00</span>,<span class="number">967</span> [INFO] Saving checkpoint at epoch <span class="number">7</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_7.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:03,<span class="number">337</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:03,<span class="number">373</span> [INFO] Start training epoch <span class="number">8</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">/cyb/LAVIS/lavis/processors/randaugment.py:<span class="number">40</span>: RuntimeWarning: overflow encountered <span class="keyword">in</span> scalar negative</span><br><span class="line">  offset = -low * scale</span><br><span class="line">Train: data epoch: [<span class="number">8</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000019</span>  loss: <span class="number">5.0990</span>  time: <span class="number">3.4835</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">8</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000019</span>  loss: <span class="number">6.2343</span>  time: <span class="number">2.1475</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">8</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1485</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:07,<span class="number">670</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">5.6667</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:07,<span class="number">674</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:07,<span class="number">705</span> [INFO] Saving checkpoint at epoch <span class="number">8</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_8.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">10</span>,070 [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">10</span>,<span class="number">104</span> [INFO] Start training epoch <span class="number">9</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">9</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000012</span>  loss: <span class="number">5.9895</span>  time: <span class="number">3.4054</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">9</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000012</span>  loss: <span class="number">5.6200</span>  time: <span class="number">2.0422</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">9</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.0434</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">14</span>,<span class="number">192</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">5.8048</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">14</span>,<span class="number">197</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">14</span>,<span class="number">233</span> [INFO] Saving checkpoint at epoch <span class="number">9</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327164</span>/checkpoint_9.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">16</span>,<span class="number">610</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">50</span>:<span class="number">16</span>,<span class="number">610</span> [INFO] Training time <span class="number">0</span>:01:<span class="number">10</span></span><br></pre></td></tr></table></figure>
<br>
<p>modeling_opt.py文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:03,<span class="number">450</span> [INFO] Start training epoch <span class="number">0</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">/cyb/LAVIS/lavis/processors/randaugment.py:<span class="number">40</span>: RuntimeWarning: overflow encountered <span class="keyword">in</span> scalar negative</span><br><span class="line">  offset = -low * scale</span><br><span class="line">Train: data epoch: [<span class="number">0</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:<span class="number">12</span>  lr: <span class="number">0.000001</span>  loss: <span class="number">4.7803</span>  time: <span class="number">6.4565</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">8321</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">10</span>,<span class="number">549</span> [INFO] Reducer buckets have been rebuilt <span class="keyword">in</span> this iteration.</span><br><span class="line">Train: data epoch: [<span class="number">0</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:03  lr: <span class="number">0.000001</span>  loss: <span class="number">5.7962</span>  time: <span class="number">3.6563</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">8714</span></span><br><span class="line">Train: data epoch: [<span class="number">0</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:07 (<span class="number">3.6570</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">10</span>,<span class="number">765</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">5.2883</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">10</span>,<span class="number">770</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">10</span>,<span class="number">815</span> [INFO] Saving checkpoint at epoch <span class="number">0</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_0.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">11</span>,<span class="number">871</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">11</span>,<span class="number">907</span> [INFO] Start training epoch <span class="number">1</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">1</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000098</span>  loss: <span class="number">3.7817</span>  time: <span class="number">3.4140</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9036</span></span><br><span class="line">Train: data epoch: [<span class="number">1</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000098</span>  loss: <span class="number">5.3590</span>  time: <span class="number">2.0641</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9555</span></span><br><span class="line">Train: data epoch: [<span class="number">1</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.0650</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">16</span>,037 [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">4.5704</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">16</span>,041 [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">16</span>,071 [INFO] Saving checkpoint at epoch <span class="number">1</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_1.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">18</span>,<span class="number">357</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">18</span>,<span class="number">399</span> [INFO] Start training epoch <span class="number">2</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">2</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:07  lr: <span class="number">0.000091</span>  loss: <span class="number">4.9213</span>  time: <span class="number">3.6945</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">2</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000091</span>  loss: <span class="number">6.5263</span>  time: <span class="number">2.1551</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">2</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1560</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">22</span>,<span class="number">713</span> [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">5.7238</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">22</span>,<span class="number">717</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">22</span>,<span class="number">765</span> [INFO] Saving checkpoint at epoch <span class="number">2</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_2.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">25</span>,<span class="number">206</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">25</span>,<span class="number">241</span> [INFO] Start training epoch <span class="number">3</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">3</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000081</span>  loss: <span class="number">4.6143</span>  time: <span class="number">3.4231</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">3</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:01  lr: <span class="number">0.000081</span>  loss: <span class="number">5.9560</span>  time: <span class="number">1.9856</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">3</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:03 (<span class="number">1.9866</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">29</span>,<span class="number">215</span> [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">5.2851</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">29</span>,<span class="number">220</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">29</span>,<span class="number">270</span> [INFO] Saving checkpoint at epoch <span class="number">3</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_3.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">31</span>,<span class="number">729</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">31</span>,<span class="number">772</span> [INFO] Start training epoch <span class="number">4</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">4</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000069</span>  loss: <span class="number">4.4050</span>  time: <span class="number">3.3401</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">4</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000069</span>  loss: <span class="number">5.5067</span>  time: <span class="number">2.1267</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">4</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1275</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">36</span>,028 [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">4.9559</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">36</span>,031 [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">36</span>,064 [INFO] Saving checkpoint at epoch <span class="number">4</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_4.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">38</span>,<span class="number">607</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">38</span>,<span class="number">643</span> [INFO] Start training epoch <span class="number">5</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">5</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:07  lr: <span class="number">0.000055</span>  loss: <span class="number">4.0671</span>  time: <span class="number">3.5544</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">5</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000055</span>  loss: <span class="number">5.3782</span>  time: <span class="number">2.2167</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">5</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.2181</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">43</span>,080 [INFO] Averaged stats: lr: <span class="number">0.0001</span>  loss: <span class="number">4.7227</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">43</span>,083 [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">43</span>,<span class="number">117</span> [INFO] Saving checkpoint at epoch <span class="number">5</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_5.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">45</span>,<span class="number">519</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">45</span>,<span class="number">555</span> [INFO] Start training epoch <span class="number">6</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">6</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:07  lr: <span class="number">0.000041</span>  loss: <span class="number">4.1874</span>  time: <span class="number">3.5827</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">6</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000041</span>  loss: <span class="number">3.9796</span>  time: <span class="number">2.1472</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">6</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1486</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">49</span>,<span class="number">853</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">4.0835</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">49</span>,<span class="number">856</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">49</span>,<span class="number">889</span> [INFO] Saving checkpoint at epoch <span class="number">6</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_6.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">52</span>,<span class="number">262</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">52</span>,<span class="number">298</span> [INFO] Start training epoch <span class="number">7</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">7</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000029</span>  loss: <span class="number">4.0438</span>  time: <span class="number">3.3334</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">7</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000029</span>  loss: <span class="number">4.3911</span>  time: <span class="number">2.2152</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">7</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.2175</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">56</span>,<span class="number">737</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">4.2175</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">56</span>,<span class="number">761</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">56</span>,<span class="number">876</span> [INFO] Saving checkpoint at epoch <span class="number">7</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_7.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">59</span>,<span class="number">494</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">58</span>:<span class="number">59</span>,<span class="number">534</span> [INFO] Start training epoch <span class="number">8</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">8</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000019</span>  loss: <span class="number">3.9183</span>  time: <span class="number">3.3688</span>  data: <span class="number">0.0001</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">8</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000019</span>  loss: <span class="number">4.3744</span>  time: <span class="number">2.1282</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">8</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1292</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:03,<span class="number">794</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">4.1464</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:03,<span class="number">797</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:03,<span class="number">830</span> [INFO] Saving checkpoint at epoch <span class="number">8</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_8.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:06,<span class="number">318</span> [INFO] Start training</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:06,<span class="number">368</span> [INFO] Start training epoch <span class="number">9</span>, <span class="number">2</span> iters per inner epoch.</span><br><span class="line">Train: data epoch: [<span class="number">9</span>]  [<span class="number">0</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:06  lr: <span class="number">0.000012</span>  loss: <span class="number">4.1729</span>  time: <span class="number">3.4050</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">9</span>]  [<span class="number">1</span>/<span class="number">2</span>]  eta: <span class="number">0</span>:<span class="number">00</span>:02  lr: <span class="number">0.000012</span>  loss: <span class="number">3.5403</span>  time: <span class="number">2.1497</span>  data: <span class="number">0.0000</span>  <span class="built_in">max</span> mem: <span class="number">9556</span></span><br><span class="line">Train: data epoch: [<span class="number">9</span>] Total time: <span class="number">0</span>:<span class="number">00</span>:04 (<span class="number">2.1513</span> s / it)</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">10</span>,<span class="number">672</span> [INFO] Averaged stats: lr: <span class="number">0.0000</span>  loss: <span class="number">3.8566</span></span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">10</span>,<span class="number">677</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">10</span>,<span class="number">716</span> [INFO] Saving checkpoint at epoch <span class="number">9</span> to /cyb/LAVIS/output/BLIP2/Pretrain_stage2/<span class="number">20240327165</span>/checkpoint_9.pth.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">13</span>,<span class="number">151</span> [INFO] No validation splits found.</span><br><span class="line"><span class="number">2024</span>-03-<span class="number">27</span> <span class="number">16</span>:<span class="number">59</span>:<span class="number">13</span>,<span class="number">151</span> [INFO] Training time <span class="number">0</span>:01:<span class="number">13</span></span><br></pre></td></tr></table></figure>
<h2 id="dataloader"><a class="markdownIt-Anchor" href="#dataloader"></a> dataloader</h2>
<p>vg_captioin</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ImageTextPairDataset</span>(BaseDataset, __DisplMixin):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vis_processor, text_processor, vis_root, ann_paths</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        vis_root (string): Root directory of images (e.g. coco/images/)</span></span><br><span class="line"><span class="string">        ann_root (string): directory to store the annotation file</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(vis_processor, text_processor, vis_root, ann_paths)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># TODO this assumes image input, not general enough</span></span><br><span class="line">        ann = self.annotation[index]</span><br><span class="line"></span><br><span class="line">        image_path = os.path.join(self.vis_root, ann[<span class="string">&quot;image&quot;</span>])</span><br><span class="line">        image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line"></span><br><span class="line">        image = self.vis_processor(image)</span><br><span class="line">        caption = self.text_processor(ann[<span class="string">&quot;caption&quot;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;image&quot;</span>: image, <span class="string">&quot;text_input&quot;</span>: caption&#125;</span><br></pre></td></tr></table></figure>
<p>coco_caption</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CaptionDataset</span>(BaseDataset, __DisplMixin):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vis_processor, text_processor, vis_root, ann_paths</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        vis_root (string): Root directory of images (e.g. coco/images/)</span></span><br><span class="line"><span class="string">        ann_root (string): directory to store the annotation file</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(vis_processor, text_processor, vis_root, ann_paths)</span><br><span class="line"></span><br><span class="line">        self.img_ids = &#123;&#125;</span><br><span class="line">        n = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ann <span class="keyword">in</span> self.annotation:</span><br><span class="line">            img_id = ann[<span class="string">&quot;image_id&quot;</span>]</span><br><span class="line">            <span class="keyword">if</span> img_id <span class="keyword">not</span> <span class="keyword">in</span> self.img_ids.keys():</span><br><span class="line">                self.img_ids[img_id] = n</span><br><span class="line">                n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># TODO this assumes image input, not general enough</span></span><br><span class="line">        ann = self.annotation[index]</span><br><span class="line"></span><br><span class="line">        image_path = os.path.join(self.vis_root, ann[<span class="string">&quot;image&quot;</span>])</span><br><span class="line">        image = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line"></span><br><span class="line">        image = self.vis_processor(image)</span><br><span class="line">        caption = self.text_processor(ann[<span class="string">&quot;caption&quot;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;image&quot;</span>: image,</span><br><span class="line">            <span class="string">&quot;text_input&quot;</span>: caption,</span><br><span class="line">            <span class="comment"># &quot;image_id&quot;: self.img_ids[ann[&quot;image_id&quot;]],</span></span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考链接"><a class="markdownIt-Anchor" href="#参考链接"></a> 参考链接</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=XrunSYwoLr">Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.12597.pdf">BLIP-2</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">projects/blip2</a></li>
</ul>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2024/03/08/BLIP-2-x-STA/" data-id="clti33kfp0000fi690lojfr7h" data-title="BLIP-2 x STA"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/blip2/" rel="tag">blip2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/internship/" rel="tag">internship</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sta/" rel="tag">sta</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://pbs.twimg.com/media/GHVZM28XUAEbhJW?format=jpg&amp;name=medium" data-sizes="auto" alt="Python:@registry.register_model"
                         class="lazyload">
                
                <a href="/2024/03/08/Python-registry-register-model/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        Python:@registry.register_model
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://pbs.twimg.com/media/GIHgVtrbAAEimI9?format=png&amp;name=240x240" data-sizes="auto" alt="智能信息网络:小作业4选1"
                         class="lazyload">
                
                <a href="/2024/03/07/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C-%E5%B0%8F%E4%BD%9C%E4%B8%9A4%E9%80%891/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        智能信息网络:小作业4选1
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="Jerome" class="lazyload">
            <div class="sidebar-author-name">Jerome</div>
            <div class="sidebar-description">Indeed, I am quite the oddity.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">365</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">34</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">393</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/74X2u8JMVooG2QbjRxXwR8?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Competition/">Competition</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Future/">Future</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/IELTS/">IELTS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/Multimudal/">Multimudal</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS224N/">CS224N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS231N/">CS231N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/Missing-Semester-of-CS/">Missing Semester of CS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Computer-Vision/">Computer Vision</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%96%87%E5%8C%96%E8%AE%A1%E7%AE%97/">文化计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/">智能信息网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/">语音信息处理</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%91%A8%E8%AE%B0/">周记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 12.78px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/17/" style="font-size: 10px;">17</a> <a href="/tags/2/" style="font-size: 13.89px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 12.22px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/3-11/" style="font-size: 10px;">3.11</a> <a href="/tags/3-4/" style="font-size: 10px;">3.4</a> <a href="/tags/4/" style="font-size: 11.11px;">4</a> <a href="/tags/5/" style="font-size: 10.56px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 11.11px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 17.78px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.11px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 13.89px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 11.67px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10.56px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 11.67px;">CNN</a> <a href="/tags/CS224N/" style="font-size: 10.56px;">CS224N</a> <a href="/tags/CS231N/" style="font-size: 10px;">CS231N</a> <a href="/tags/CV/" style="font-size: 12.78px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 12.78px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/ComPetition/" style="font-size: 10px;">ComPetition</a> <a href="/tags/Competition/" style="font-size: 12.22px;">Competition</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 16.11px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.56px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Filter/" style="font-size: 10px;">Filter</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/Future/" style="font-size: 13.33px;">Future</a> <a href="/tags/GB/" style="font-size: 10px;">GB</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GiB/" style="font-size: 10px;">GiB</a> <a href="/tags/Git/" style="font-size: 10.56px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 17.22px;">GoAbroad</a> <a href="/tags/Graduate/" style="font-size: 10px;">Graduate</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 12.22px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/LayerNorm/" style="font-size: 10px;">LayerNorm</a> <a href="/tags/Lec01/" style="font-size: 11.11px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.56px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.67px;">Linux</a> <a href="/tags/Listening/" style="font-size: 10px;">Listening</a> <a href="/tags/M2/" style="font-size: 10.56px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.22px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 13.89px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.56px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 11.11px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NECCS/" style="font-size: 10px;">NECCS</a> <a href="/tags/NLP/" style="font-size: 11.67px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 16.67px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/OS/" style="font-size: 13.89px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.56px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Reading/" style="font-size: 10px;">Reading</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10.56px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.11px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.22px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Script/" style="font-size: 10px;">Script</a> <a href="/tags/Shell/" style="font-size: 10.56px;">Shell</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-Neural-Network-for-Ultra-low-latency-and-High-accurate-Object-Detection/" style="font-size: 10px;">Spiking Neural Network for Ultra-low-latency and High-accurate Object Detection</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.56px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.56px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/Undergraduate/" style="font-size: 10px;">Undergraduate</a> <a href="/tags/University/" style="font-size: 12.78px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 11.11px;">ViT</a> <a href="/tags/Vim/" style="font-size: 10px;">Vim</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.56px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/alexnet/" style="font-size: 10px;">alexnet</a> <a href="/tags/anygpt/" style="font-size: 10px;">anygpt</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 11.67px;">bert</a> <a href="/tags/blip2/" style="font-size: 10px;">blip2</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/bug/" style="font-size: 16.67px;">bug</a> <a href="/tags/cat/" style="font-size: 10px;">cat</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.11px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.56px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.56px;">chapter05</a> <a href="/tags/chapter6/" style="font-size: 10px;">chapter6</a> <a href="/tags/chapter7/" style="font-size: 10px;">chapter7</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/chmod/" style="font-size: 10px;">chmod</a> <a href="/tags/chrome/" style="font-size: 10px;">chrome</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/code/" style="font-size: 11.67px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/commit/" style="font-size: 10px;">commit</a> <a href="/tags/competition/" style="font-size: 10px;">competition</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/copilot/" style="font-size: 10.56px;">copilot</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10.56px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.33px;">d2l</a> <a href="/tags/database/" style="font-size: 13.89px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.56px;">deep neural network</a> <a href="/tags/delete/" style="font-size: 10px;">delete</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/dowhy/" style="font-size: 10.56px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10.56px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/embedding/" style="font-size: 10px;">embedding</a> <a href="/tags/explainer/" style="font-size: 10.56px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/github/" style="font-size: 12.22px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 11.11px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10.56px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/import/" style="font-size: 10px;">import</a> <a href="/tags/instructor/" style="font-size: 11.67px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 11.67px;">intern00</a> <a href="/tags/interns/" style="font-size: 10px;">interns</a> <a href="/tags/internship/" style="font-size: 18.89px;">internship</a> <a href="/tags/interview/" style="font-size: 10px;">interview</a> <a href="/tags/introduction/" style="font-size: 11.11px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/jmbook/" style="font-size: 10.56px;">jmbook</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab/" style="font-size: 10px;">lab</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.56px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 11.11px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lr/" style="font-size: 10px;">lr</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 12.22px;">mac</a> <a href="/tags/memory/" style="font-size: 11.67px;">memory</a> <a href="/tags/mentor/" style="font-size: 10.56px;">mentor</a> <a href="/tags/mid/" style="font-size: 10.56px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/multimudal/" style="font-size: 10px;">multimudal</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.56px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.56px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 15px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.11px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.44px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 11.67px;">prml</a> <a href="/tags/profile/" style="font-size: 10px;">profile</a> <a href="/tags/project/" style="font-size: 12.78px;">project</a> <a href="/tags/pycharm/" style="font-size: 10px;">pycharm</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/pytorch/" style="font-size: 14.44px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10.56px;">reading</a> <a href="/tags/register/" style="font-size: 10px;">register</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 15px;">review</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.56px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/server/" style="font-size: 10px;">server</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/sora/" style="font-size: 10px;">sora</a> <a href="/tags/spike/" style="font-size: 10.56px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.56px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikebert/" style="font-size: 10px;">spikebert</a> <a href="/tags/spikingjelly/" style="font-size: 12.22px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.56px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.56px;">ssh</a> <a href="/tags/sta/" style="font-size: 10px;">sta</a> <a href="/tags/terminal/" style="font-size: 10px;">terminal</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.56px;">tips</a> <a href="/tags/tittle/" style="font-size: 10px;">tittle</a> <a href="/tags/tmux/" style="font-size: 10px;">tmux</a> <a href="/tags/tool/" style="font-size: 18.33px;">tool</a> <a href="/tags/transformer/" style="font-size: 12.78px;">transformer</a> <a href="/tags/transformers/" style="font-size: 10px;">transformers</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10.56px;">vscode</a> <a href="/tags/wakatime/" style="font-size: 10px;">wakatime</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/yeild/" style="font-size: 10px;">yeild</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">专业知识</a> <a href="/tags/%E4%B8%93%E7%A1%95/" style="font-size: 10px;">专硕</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E4%BB%A3%E7%90%86/" style="font-size: 10px;">代理</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">公选课</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.56px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7/" style="font-size: 10px;">写作技巧</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" style="font-size: 10px;">博弈论</a> <a href="/tags/%E5%91%A8%E8%AE%B0/" style="font-size: 11.11px;">周记</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">图像描述生成</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">大作业</a> <a href="/tags/%E5%A4%A7%E5%88%9B/" style="font-size: 10px;">大创</a> <a href="/tags/%E5%A4%A7%E8%8B%B1%E8%B5%9B/" style="font-size: 10px;">大英赛</a> <a href="/tags/%E5%AD%A6%E7%A1%95/" style="font-size: 10px;">学硕</a> <a href="/tags/%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/" style="font-size: 10px;">实验报告</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.56px;">审稿意见</a> <a href="/tags/%E5%B0%8F%E4%BD%9C%E4%B8%9A/" style="font-size: 10.56px;">小作业</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%82%84%E6%82%84%E8%AF%9D/" style="font-size: 10px;">悄悄话</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%99%E8%82%B2%E8%AE%B8%E5%8F%AF/" style="font-size: 10px;">教育许可</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%96%87%E5%8C%96%E8%AE%A1%E7%AE%97/" style="font-size: 11.11px;">文化计算</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">显卡</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.56px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/" style="font-size: 11.11px;">智能信息网络</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 13.89px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.56px;">服务器</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期中复习</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">朴素贝叶斯</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 11.67px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.56px;">李宏毅</a> <a href="/tags/%E6%9D%8E%E6%B2%90/" style="font-size: 10px;">李沐</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">模型训练流程</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 12.78px;">毛概</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.56px;">特征学习</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 10px;">目标检测</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">第一次作业</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">第三章</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E7%BD%91%E6%98%93/" style="font-size: 10px;">网易</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3/" style="font-size: 10px;">脑机接口</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%8A%82%E8%83%BD%E5%87%8F%E6%8E%92/" style="font-size: 11.11px;">节能减排</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">评测指标</a> <a href="/tags/%E8%AF%AD%E4%B9%89%E7%A9%BA%E9%97%B4/" style="font-size: 10px;">语义空间</a> <a href="/tags/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/" style="font-size: 11.67px;">语音信息处理</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">课程</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.11px;">调研</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">软件概要设计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%87%8F%E5%8C%96/" style="font-size: 10px;">量化</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">四月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                Jerome
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    734.8k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    46:34
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="Jerome" class="lazyload">
            <div class="sidebar-author-name">Jerome</div>
            <div class="sidebar-description">Indeed, I am quite the oddity.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">365</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">34</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">393</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

