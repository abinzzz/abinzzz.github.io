
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        NLP:阅读作业1-JMBook Chap 6 |
        
        布洛戈</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });     Vector Semantics and Embeddings目录 Vector Semantics and Embeddings 目录 Introduction 1. Vector Semantics 2.Wor">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP:阅读作业1-JMBook Chap 6">
<meta property="og:url" content="https://abinzzz.github.io/2024/03/05/NLP-%E9%98%85%E8%AF%BB%E4%BD%9C%E4%B8%9A1-JMBook-Chap-6/index.html">
<meta property="og:site_name" content="布洛戈">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });     Vector Semantics and Embeddings目录 Vector Semantics and Embeddings 目录 Introduction 1. Vector Semantics 2.Wor">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/GH_OheSXgAAznPs?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GH_QxmyWsAA5len?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GH_U5wbXEAAyXu7?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GH_iN4WWkAADtHE?format=png&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GH_iSRxWkAELL7p?format=png&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GH_lAPyWAAAHvy9?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GH_mQDyX0AA5UCe?format=png&amp;name=medium">
<meta property="article:published_time" content="2024-03-05T14:10:23.000Z">
<meta property="article:modified_time" content="2024-03-06T14:53:32.243Z">
<meta property="article:author" content="ab">
<meta property="article:tag" content="专业知识">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="jmbook">
<meta property="article:tag" content="chapter6">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/GH_OheSXgAAznPs?format=jpg&amp;name=medium">
    
        <link rel="alternate" href="/atom.xml" title="布洛戈" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://pbs.twimg.com/media/GH91GXWW4AAwR9A?format=jpg&amp;name=medium" data-sizes="auto" alt="NLP:阅读作业1-JMBook Chap 6" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>NLP:阅读作业1-JMBook Chap 6</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-NLP-阅读作业1-JMBook-Chap-6" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2024/03/05/NLP-%E9%98%85%E8%AF%BB%E4%BD%9C%E4%B8%9A1-JMBook-Chap-6/" class="article-date-link">
        <time datetime="2024-03-05T14:10:23.000Z"
              itemprop="datePublished">2024-03-05</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><a class="article-category-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NLP/">NLP</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>


<!-- # Vector Semantics and Embeddings
**distributional hypothesis**: This link between similarity in how words are distributed and similarity in what they mean  
**vector semantics**: instantiates this linguistic hypothesis by learning representations of the meaning of words   
**embeddings**: representations of the meaning of words


## 6.1 Lexical Semantics
**lexical semantics**:  these desiderata, drawing on results in the linguistic study of word meaning  
**lemma**:mouse的形式也是mice这个词的lemma;sing是sing, sang, sung的lemma  
**wordforms**:特定的形式,例如sung   
**word sense**： each of these aspects of the meaning of mouse(不只是mouse哦)    
**polysemous**：have multiple senses     
**synonyms**：(substitutable)identical to a sense of another word, or nearly identical     
**word relatedness**: The meaning of two words can be related in ways other than similarity.   
**semantic field**: One common kind of relatedness between words is if they belong to the same semantic field。举例：医院(外科医生、手术刀、护士、外科医生、医院)  
**semantic frame**：a set of words that denote perspectives or participants in a particular type of even，框架具有语义角色(如买方、卖方、货物、货币)   
**connotations**：words have affective meanings     
**sentiment**：Positive or negative evaluation language    


<br>


## 6.2 Vector Semantics
**Vector semantics**: the standard way to represent word meaning in NLP  
**Vector semantics的思想**: 将一个单词表示为多维语义空间中的一个点，这个多维语义空间是从单词邻居的分布中派生出来的(我们将看到的方式)。   
**embeddings**: Vectors for representing words   

![](https://pbs.twimg.com/media/GH-dX0gWAAAEjng?format=jpg&name=medium)


## 6.3 Words and Vectors
co-occurrence matrix:
-  the term-document matrix
-  the term-term matrix


### 6.3.1 Vectors and documents
term-document matrix:
- 一行表示词汇表中的一个单词
- 每一列表示某个文档集合中的一个文档

![](https://pbs.twimg.com/media/GH-e2ZrWEAMASch?format=jpg&name=medium)  


**vector**:本质是一个数字列表或数组，例如上图的Julius Caesar[7,62,1,2]   
**vector space**: vector的集合，维数来表示特征     

Information retrieval (IR)：从最匹配查询q的集合中的D个文档中查找文档d的任务


### 6.3.2 Words as vectors: document dimensions
我们已经看到document可以表示为向量空间中的向量,但是vector semantics也可以用来表示单词的意思(row vector)。term-document矩阵可以让我们通过单词出现在哪些文档中来表示单词的含义

### 6.3.3 Words as vectors: word dimensions
**word-word matrix**:区别是列由单词而不是文档标记



### 6.4 Cosine for measuring similarity
**cosine of the angle between the vectors**: 衡量两个目标单词v和w之间的相似度

点积作为一个相似度度量，因为当两个向量在相同的维度上具有较大的值时，它会趋于高。另外，在不同维数上为零的向量——正交向量——的点积为0，表示它们之间有很强的不相似性

缺点：倾向于长向量(向量较长，则点积越大，在每个维度上的值也越大)

解决方法：点积除以两个向量的长度来归一化向量的长度


### 6.5 TF-IDF:向量中的加权项
原始频率并不是衡量单词之间关联的最佳指标。原始频率是非常偏斜的，没有很大的区别。如果我们想知道cherry和strawberry共享什么样的语境，而不是数字和信息，我们无法很好地区分像the、it或they这样的词，它们频繁地出现在各种单词中，并不能提供任何特定单词的信息。


tf-idf权重，通常在维度是文档时使用。接下来我们将介绍PPMI算法(通常在维度为单词时使用)

term frequency:文档d中单词t的频率

tf-idf中的第二个因素用于为只在少数文档中出现的单词赋予更高的权重

一个词t的文档频率dft: 是它出现在文档中的数量

我们通过逆文档频率或idf项权重来强调像Romeo这样的判别词(Sparck Jones, 1972)。idf是使用分数N/dft定义的，其中N是集合中文档的总数，dft是出现术语t的文档的数量。


## 6.6 Pointwise Mutual Information (PMI)
PPMI利用直觉来衡量两个词之间的关联，最好的方法是问这两个词在我们的语料库中同时出现的次数比我们先验地预期它们偶然出现的次数多多少

它衡量的是两个事件x和y发生的频率，与它们独立的情况下我们的期望相比

PMI值的范围从负无穷到正无穷


通常使用正PMI(称为PPMI)来取代所有负PMI值
为零


PMI存在偏向于不常见事件的问题;非常罕见的单词往往具有非常高的PMI值。有一种方法可以减少这种偏向于低频的偏差


## 6.7 Applications of the tf-idf or PPMI vector models


## 6.8 Word2vec
我们看到了如何将单词表示为一个稀疏的长向量，其维度对应于词汇表中的单词或集合中的文档

我们现在引入一个更短而密集的词表示:embeddings

在本节中，我们介绍一种计算嵌入的方法:带负采样的skip-gram，有时称为SGNS(word2vec软件包中的两种算法之一)

word2vec的直觉是，我们不是计算每个单词w出现在杏子附近的频率，而是训练一个分类器进行二元预测任务

单词c出现在目标单词apricot附近，就相当于“单词c可能出现在apricot附近吗?”这种方法通常被称为自我监督


### 6.8.1 The classifier
我们的目标是训练一个分类器，给定目标词w的元组(w,c)与候选上下文词c配对(例如(apricot, jam)，或者(apricot, aardvark))，它将返回c是真实上下文词的概率(jam为真，aardvark为假)。

分类器如何计算概率P?skip- gram模型的直觉是将这种概率建立在嵌入相似性的基础上:如果一个单词的嵌入向量与目标嵌入向量相似，那么它就可能出现在目标附近。为了计算这些密集嵌入之间的相似性，我们依赖于这样的直觉:如果两个向量具有较高的点积，则它们是相似的


了将点积转化为概率，我们将使用逻辑函数或s型函数σ (x)


总而言之，skip-gram训练了一个概率分类器，给定一个测试目标单词w及其L个单词的上下文窗口c1:L，根据上下文窗口与目标单词的相似程度分配概率


### 6.8.2 Learning skip-gram embeddings
skip-gram嵌入的学习算法将文本语料库和选定的词汇量N作为输入，首先为N个词汇中的每个单词分配一个随机嵌入向量，然后继续迭代地移动每个单词w的嵌入，使其更像文本中附近出现的单词的嵌入，而不是附近出现的单词的嵌入。

为了训练二值分类器，我们还需要负例。事实上，带负采样的跳跃图(SGNS)使用的负样例多于正样例

噪声词根据其加权单格频率pα (w)来选择，其中α是权重

给定一组正训练和负训练实例，以及一组初始嵌入，学习算法的目标是将这些嵌入调整为:- 最大化从正例中提取的目标词，上下文词对(w,cpos)的相似性
- 最小化(w,cneg)对与反例的相似性。


如果我们考虑一个单词/上下文对(w, cpos)及其k个噪声单词cneg1…cnegk，我们可以将这两个目标表示为以下损失函数L最小化(因此有−);在这里，第一项表示我们希望分类器将真实上下文单词cpos分配为近邻的高概率，第二项表示我们希望将噪声单词分配为非近邻的高概率，所有相乘是因为我们假设独立性

### 6.8.3 Other kinds of static embeddings
有很多种静态嵌入。word2vec的扩展fasttext (Bojanowski等人，2017)解决了目前为止word2vec的一个问题:它没有很好的方法来处理未知单词——出现在测试语料库中但在训练语料库中未见过的单词

Fasttext通过使用子单词模型来处理这些问题，每个单词表示为自身加上一个n元语法包，带有特殊的边界符号&lt;和比;添加到每个单词。例如，当n = 3时，单词where将由序列&lt;加上字符n元语法

<br>


## 6.9 Visualizing Embeddings
Visualizing Embeddings: 帮助理解、应用和改进这些词义模型的一个重要目标。

可视化嵌入在空间中的单词w的含义的最简单方法是通过将词汇表中所有单词的向量按其与w的向量的余弦进行排序，列出与w最相似的单词

x


## 6.10 Semantic properties of embeddings
一阶共现(有时称为组合联想)，如果它们通常在彼此附近。这样写的是书或诗的一级联想。如果两个单词有相似的邻接词，则它们具有二级共现(有时称为聚合联想)。因此，written是said或remark等词的二级关联词


类比/关系相似性:嵌入的另一个语义属性是它们捕获关系意义的能力。

平行四边形方法通常过于简单，无法模拟人类形成这种类比的认知过程。

### 6.10.1 Embeddings and Historical Semantics

补

嵌入也是研究意义如何随时间变化的有用工具，
通过计算多个嵌入空间，每个嵌入空间来自在特定时间段内编写的文本。


## 6.11 Bias and Embeddings
除了他们从文本中学习词义的能力，嵌入，alas还重现了文本中潜在的内隐偏见和刻板印象。正如上一节所示，嵌入可以大致模拟关系相似性:“女王”是最接近“国王”的单词——“男人”+“女人”意味着类比:女人::国王:女王。但这些相同的嵌入类比也表现出性别刻板印象


## 6.12 Evaluating Vector Models
最常见的指标是测试它们在相似度上的表现，计算算法的单词相似度得分和人类分配的单词相似度评级之间的相关性。



## 6.13 Summary
embedding: 高维空间中的向量点，也称为嵌入

static embed: word is mapped to a fixed embedding

向量语义模型分为稀疏和稠密两类。在稀疏模型中，每个维度对应于词汇表V中的一个单词，单元格是共现计数的函数。词项-文档矩阵中，每个词(词)对应一行，每个文档对应一列。词-上下文或词-词矩阵的一行对应词汇表中的每个(目标)单词，一列对应词汇表中的每个上下文术语。两种稀疏加权是常见的:tf-idf加权，它通过词频和逆文档频率对每个单元格进行加权，以及PPMI (point- wise positive mutual information)，它在单词上下文矩阵中最常见。


稠密向量模型的维度为50 ~ 1000。像skip-gram这样的Word2vec算法是计算密集嵌入的流行方法。Skip-gram训练一个逻辑回归分类器来计算两个单词“可能在文本中出现在附近”的概率。这个概率是通过两个单词的嵌入之间的点积来计算的

Skip-gram使用随机梯度下降来训练分类器，通过学习与附近出现的词的嵌入具有高点积和与噪声词的低点积的嵌入

其他重要的嵌入算法包括GloVe，这是一种基于单词共现概率比率的方法。

无论是使用稀疏向量还是密集向量，单词和文档的相似度都是通过向量之间的点积函数来计算的。两个向量的余弦——一个归一化的点积——是这类度量中最流行的。


---- -->
<h1 id="Vector-Semantics-and-Embeddings"><a href="#Vector-Semantics-and-Embeddings" class="headerlink" title="Vector Semantics and Embeddings"></a>Vector Semantics and Embeddings</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li><a href="#vector-semantics-and-embeddings">Vector Semantics and Embeddings</a><ul>
<li><a href="#目录">目录</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#1-vector-semantics">1. Vector Semantics</a></li>
<li><a href="#2words-and-vectors">2.Words and Vectors</a><ul>
<li><a href="#21-words-as-vectors-document-dimensions">2.1 Words as vectors: document dimensions</a></li>
<li><a href="#22-words-as-vectors-word-dimensions">2.2 Words as vectors: word dimensions</a></li>
</ul>
</li>
<li><a href="#3-cosine-for-measuring-similarity">3. Cosine for measuring similarity</a></li>
<li><a href="#4-tf-idf-weighing-terms-in-the-vector">4. TF-IDF: Weighing terms in the vector</a><ul>
<li><a href="#41-tf">4.1 TF</a></li>
<li><a href="#42-idf">4.2 IDF</a></li>
<li><a href="#43-tf-idf">4.3 TF-IDF</a></li>
</ul>
</li>
<li><a href="#5pointwise-mutual-information-pmi">5.Pointwise Mutual Information (PMI)</a></li>
<li><a href="#6-word2vec">6. Word2vec</a><ul>
<li><a href="#61-the-classifier">6.1 The classifier</a></li>
<li><a href="#62-learning-skip-gram-embeddings">6.2 Learning skip-gram embeddings</a></li>
<li><a href="#63-other-kinds-of-static-embeddings">6.3 Other kinds of static embeddings</a></li>
</ul>
</li>
<li><a href="#7-bias-and-embeddings">7. Bias and Embeddings</a></li>
<li><a href="#8-questions">8. Questions</a></li>
</ul>
</li>
</ul>
<p><br></p>
<p><Br><br><br></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>传统的NLP应用中，词语通常仅以字符串或在词汇表中的索引形式存在，这种表征方式未能捕捉到词义的深层含义。例如，将“狗”的意义仅表示为DOG，这显然是不够的。我们期望词义模型能够表达词义之间的相似性、反义关系、情感色彩等</p>
<p>在词汇语义学中，词元(lemma)和词义(sense)是基础概念，一个词元可以有多个词义，例如“mouse”既可以指老鼠，也可以指鼠标。</p>
<p><strong>同义词</strong>是词义关系中的重要组成部分，它涉及到两个词义几乎或完全相同的情况，如“sofa”与“couch”。除了同义关系，词语之间的<strong>相似性</strong>也是一个重要的研究领域，相似性关系帮助我们理解词语之间的近似关系，而非严格的同义关系，这对于语义相关任务如问答、改写和摘要等非常关键。与词义相似容易混淆的是<strong>词义相关性</strong>，它关注的是词语在特定事件或情境中的关联性，如“coffee”和“cup”的关系。</p>
<p>最后，词语的<strong>情感含义</strong>或内涵反映了词语在情感、评价、意见方面的特征。这对于情感分析、立场检测等NLP应用领域具有重要意义。</p>
<p><br></p>
<Br>


<h2 id="1-Vector-Semantics"><a href="#1-Vector-Semantics" class="headerlink" title="1. Vector Semantics"></a>1. Vector Semantics</h2><p>在前一节中讨论了词义的许多方面，<strong>Vector Semantics</strong> 是 NLP 中表示词义的标准方法，他的理念是将一个词表示为多维语义空间中的一个点，这个空间是根据词的分布派生出来的，而表示词的 vector 被称为<strong>Embeddings</strong>。<br><img src="https://pbs.twimg.com/media/GH_OheSXgAAznPs?format=jpg&amp;name=medium" alt=""></p>
<h2 id="2-Words-and-Vectors"><a href="#2-Words-and-Vectors" class="headerlink" title="2.Words and Vectors"></a>2.Words and Vectors</h2><p>在 Vector Semantics 中，单词共同出现频率通常使用基于共现矩阵的模型来表示，我们主要关注两种矩阵：<strong>term-document矩阵</strong>和<strong>term-term矩阵</strong>。</p>
<h3 id="2-1-Words-as-vectors-document-dimensions"><a href="#2-1-Words-as-vectors-document-dimensions" class="headerlink" title="2.1 Words as vectors: document dimensions"></a>2.1 Words as vectors: document dimensions</h3><p>在<strong>term-document矩阵</strong>中，每一行表示词汇表中的一个单词，每一列表示某个文档集合中的一个文档，这种矩阵展示了在给定文档中每个词出现的次数。因此，这两个向量的第一个维度对应于单词”battle”出现的次数，我们可以比较每个维度，例如，注意到”As You Like It”和”Twelfth Night”的向量在第一个维度(battle)上具有相似的值(分别为1和0),…</p>
<p>举例：下面展现了四部莎士比亚戏剧中四个单词的term-document矩阵：<br><img src="https://pbs.twimg.com/media/GH_QxmyWsAA5len?format=jpg&amp;name=medium" alt=""></p>
<h3 id="2-2-Words-as-vectors-word-dimensions"><a href="#2-2-Words-as-vectors-word-dimensions" class="headerlink" title="2.2 Words as vectors: word dimensions"></a>2.2 Words as vectors: word dimensions</h3><p>与使用term-document矩阵表示词向量的文档计数不同，我们还可以使用term-term矩阵，也称为term-term矩阵或term-context矩阵，其中<strong>列是词而不是文档</strong>，这种矩阵记录了在某个训练语料中行词(target)和列词(contaxt)共现的次数。</p>
<p>举例：下面是从维基百科语料库中计算的四个词的term-term矩阵，可以看出单词”digital”在单词”aardvark”附近出现了0次但在单词”computer”附近出现了1670次,这反映了<strong>词义相似性</strong>的概念.<br><img src="https://pbs.twimg.com/media/GH_U5wbXEAAyXu7?format=jpg&amp;name=medium" alt=""></p>
<h2 id="3-Cosine-for-measuring-similarity"><a href="#3-Cosine-for-measuring-similarity" class="headerlink" title="3. Cosine for measuring similarity"></a>3. Cosine for measuring similarity</h2><p>余弦相似度是衡量两个目标词v 和 w 之间相似度的常用度量指标。它通过计算两个同维度向量之间角度的余弦值并除两个向量的长度来给出它们的相似度，其计算公式为：</p>
<script type="math/tex; mode=display">\text{cosine}(v, w) = \frac{v \cdot w}{|v||w|} = \frac{\sum_{i=1}^{N} v_i w_i}{\sqrt{\sum_{i=1}^{N} v_i^2} \sqrt{\sum_{i=1}^{N} w_i^2}}</script><p>余弦值的范围从1（两个向量方向完全相同）通过0（正交向量）到-1（两个向量方向完全相反）。但由于原始频率值非负，这些向量的余弦值范围从0到1。</p>
<h2 id="4-TF-IDF-Weighing-terms-in-the-vector"><a href="#4-TF-IDF-Weighing-terms-in-the-vector" class="headerlink" title="4. TF-IDF: Weighing terms in the vector"></a>4. TF-IDF: Weighing terms in the vector</h2><p>上面的共现矩阵用频率表示每个单元格,但频率并不是衡量单词之间关联的最佳指标。例如，我们无法很好地区分像the、it或they这样的词，它们频繁地出现在各种单词中，并不能提供任何特定单词的信息。有两种方法来解决上面的问题：TF-IDF(适用于term-document矩阵)和PMI(适用于term-term矩阵)，接下来我们要讲的TF-IDF这种方法。</p>
<p>TF-IDF（Term Frequency-Inverse Document Frequency）是在向量中衡量词项重要性的一种方法，它结合了词频（TF）和逆文档频率（IDF）两个指标。TF-IDF旨在减少常用词对文档相似度计算的影响，同时强调对文档内容有较好区分度的词</p>
<h3 id="4-1-TF"><a href="#4-1-TF" class="headerlink" title="4.1 TF"></a>4.1 TF</h3><p>词频是词项在文档中出现的频率，公式为：</p>
<script type="math/tex; mode=display">\text{tf}(t, d) = 1 + \log_{10}(\text{count}(t, d))</script><p>其中，$\text{count}(t, d)$是单词t在文件d中出现的次数，<strong>如果词项在文档中未出现，则词频为0</strong>。这种处理方式是基于出现次数较多的词项对文档意义的影响并不是线性增长的这一观点。</p>
<h3 id="4-2-IDF"><a href="#4-2-IDF" class="headerlink" title="4.2 IDF"></a>4.2 IDF</h3><p>逆文档频率是用来减少在整个文档集合中出现频率高的词项的权重的指标。它是基于一个词项在多少不同的文档中出现来计算的，旨在给予罕见词更高的权重。IDF的计算公式为：</p>
<script type="math/tex; mode=display">\text{idf}(t) = \log_{10}\left(\frac{N}{\text{df}(t)}\right)</script><p>其中，N 是文档集合中文档的总数，$\text{df}(t)$ 是包含词项 t 的文档数。如果一个词项在许多文档中出现，它的IDF值会变小，反之则会变大。</p>
<p><br></p>
<h3 id="4-3-TF-IDF"><a href="#4-3-TF-IDF" class="headerlink" title="4.3 TF-IDF"></a>4.3 TF-IDF</h3><p>一个词项在特定文档中的TF-IDF加权值是其TF值和IDF值的乘积：</p>
<script type="math/tex; mode=display">\text{w}(t, d) = \text{tf}(t, d) \times \text{idf}(t)</script><p>这个加权值越高，词项在文档中的重要性就越大。TF-IDF加权有效地平衡了词项在文档中的局部重要性和在整个文档集合中的区分度。因此，TF-IDF是信息检索和文本挖掘中常用的特征加权技术，广泛应用于文档相似性计算、文本分类和搜索引擎的相关性评分等。</p>
<h2 id="5-Pointwise-Mutual-Information-PMI"><a href="#5-Pointwise-Mutual-Information-PMI" class="headerlink" title="5.Pointwise Mutual Information (PMI)"></a>5.Pointwise Mutual Information (PMI)</h2><p>点对互信息（PMI）是用于term-term矩阵的另一种权重函数，适用于向量维度对应于词而非文档的情况。PMI基于这样一种思路：衡量两个词之间关联的最佳方式是询问这两个词在语料库中共同出现的频率超出了我们预先期望它们偶然出现的频率多少。</p>
<p>PMI是衡量两个事件 x 和 y共同发生的频率与它们独立发生的预期频率之比的度量。对于目标词 w 和上下文词 c之间的PMI定义为：</p>
<script type="math/tex; mode=display">\text{PMI}(w, c) = \log_2 \frac{P(w, c)}{P(w)P(c)}</script><p>分子告诉我们我们观察到两个词一起出现的频率,分母告诉我们假设每个词独立出现时，我们期望两个词共同出现的频率；然而，PMI的值范围从负无穷到正无穷,负的PMI值（意味着事物比我们通过偶然预期的出现得更少）除非我们的语料库非常大，否则往往不可靠。因此，更常见的做法是使用正的点对互信息（PPMI），它将所有负的PMI值替换为零：</p>
<script type="math/tex; mode=display">\text{PPMI}(w, c) = \max\left(\log_2 \frac{P(w, c)}{P(w)P(c)}, 0\right)</script><p>通过一个共现矩阵 F，我们可以将它转换为PPMI矩阵，其中$PPMI_{ij}$ 给出了词 $w_i$ 与上下文 $c_j$的PPMI值。</p>
<h2 id="6-Word2vec"><a href="#6-Word2vec" class="headerlink" title="6. Word2vec"></a>6. Word2vec</h2><p>Word2vec，特别是其skip-gram和SGNS，代表了一种生成密集词向量的方法(<br>embedding)。与之前讨论的稀疏、长向量不同，embedding是短而密集的向量，其维度 d 通常介于50到1000之间，而非词汇量 $|V|$ 或文档数量 D的规模。</p>
<p>密集向量在每项NLP任务中的表现都优于稀疏向量。虽然我们不完全理解这背后的所有原因，但实验表示，使用300维的密集向量表示词语，比起使用50,000维的向量，需要学习的参数更少，这可能有助于泛化并避免过拟合，可能也更好地捕捉同义词之间的关系。</p>
<h3 id="6-1-The-classifier"><a href="#6-1-The-classifier" class="headerlink" title="6.1 The classifier"></a>6.1 The classifier</h3><p>我们的目标是训练一个分类器，让其判断一个候选的上下文词 c是否为目标词 w 的真实上下文。例如，对于给定的目标词对组合（apricot, jam）或（apricot, aardvark），分类器将返回c是w真实上下文词的概率 $P(+|w, c)$。</p>
<p><strong>Skip-gram模型的思路</strong>：如果一个词的embedding向量与目标embedding向量相似，那么这个词很可能出现在目标词附近。为了计算这些密集嵌入之间的相似度，我们依赖于一个想法，即如果两个向量进行点积运算后的逻辑回归值(为了映射到0~1)高，那么它们相似。其公式如下：</p>
<script type="math/tex; mode=display">P(+|w,c) = \sigma(c \cdot w) = \frac{1}{1 + \exp(-c \cdot w)}</script><p>Skip-gram模型还做了一个简化的假设，即所有上下文词都是独立的，允许我们简单地将它们的概率相乘：</p>
<script type="math/tex; mode=display">\log P(+|w,c_{1:L}) = \sum_{i=1}^{L} \log \sigma(c_i \cdot w)</script><p>总结来说，Skip-gram模型训练了一个概率分类器，给定一个测试目标词 w 及其上下文窗口 $L$个词 $c_{1:L}$，基于这个上下文窗口与目标词的相似度分配概率。</p>
<h3 id="6-2-Learning-skip-gram-embeddings"><a href="#6-2-Learning-skip-gram-embeddings" class="headerlink" title="6.2 Learning skip-gram embeddings"></a>6.2 Learning skip-gram embeddings</h3><p>Skip-gram模型的学习算法以文本语料和选择的词汇表大小 N 作为输入。它首先为词汇表中的每个词随机分配一个embedding向量，然后迭代地调整每个词 w的嵌入，使其与文本中出现的近邻词的嵌入更相似。</p>
<p>对于单个训练数据来说，Skip-gram模型训练了一个二元分类器，需要正例和负例。对于每一个目标词 w和正例上下文词 $c_{\text{pos}}$ 的组合，我们将创建 k个负样本，每个负样本由目标词 w加上一个”噪声词” $c_{\text{neg}}$ 组成。噪声词是从词典中随机选取的，不是目标词 w,且负样本多于正样本，比例由参数 k 设置。</p>
<p>具体例子如下：<br><img src="https://pbs.twimg.com/media/GH_iN4WWkAADtHE?format=png&amp;name=medium" alt=""></p>
<p><img src="https://pbs.twimg.com/media/GH_iSRxWkAELL7p?format=png&amp;name=medium" alt=""></p>
<p>给定正负训练实例集和初始嵌入集合，学习算法的目标是调整这些嵌入以最大化正例中目标词和上下文词对（$w,c_{\text{pos}}$）的相似度，同时最小化负例中的（$w,c_{\text{neg}}$）对的相似度。如果我们考虑一个词/上下文对（$w, c_{\text{pos}}$）及其 k个噪声词 $c_{\text{neg1}} … c_{\text{negk}}$，我们可以将这两个目标表达为一个要最小化的损失函数 L，其中第一项表达了我们希望分类器将真实上下文词 $c_{\text{pos}}$ 分配高概率为邻居，第二项表达了我们希望为每个噪声词 $c_{\text{negi}}$ 分配高概率为非邻居。</p>
<p><img src="https://pbs.twimg.com/media/GH_lAPyWAAAHvy9?format=jpg&amp;name=medium" alt=""></p>
<p>最后使用随机梯度下降法来最小化这个损失函数，获得最优解。</p>
<h3 id="6-3-Other-kinds-of-static-embeddings"><a href="#6-3-Other-kinds-of-static-embeddings" class="headerlink" title="6.3 Other kinds of static embeddings"></a>6.3 Other kinds of static embeddings</h3><p>目前为止word2vec的一个问题,即它没有很好的方法来处理未知单词，FastText通过使用子词模型来处理这些问题，它将每个词表示为自身加上一个由构成词的n-gram组成的包，并在每个词的开头和结尾添加特殊边界符号 &lt; 和 &gt;。</p>
<p><img src="https://pbs.twimg.com/media/GH_mQDyX0AA5UCe?format=png&amp;name=medium" alt=""></p>
<p>例如，当 n=3 时，单词 “where” 将由序列 <where> 加上字符 n-gram <wh, whe, her, ere, re> 来表示。然后为每个构成的n-gram学习一个跳字嵌入，单词 “where” 由其所有构成n-gram的嵌入之和来表示。未知词则可以仅由构成n-gram的和来表示。</p>
<h2 id="7-Bias-and-Embeddings"><a href="#7-Bias-and-Embeddings" class="headerlink" title="7. Bias and Embeddings"></a>7. Bias and Embeddings</h2><p>嵌入编码了人类推理的隐性关联性质。隐性关联测试通过测量人们标记各种类别词语的延迟差异来衡量人们对概念（如“花”或“昆虫”）和属性（如“愉悦”和“不愉悦”）之间的关联。使用这种方法，美国人已被证明将非裔美国人的名字与不愉悦的词联系在一起（比欧裔美国人的名字更多），男性名字更多与数学联系在一起，女性名字与艺术联系在一起，老年人的名字与不愉悦的词联系在一起。</p>
<p>最近的研究专注于尝试消除这些偏见的方法，例如通过开发转换嵌入空间的方法来去除性别刻板印象，但保留定义上的性别（Bolukbasi等人，2016；Zhao等人，2017），或改变训练程序（Zhao等人，2018）。然而，尽管这些去偏见的方法可能减少了嵌入中的偏见，它们并没有消除它（Gonen和Goldberg，2019），这仍然是一个开放的问题。</p>
<h2 id="8-Questions"><a href="#8-Questions" class="headerlink" title="8. Questions"></a>8. Questions</h2><p>对于一词多义，在空间上是同一位置吗，如果是同一位置如何区分不同的语义呢？</p>
<p>如果是多语言任务，如何调整不同语言的Embeddings呢？</p>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2024/03/05/NLP-%E9%98%85%E8%AF%BB%E4%BD%9C%E4%B8%9A1-JMBook-Chap-6/" data-id="cltfe9yzp0001os692ygmeqg5" data-title="NLP:阅读作业1-JMBook Chap 6"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/chapter6/" rel="tag">chapter6</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/jmbook/" rel="tag">jmbook</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" rel="tag">专业知识</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://pbs.twimg.com/media/GH91GXWW4AAwR9A?format=jpg&amp;name=medium" data-sizes="auto" alt="[待完成]NLP:Chapter01-语言处理单元的获取"
                         class="lazyload">
                
                <a href="/2024/03/06/NLP-Chapter01-%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%8D%95%E5%85%83%E7%9A%84%E8%8E%B7%E5%8F%96/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        [待完成]NLP:Chapter01-语言处理单元的获取
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://pbs.twimg.com/media/GIC6VEQbMAAk8RC?format=png&amp;name=medium" data-sizes="auto" alt=" Missing Semester of CS:Lecture02-Shell工具与脚本"
                         class="lazyload">
                
                <a href="/2024/03/05/Missing-Semester-of-CS-Lecture02-Shell%E5%B7%A5%E5%85%B7%E4%B8%8E%E8%84%9A%E6%9C%AC/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                         Missing Semester of CS:Lecture02-Shell工具与脚本
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="ab" class="lazyload">
            <div class="sidebar-author-name">ab</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">323</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">31</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">367</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/74X2u8JMVooG2QbjRxXwR8?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Competition/">Competition</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Future/">Future</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/ItWorks-SNN/">ItWorks-SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/boring-SNN/">boring-SNN</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS224N/">CS224N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS231N/">CS231N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/Missing-Semester-of-CS/">Missing Semester of CS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/">智能信息网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/">语音信息处理</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 11.67px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/17/" style="font-size: 10px;">17</a> <a href="/tags/2/" style="font-size: 12.22px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 11.11px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/4/" style="font-size: 11.11px;">4</a> <a href="/tags/5/" style="font-size: 10.56px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 11.11px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 17.78px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.11px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 13.89px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 11.67px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10.56px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 11.67px;">CNN</a> <a href="/tags/CS224N/" style="font-size: 10px;">CS224N</a> <a href="/tags/CS231N/" style="font-size: 10px;">CS231N</a> <a href="/tags/CV/" style="font-size: 10.56px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 12.78px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/ComPetition/" style="font-size: 10px;">ComPetition</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 15.56px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.56px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Filter/" style="font-size: 10px;">Filter</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/Future/" style="font-size: 12.78px;">Future</a> <a href="/tags/GB/" style="font-size: 10px;">GB</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GiB/" style="font-size: 10px;">GiB</a> <a href="/tags/Git/" style="font-size: 10.56px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.11px;">GoAbroad</a> <a href="/tags/Graduate/" style="font-size: 10px;">Graduate</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 10.56px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/ItWorks/" style="font-size: 10px;">ItWorks</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/LayerNorm/" style="font-size: 10px;">LayerNorm</a> <a href="/tags/Lec01/" style="font-size: 11.11px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.56px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.67px;">Linux</a> <a href="/tags/M2/" style="font-size: 10.56px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.22px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 13.89px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.56px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 10.56px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NLP/" style="font-size: 11.11px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 17.22px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/OS/" style="font-size: 13.89px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.56px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10.56px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.11px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.22px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SNNNLP/" style="font-size: 10px;">SNNNLP</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Script/" style="font-size: 10px;">Script</a> <a href="/tags/Shell/" style="font-size: 10.56px;">Shell</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-Neural-Network-for-Ultra-low-latency-and-High-accurate-Object-Detection/" style="font-size: 10px;">Spiking Neural Network for Ultra-low-latency and High-accurate Object Detection</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.56px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.56px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/Undergraduate/" style="font-size: 10px;">Undergraduate</a> <a href="/tags/University/" style="font-size: 12.78px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 11.11px;">ViT</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.56px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/alexnet/" style="font-size: 10px;">alexnet</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 11.67px;">bert</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/boring/" style="font-size: 11.11px;">boring</a> <a href="/tags/bug/" style="font-size: 16.67px;">bug</a> <a href="/tags/cat/" style="font-size: 10px;">cat</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.11px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.56px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.56px;">chapter05</a> <a href="/tags/chapter6/" style="font-size: 10px;">chapter6</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/chmod/" style="font-size: 10px;">chmod</a> <a href="/tags/chrome/" style="font-size: 10px;">chrome</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/code/" style="font-size: 11.11px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/commit/" style="font-size: 10px;">commit</a> <a href="/tags/competition/" style="font-size: 10px;">competition</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/copilot/" style="font-size: 10.56px;">copilot</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.33px;">d2l</a> <a href="/tags/database/" style="font-size: 13.89px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.56px;">deep neural network</a> <a href="/tags/delete/" style="font-size: 10px;">delete</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/dowhy/" style="font-size: 10.56px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10.56px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/embedding/" style="font-size: 10px;">embedding</a> <a href="/tags/explainer/" style="font-size: 10.56px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/github/" style="font-size: 12.22px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 10.56px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10.56px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/import/" style="font-size: 10px;">import</a> <a href="/tags/instructor/" style="font-size: 11.67px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 11.67px;">intern00</a> <a href="/tags/internship/" style="font-size: 18.89px;">internship</a> <a href="/tags/interview/" style="font-size: 10px;">interview</a> <a href="/tags/introduction/" style="font-size: 11.11px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/jmbook/" style="font-size: 10px;">jmbook</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.56px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 11.11px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 12.22px;">mac</a> <a href="/tags/memory/" style="font-size: 11.11px;">memory</a> <a href="/tags/mentor/" style="font-size: 10.56px;">mentor</a> <a href="/tags/mid/" style="font-size: 10.56px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.56px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.56px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 14.44px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.11px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.44px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 11.67px;">prml</a> <a href="/tags/profile/" style="font-size: 10px;">profile</a> <a href="/tags/project/" style="font-size: 11.67px;">project</a> <a href="/tags/pycharm/" style="font-size: 10px;">pycharm</a> <a href="/tags/pytorch/" style="font-size: 13.89px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10.56px;">reading</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 14.44px;">review</a> <a href="/tags/rf/" style="font-size: 10px;">rf</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/server/" style="font-size: 10px;">server</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/snn/" style="font-size: 11.11px;">snn</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/sora/" style="font-size: 10px;">sora</a> <a href="/tags/spike/" style="font-size: 10.56px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.56px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikebert/" style="font-size: 10px;">spikebert</a> <a href="/tags/spikingjelly/" style="font-size: 12.22px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.56px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.56px;">ssh</a> <a href="/tags/terminal/" style="font-size: 10px;">terminal</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.56px;">tips</a> <a href="/tags/tool/" style="font-size: 18.33px;">tool</a> <a href="/tags/transformer/" style="font-size: 12.78px;">transformer</a> <a href="/tags/transformers/" style="font-size: 10px;">transformers</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10.56px;">vscode</a> <a href="/tags/wakatime/" style="font-size: 10px;">wakatime</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">专业知识</a> <a href="/tags/%E4%B8%93%E7%A1%95/" style="font-size: 10px;">专硕</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E4%BB%A3%E7%90%86/" style="font-size: 10px;">代理</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">公选课</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.56px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" style="font-size: 10px;">博弈论</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">图像描述生成</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">大作业</a> <a href="/tags/%E5%A4%A7%E5%88%9B/" style="font-size: 10px;">大创</a> <a href="/tags/%E5%A4%A7%E8%8B%B1%E8%B5%9B/" style="font-size: 10px;">大英赛</a> <a href="/tags/%E5%AD%A6%E7%A1%95/" style="font-size: 10px;">学硕</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.56px;">审稿意见</a> <a href="/tags/%E5%B0%8F%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">小作业</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%82%84%E6%82%84%E8%AF%9D/" style="font-size: 10px;">悄悄话</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%99%E8%82%B2%E8%AE%B8%E5%8F%AF/" style="font-size: 10px;">教育许可</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">显卡</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.56px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">智能信息网络</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 13.89px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.56px;">服务器</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期中复习</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 11.67px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.56px;">李宏毅</a> <a href="/tags/%E6%9D%8E%E6%B2%90/" style="font-size: 10px;">李沐</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">模型训练流程</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 12.78px;">毛概</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.56px;">特征学习</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">第三章</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3/" style="font-size: 10px;">脑机接口</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">评测指标</a> <a href="/tags/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/" style="font-size: 10px;">语音信息处理</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">课程</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.11px;">调研</a> <a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">贝叶斯</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">软件概要设计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%87%8F%E5%8C%96/" style="font-size: 10px;">量化</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                ab
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    655.1k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    41:30
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="ab" class="lazyload">
            <div class="sidebar-author-name">ab</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">323</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">31</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">367</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

