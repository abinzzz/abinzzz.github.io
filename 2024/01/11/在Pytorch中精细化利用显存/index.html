<!DOCTYPE html>

<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <!--
        hexo-theme-suka © SukkaW
        GitHub: https://github.com/SukkaW/hexo-theme-suka
    -->

    <!-- ### Resource Hint ### -->

    <!-- ## DNS Prefetch ## -->
    <meta http-equiv="x-dns-prefetch-control" content="on">

<!-- busuanzi -->

    <link rel="dns-prefetch" href="//busuanzi.ibruce.info">


<!-- comment -->


    <link rel="dns-prefetch" href="//disqus.com">
    <link rel="dns-prefetch" href="//robin02.disqus.com">






<!-- analytics -->







    <!-- ## Preload ## -->
    
    <!-- Busuanzi -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" as="script">







    <!-- ### Meta & Title & Info ### -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <meta name="renderer" content="webkit">

    <!-- Title -->
    <title>在Pytorch中精细化利用显存 | blog</title>

    <!-- Favicons -->
    <link rel="icon" type="image&#x2F;ico" href="/img/blog.ico">

    <!-- ### Import File ### -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spectre.css@0.5.3"><style>
    body {
        background-color: #f8f9fa;
    }

    a, a:visited {
        color: blue;
    }

    a:active, a:focus, a:hover {
        color: blue;
        opacity: .75;
    }

    #post-content a,
    #post-content a:hover,
    #post-content a:focus,
    #post-content a:visited {
        color: blue;
        opacity: 1;
    }

    

    .post-entry .card-body a {
        color: red;
    }

    .avatar {
        background: red;
    }

    .navbar-link,
    .navbar-link:visited,
    .timeline .timeline-item .timeline-icon.icon-lg {
        color: red;
    }

    .navbar-link:hover {
        color: red;
        opacity: .8;
    }

    #search-input .btn,
    #disqus_click_btn,
    #disqus-switch-to-direct,
    #disqus-loadmore-button {
        background: red;
        border-color: red;
        color: #fff;
    }

    #post-toc a.post-toc-link,
    #post-toc a.post-toc-link:visited,
    .share-menu.menu .menu-item>a {
        color: red;
    }

    .share-menu.menu .menu-item>a:hover,
    .share-menu.menu .menu-item>a:focus,
    .share-menu.menu .menu-item>a:visited {
        color: #50596c;
        background: #f8f9fa;
        opacity: .85;
    }
</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/style.min.css">








    <!-- Prettify Theme -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css"></noscript>





<script>
/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
!function(t){"use strict";t.loadCSS||(t.loadCSS=function(){});var e=loadCSS.relpreload={};if(e.support=function(){var e;try{e=t.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),e.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.addEventListener?t.removeEventListener("load",a):t.attachEvent&&t.detachEvent("onload",a),t.setAttribute("onload",null),t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},e.poly=function(){if(!e.support())for(var a=t.document.getElementsByTagName("link"),n=0;n<a.length;n++){var o=a[n];"preload"!==o.rel||"style"!==o.getAttribute("as")||o.getAttribute("data-loadcss")||(o.setAttribute("data-loadcss",!0),e.bindMediaToggle(o))}},!e.support()){e.poly();var a=t.setInterval(e.poly,500);t.addEventListener?t.addEventListener("load",function(){e.poly(),t.clearInterval(a)}):t.attachEvent&&t.attachEvent("onload",function(){e.poly(),t.clearInterval(a)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:t.loadCSS=loadCSS}("undefined"!=typeof global?global:this);
</script>

    <!-- ### Site Verification ### -->
    


    <meta name="mobile-web-app-capable" content="yes"><meta name="application-name" content="blog"><meta name="msapplication-starturl" content="https://abinzzz.github.io"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="search" type="application/opensearchdescription+xml" href="opensearch.xml" title="blog">

    <!-- ### The Open Graph & Twitter Card Protocol ### -->
    <meta property="og:title" content="在Pytorch中精细化利用显存 | blog"><meta property="og:site_name" content="blog"><meta property="og:type" content="article"><meta property="og:url" content="https://abinzzz.github.io/2024/01/11/%E5%9C%A8Pytorch%E4%B8%AD%E7%B2%BE%E7%BB%86%E5%8C%96%E5%88%A9%E7%94%A8%E6%98%BE%E5%AD%98/"><meta property="og:locale" content="zh-CN"><meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;, &amp;apos;$&amp;apos;]]}, messageStyle: &quot;none&quot; });   在Pytorch中精细化利用显存  前言 在上篇文章《计算模型以及中间变量的显存占用大小》中我们对如何计算各种变量所占显存大小进行了一些探索。而这篇文章我们着重讲解如何利用Pytorch深度学习框架的一些特性,去查看我们当前使用的 - ab - blog"><meta name="keywords" content="Accumulate, pytorch, memory, blog"><meta property="og:image" content="https://pbs.twimg.com/media/GDfu20gbMAEIF3C?format=jpg&amp;name=medium"><meta property="article:published_time" content="2024-01-10T16:25:17.000Z"><meta property="article:modified_time" content="2024-01-10T16:55:03.299Z"><meta property="og:updated_time" content="2024-01-10T16:55:03.299Z"><meta property="article:author" content="ab"><meta property="article:tag" content="Accumulate, pytorch, memory, blog"><meta name="twitter:card" content="summary">

    

    <!-- ### Canonical link ### -->
    <link rel="canonical" href="https://abinzzz.github.io/2024/01/11/%E5%9C%A8Pytorch%E4%B8%AD%E7%B2%BE%E7%BB%86%E5%8C%96%E5%88%A9%E7%94%A8%E6%98%BE%E5%AD%98/">

    <meta name="generator" content="Hexo 5.4.2">

    <!-- ### Analytics ### -->
    







    <!-- ### Structured Data ### -->
    



<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://abinzzz.github.io/2024/01/11/%E5%9C%A8Pytorch%E4%B8%AD%E7%B2%BE%E7%BB%86%E5%8C%96%E5%88%A9%E7%94%A8%E6%98%BE%E5%AD%98/",
    "@type": "BlogPosting",
    "logo": "https://abinzzz.github.io/img/blog.ico",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://abinzzz.github.io/2024/01/11/%E5%9C%A8Pytorch%E4%B8%AD%E7%B2%BE%E7%BB%86%E5%8C%96%E5%88%A9%E7%94%A8%E6%98%BE%E5%AD%98/"
    },
    "headline": "在Pytorch中精细化利用显存 | blog",
    
    "image": {
        "@type": "ImageObject",
        "url": "https://abinzzz.github.io/img/blog.ico"
    },
    
    "datePublished": "2024-01-10T16:25:17.000Z",
    "dateModified": "2024-01-10T16:55:03.299Z",
    "author": {
        "@type": "Person",
        "name": "ab",
        "image": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/avatar.jpg"
        },
        "description": "Welcome to my blog!"
    },
    "publisher": {
        "@type": "Organization",
        "name": "blog",
        "logo": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/blog.ico"
        }
    },
    
    "potentialAction": {
        "@type": "SearchAction",
        "target": "https://abinzzz.github.io/search?s={search_term_string}",
        "query-input": "required name=search_term_string"
    },
    
    "keywords": "Accumulate, pytorch, memory, blog",
    "description": "MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;, &amp;apos;$&amp;apos;]]}, messageStyle: &amp;quot;none&amp;quot; });   在Pytorch中精细化利用显存  前言 在上篇文章《计算模型以及中间变量的显存占用大小》中我们对如何计算各种变量所占显存大小进行了一些探索。而这篇文章我们着重讲解如何利用Pytorch深度学习框架的一些特性,去查看我们当前使用的 - ab - blog"
}
</script>



    <!-- ### Custom Head ### -->
    
</head>

    <body>
            

            <!-- ### Main content ### -->
            <!-- ## Header ##-->
<header>
    <h1 class="header-title text-center"><a href="/">blog</a></h1>

    <p class="text-center header-slogan">
        
            
                Welcome to my blog!
            
        
    </p>

    <nav class="navbar-section text-center">
    
        <a href="/" class="navbar-link">首页</a>
    
    
        <a href="/archives/" class="navbar-link">归档</a>
    
    
        <a href="/search" class="navbar-link">搜索</a>
    
    
    
    
</nav>
</header>

            
    <!-- ## Post ## -->
    <div class="post-container">
    <div id="post-card" class="card">
        
        <div class="card-item-container">
            <div class="card-inner-cell">
                <!-- # Post Header Info # -->
                <div class="card-header">
                    
    <h1 class="card-title h3 mb-2">在Pytorch中精细化利用显存</h1>




<div class="post-header-info">
    <p class="post-header-info-left text-gray">
        <img class="author-thumb lazyload" data-src="/img/avatar.jpg" src="/img/suka-lazyload.gif" alt="ab's Avatar">
        <span>2024-01-11</span>
        
            <span class="suka-devide-dot"></span>
            <a class="category-link" href="/categories/Accumulate/">Accumulate</a>
        
        
        
    </p>
    <div class="post-header-info-right">
        
            <div class="dropdown dropdown-right">
<a class="dropdown-toggle" tabindex="0">分享本文</a>
<ul class="menu share-menu">
    <!-- Share Weibo -->
    

    <!-- Share Twitter -->
    

    <!-- Share Facebook -->
    

    <!-- Share Google+ -->
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    

    <!-- Share Telegram -->
    

    <!-- QRCode -->
    
    <li class="menu-item">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAANQAAADUCAAAAADBtVLEAAADsUlEQVR42u3aQXIbMQxEUd//0skqi7iG6N+yrIFUX5vE8ojicxVJEMDXnw98fYkSJUqUqI9DfQ2vf7///uzV+1fPfP/dfxO4GOv7d59+dzVPUaI2oU5fejXoaeJXP5+eT7jTGNM8RYnahqIbxDTo9Ec4TewKn8Y4zlOUqDdATQfydDCnQzrBTp8VJerdUSmwnA7tFCSfxqHjihK1GTUGivDQnA7rq5/JwV3NU5SoJSi6qO/+9+nZJFGifgEVk+7h8kgvhOmimC6jT616iBL1i6gpWUkOzOkiOb1PMFMR4vLQFyVqAYpsCinZn7DTdzxSjDtuRqJELUNNizQdqmlS6QAmk49/XFGiFqBSkiMdouS9R8ZNm44oUVtRU2DaND81m0wqMKRxq8SLKFE3oUjjFQlsSZMHKSy0TY6iRG1DkQRl0yA1HZo/CWrHxJAoUUtQU9GaBK1N0oYUCZqA97hRiBJ1IyptAiS5mApmTbBMmrHqKF2UqBehpmQGWaQpsZI2HxLUkoNXlKhtqOnDdOJtsSAlPp9anRcl6kUoUpBukifN59KGQoNeUaI2oeglMCVOSNJ/SqTQhOr4nChRS1A0KZIORZIYTRdAUow7zk2UqAWolBihSUvS/EGDWTKncaMQJepGVFr4jxaX2+bhpknyeIiLErUI1TblksLC9Pm2UTGNL0rUFlRqvGonSeApudL8AauOF1GiXoiiTb8kAdoEx7QoRwJbUaI2osj/yQWxufgRJEroiBK1CNUeguT5lPRMSc36wilK1DIUXeC0uEAvoNPiby6sokRtQJFmqvbwbBs8aHEhbRiiRG1ANQnFtGHQRU2SmtPzdUArStQLUbQxlzR7TAkc8mxqSImN+KJELUFNSY60QJuJp0Q/bczCXWSiRN2EmooDaYHiZijYxE8ujUegKFELUTQBQy6CTYG8KRLEopsoUTeinrX4ScMwWfy0wDAGtKJELUHRpo0JkYJeOuZDTcSiRC1CkQscSazQZM5UjEsXRHz4ihJ1I4o2YZAGkJ9e/NJ3oKqHKFE3oXCRODxHEzhN0a0paosStQXVNHOQglqbaJmC6FSMQ11kokS9GNUUBNrglSRGf3L5FCVqK6rZIMjips/+6kYhStTNKJIEmTaWKSkzXezIhZSOJUrUO6CmwLUNWtPmMiUtUwAtStS7oVJikhSu2+YuWhgQJWojiiQUyUJvk5PkAom/R5SoJagm8UIaSsgmkpI6pFESFd1EiXox6pNeokSJEiXqY15/Ac5xMU2nueYPAAAAAElFTkSuQmCC" alt="QRCode">
    </li>
    

</ul>
</div>
        
    </div>
</div>
                </div>
                <div class="card-body">
                    
                        
                        
                            <div id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E5%9C%A8pytorch%E4%B8%AD%E7%B2%BE%E7%BB%86%E5%8C%96%E5%88%A9%E7%94%A8%E6%98%BE%E5%AD%98"><span class="post-toc-number">1.</span> <span class="post-toc-text"> 在Pytorch中精细化利用显存</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%89%8D%E8%A8%80"><span class="post-toc-number">1.1.</span> <span class="post-toc-text"> 前言</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E4%BC%98%E5%8C%96%E6%98%BE%E5%AD%98"><span class="post-toc-number">1.2.</span> <span class="post-toc-text"> 优化显存</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E4%BC%B0%E6%B5%8B%E6%A8%A1%E5%9E%8B%E6%89%80%E5%8D%A0%E7%9A%84%E5%86%85%E5%AD%98"><span class="post-toc-number">1.3.</span> <span class="post-toc-text"> 估测模型所占的内存</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%85%B3%E4%BA%8Einplacefalse"><span class="post-toc-number">1.4.</span> <span class="post-toc-text"> 关于inplace&#x3D;False</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E7%89%BA%E7%89%B2%E8%AE%A1%E7%AE%97%E9%80%9F%E5%BA%A6%E5%87%8F%E5%B0%91%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%E9%87%8F"><span class="post-toc-number">1.5.</span> <span class="post-toc-text"> 牺牲计算速度减少显存使用量</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E8%B7%9F%E8%B8%AA%E6%98%BE%E5%AD%98%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5"><span class="post-toc-number">1.6.</span> <span class="post-toc-text"> 跟踪显存使用情况</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%90%8E%E8%AE%B0"><span class="post-toc-number">1.7.</span> <span class="post-toc-text"> 后记</span></a></li></ol></li></ol></div>
                        
                    
                    <article id="post-content">
                        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h1 id="在pytorch中精细化利用显存"><a class="markdownIt-Anchor" href="#在pytorch中精细化利用显存"></a> 在Pytorch中精细化利用显存</h1>
<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> <strong>前言</strong></h2>
<p>在上篇文章《计算模型以及中间变量的显存占用大小》中我们对如何计算各种变量所占显存大小进行了一些探索。而这篇文章我们着重讲解如何利用Pytorch深度学习框架的一些特性,去查看我们当前使用的变量所占用的显存大小,以及一些优化工作。以下代码所使用的平台框架为Pytorch。</p>
<br>
<h2 id="优化显存"><a class="markdownIt-Anchor" href="#优化显存"></a> <strong>优化显存</strong></h2>
<p>在Pytorch中优化显存是我们处理大量数据时必要的做法,因为我们并不可能拥有无限的显存。显存是有限的,而数据是无限的,我们只有优化显存的使用量才能够最大化地利用我们的数据,实现多种多样的算法。</p>
<Br>
<h2 id="估测模型所占的内存"><a class="markdownIt-Anchor" href="#估测模型所占的内存"></a> <strong>估测模型所占的内存</strong></h2>
<p>上篇文章中说过,一个模型所占的显存无非是这<strong>两种</strong>:</p>
<ul>
<li>
<p><strong>模型权重参数</strong></p>
</li>
<li>
<p><strong>模型所储存的中间变量</strong></p>
</li>
</ul>
<p>其实权重参数一般来说并不会占用很多的显存空间,<strong>主要占用显存空间的还是计算时产生的中间变量</strong>,当我们定义了一个model之后,我们可以通过以下代码简单计算出这个模型权重参数所占用的数据量:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># model是我们在pytorch定义的神经网络层</span></span><br><span class="line"><span class="comment"># model.parameters()取出这个model所有的权重参数</span></span><br><span class="line">para = <span class="built_in">sum</span>([np.prod(<span class="built_in">list</span>(p.size())) <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()])</span><br></pre></td></tr></table></figure>
<p>假设我们有这样一个model:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (conv_1): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (relu_1): ReLU(inplace)</span><br><span class="line">  (conv_2): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (relu_2): ReLU(inplace)</span><br><span class="line">  (pool_2): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (conv_3): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>然后我们得到的<code>para</code>是<code>112576</code>,但是我们计算出来的仅仅是权重参数的“数量”,单位是B,我们需要转化一下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面的type_size是4,因为我们的参数是float32也就是4B,4个字节</span></span><br><span class="line"> <span class="built_in">print</span>(<span class="string">&#x27;Model &#123;&#125; : params: &#123;:4f&#125;M&#x27;</span>.<span class="built_in">format</span>(model._get_name(), para * type_size / <span class="number">1000</span> / <span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>这样就可以打印出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Model Sequential : params: <span class="number">0.450304</span>M</span><br></pre></td></tr></table></figure>
<p>但是我们之前说过一个神经网络的模型,不仅仅有权重参数还要计算中间变量的大小。怎么去计算,我们可以假设一个<code>输入变量</code>,然后将这个输入变量投入这个模型中,然后我们主动提取这些计算出来的中间变量:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model是我们加载的模型</span></span><br><span class="line"><span class="comment"># input是实际中投入的input(Tensor)变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用clone()去复制一个input,这样不会对input造成影响</span></span><br><span class="line">input_ = <span class="built_in">input</span>.clone()   </span><br><span class="line"><span class="comment"># 确保不需要计算梯度,因为我们的目的只是为了计算中间变量而已</span></span><br><span class="line">input_.requires_grad_(requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">mods = <span class="built_in">list</span>(model.modules())</span><br><span class="line">out_sizes = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(mods)):</span><br><span class="line">    m = mods[i]</span><br><span class="line">    <span class="comment"># 注意这里,如果relu激活函数是inplace则不用计算</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.ReLU):  </span><br><span class="line">        <span class="keyword">if</span> m.inplace:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    out = m(input_)</span><br><span class="line">    out_sizes.append(np.array(out.size()))</span><br><span class="line">    input_ = out</span><br><span class="line"></span><br><span class="line">total_nums = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(out_sizes)):</span><br><span class="line">    s = out_sizes[i]</span><br><span class="line">    nums = np.prod(np.array(s))</span><br><span class="line">    total_nums += nums</span><br></pre></td></tr></table></figure>
<p>上面得到的值是模型在运行时候产生所有的中间变量的“数量”,当然我们需要换算一下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印两种,只有 forward 和 foreward、backward的情况</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Model &#123;&#125; : intermedite variables: &#123;:3f&#125; M (without backward)&#x27;</span></span><br><span class="line">        .<span class="built_in">format</span>(model._get_name(), total_nums * type_size / <span class="number">1000</span> / <span class="number">1000</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Model &#123;&#125; : intermedite variables: &#123;:3f&#125; M (with backward)&#x27;</span></span><br><span class="line">        .<span class="built_in">format</span>(model._get_name(), total_nums * type_size*<span class="number">2</span> / <span class="number">1000</span> / <span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>因为在<code>backward</code>的时候所有的中间变量需要保存下来再来进行计算,所以我们在计算<code>backward</code>的时候,计算出来的中间变量需要乘个2。</p>
<p>然后我们得出,上面这个模型的中间变量需要的占用的显存,很显然,中间变量占用的值比模型本身的权重值多多了。如果进行一次backward那么需要的就更多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Model Sequential : intermedite variables: <span class="number">336.089600</span> M (without backward)</span><br><span class="line">Model Sequential : intermedite variables: <span class="number">672.179200</span> M (<span class="keyword">with</span> backward)</span><br></pre></td></tr></table></figure>
<p>我们总结一下之前的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型显存占用监测函数</span></span><br><span class="line"><span class="comment"># model:输入的模型</span></span><br><span class="line"><span class="comment"># input:实际中需要输入的Tensor变量</span></span><br><span class="line"><span class="comment"># type_size 默认为 4 默认类型为 float32 </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">modelsize</span>(<span class="params">model, <span class="built_in">input</span>, type_size=<span class="number">4</span></span>):</span><br><span class="line">    para = <span class="built_in">sum</span>([np.prod(<span class="built_in">list</span>(p.size())) <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters()])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Model &#123;&#125; : params: &#123;:4f&#125;M&#x27;</span>.<span class="built_in">format</span>(model._get_name(), para * type_size / <span class="number">1000</span> / <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    input_ = <span class="built_in">input</span>.clone()</span><br><span class="line">    input_.requires_grad_(requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    mods = <span class="built_in">list</span>(model.modules())</span><br><span class="line">    out_sizes = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(mods)):</span><br><span class="line">        m = mods[i]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.ReLU):</span><br><span class="line">            <span class="keyword">if</span> m.inplace:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        out = m(input_)</span><br><span class="line">        out_sizes.append(np.array(out.size()))</span><br><span class="line">        input_ = out</span><br><span class="line"></span><br><span class="line">    total_nums = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(out_sizes)):</span><br><span class="line">        s = out_sizes[i]</span><br><span class="line">        nums = np.prod(np.array(s))</span><br><span class="line">        total_nums += nums</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Model &#123;&#125; : intermedite variables: &#123;:3f&#125; M (without backward)&#x27;</span></span><br><span class="line">          .<span class="built_in">format</span>(model._get_name(), total_nums * type_size / <span class="number">1000</span> / <span class="number">1000</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Model &#123;&#125; : intermedite variables: &#123;:3f&#125; M (with backward)&#x27;</span></span><br><span class="line">          .<span class="built_in">format</span>(model._get_name(), total_nums * type_size*<span class="number">2</span> / <span class="number">1000</span> / <span class="number">1000</span>))</span><br></pre></td></tr></table></figure>
<p>当然我们计算出来的占用显存值仅仅是做参考作用,因为Pytorch在运行的时候需要额外的显存值开销,所以实际的显存会比我们计算的稍微大一些。</p>
<br>
<h2 id="关于inplacefalse"><a class="markdownIt-Anchor" href="#关于inplacefalse"></a> 关于inplace=False</h2>
<p>我们都知道激活函数Relu()有一个默认参数inplace,默认设置为False,当设置为True时,我们在通过relu()计算时的得到的新值不会占用新的空间而是直接覆盖原来的值,这也就是为什么当inplace参数设置为True时可以节省一部分内存的缘故。</p>
<p><img src="https://pbs.twimg.com/media/GDfu20gbMAEIF3C?format=jpg&amp;name=medium" alt="" /></p>
<br>
<h2 id="牺牲计算速度减少显存使用量"><a class="markdownIt-Anchor" href="#牺牲计算速度减少显存使用量"></a> <strong>牺牲计算速度减少显存使用量</strong></h2>
<p>在Pytorch-0.4.0出来了一个新的功能,可以将一个计算过程分成两半,也就是如果一个模型需要占用的显存太大了,我们就可以先计算一半,保存后一半需要的中间结果,然后再计算后一半。</p>
<p>也就是说,新的checkpoint允许我们只存储反向传播所需要的部分内容。如果当中缺少一个输出(为了节省内存而导致的),checkpoint将会从最近的检查点重新计算中间输出,以便减少内存使用(当然计算时间增加了):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入</span></span><br><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 假设我们有一个非常深的网络  </span></span><br><span class="line">layers = [nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)]</span><br><span class="line">model = nn.Sequential(*layers)</span><br><span class="line">output = model(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<p>上面的模型需要占用很多的内存,因为计算中会产生很多的中间变量。为此checkpoint就可以帮助我们来节省内存的占用了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先设置输入的input=&gt;requires_grad=True</span></span><br><span class="line"><span class="comment"># 如果不设置可能会导致得到的gradient为0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">1</span>, <span class="number">10</span>, requires_grad=<span class="literal">True</span>) </span><br><span class="line">layers = [nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要计算的层函数,可以看到我们定义了两个</span></span><br><span class="line"><span class="comment"># 一个计算前500个层,另一个计算后500个层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_first_half</span>(<span class="params">*args</span>):</span><br><span class="line">    x = args[<span class="number">0</span>]  </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers[:<span class="number">500</span>]:</span><br><span class="line">        x = layer(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_second_half</span>(<span class="params">*args</span>):</span><br><span class="line">    x = args[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers[<span class="number">500</span>:-<span class="number">1</span>]:</span><br><span class="line">        x = layer(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们引入新加的checkpoint</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.checkpoint <span class="keyword">import</span> checkpoint</span><br><span class="line"></span><br><span class="line">x = checkpoint(run_first_half, <span class="built_in">input</span>)</span><br><span class="line">x = checkpoint(run_second_half, x)</span><br><span class="line"><span class="comment"># 最后一层单独调出来执行  </span></span><br><span class="line">x = layers[-<span class="number">1</span>](x)</span><br><span class="line">x.<span class="built_in">sum</span>.backward()  <span class="comment"># 这样就可以了</span></span><br></pre></td></tr></table></figure>
<p>对于Sequential-model来说,因为Sequential()中可以包含很多的block,所以官方提供了另一个功能包:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">1</span>, <span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">layers = [nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)] </span><br><span class="line">model = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.checkpoint <span class="keyword">import</span> checkpoint_sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分成两个部分</span></span><br><span class="line">num_segments = <span class="number">2</span></span><br><span class="line">x = checkpoint_sequential(model, num_segments, <span class="built_in">input</span>)</span><br><span class="line">x.<span class="built_in">sum</span>().backward() <span class="comment"># 这样就可以了</span></span><br></pre></td></tr></table></figure>
<h2 id="跟踪显存使用情况"><a class="markdownIt-Anchor" href="#跟踪显存使用情况"></a> <strong>跟踪显存使用情况</strong></h2>
<p>显存的使用情况,在编写程序中我们可能无法精确计算,但是我们可以通过pynvml这个Nvidia的Python环境库和Python的垃圾回收工具,可以实时地打印我们使用的显存以及哪些Tensor使用了我们的显存。</p>
<p>类似于下面的报告:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 08-Jun-18-17:56:51-gpu_mem_prof</span><br><span class="line"></span><br><span class="line">At __main__ &lt;module&gt;: line 39                        Total Used Memory:399.4  Mb</span><br><span class="line">At __main__ &lt;module&gt;: line 40                        Total Used Memory:992.5  Mb</span><br><span class="line">+ __main__ &lt;module&gt;: line 40                         (1, 1, 682, 700)     1.82 M &lt;class &#x27;torch.Tensor&#x27;&gt;  </span><br><span class="line">+ __main__ &lt;module&gt;: line 40                         (1, 3, 682, 700)     5.46 M &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">At __main__ &lt;module&gt;: line 126                       Total Used Memory:1088.5 Mb  </span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (64, 64, 3, 3)       0.14 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (128, 64, 3, 3)      0.28 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (128, 128, 3, 3)     0.56 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;  </span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (64, 3, 3, 3)        0.00 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (256, 256, 3, 3)     2.25 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (512, 256, 3, 3)     4.5 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt; </span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (512, 512, 3, 3)     9.0 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (64,)                0.00 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (1, 3, 682, 700)     5.46 M &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (128,)               0.00 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (256,)               0.00 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt; </span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (512,)               0.00 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (3,)                 1.14 M &lt;class &#x27;torch.Tensor&#x27;&gt;</span><br><span class="line">+ __main__ &lt;module&gt;: line 126                        (256, 128, 3, 3)     1.12 M &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">None</span><br></pre></td></tr></table></figure>
<p>以下是相关的代码,目前代码依然有些地方需要修改,等修改完善好我会将完整代码以及使用说明放到github上:<a target="_blank" rel="noopener" href="https://github.com/Oldpan/Pytorch-Memory-Utils">https://github.com/Oldpan/Pytorch-Memory-Utils</a><br />
请大家多多留意。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> linecache</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> pynvml</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print_tensor_sizes = <span class="literal">True</span></span><br><span class="line">last_tensor_sizes = <span class="built_in">set</span>()</span><br><span class="line">gpu_profile_fn = <span class="string">f&#x27;<span class="subst">&#123;datetime.datetime.now():%d-%b-%y-%H:%M:%S&#125;</span>-gpu_mem_prof.txt&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if &#x27;GPU_DEBUG&#x27; in os.environ:</span></span><br><span class="line"><span class="comment">#     print(&#x27;profiling gpu usage to &#x27;, gpu_profile_fn)</span></span><br><span class="line"></span><br><span class="line">lineno = <span class="literal">None</span></span><br><span class="line">func_name = <span class="literal">None</span></span><br><span class="line">filename = <span class="literal">None</span></span><br><span class="line">module_name = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fram = inspect.currentframe()</span></span><br><span class="line"><span class="comment"># func_name = fram.f_code.co_name</span></span><br><span class="line"><span class="comment"># filename = fram.f_globals[&quot;__file__&quot;]</span></span><br><span class="line"><span class="comment"># ss = os.path.dirname(os.path.abspath(filename)) </span></span><br><span class="line"><span class="comment"># module_name = fram.f_globals[&quot;__name__&quot;]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpu_profile</span>(<span class="params">frame, event</span>):</span><br><span class="line">    <span class="comment"># it is _about to_ execute (!)</span></span><br><span class="line">    <span class="keyword">global</span> last_tensor_sizes</span><br><span class="line">    <span class="keyword">global</span> lineno, func_name, filename, module_name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> event == <span class="string">&#x27;line&#x27;</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># about _previous_ line (!)</span></span><br><span class="line">            <span class="keyword">if</span> lineno <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                pynvml.nvmlInit()</span><br><span class="line">                <span class="comment"># handle = pynvml.nvmlDeviceGetHandleByIndex(int(os.environ[&#x27;GPU_DEBUG&#x27;]))</span></span><br><span class="line">                handle = pynvml.nvmlDeviceGetHandleByIndex(<span class="number">0</span>)</span><br><span class="line">                meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)</span><br><span class="line">                line = linecache.getline(filename, lineno)</span><br><span class="line">                where_str = module_name+<span class="string">&#x27; &#x27;</span>+func_name+<span class="string">&#x27;:&#x27;</span>+<span class="string">&#x27; line &#x27;</span>+<span class="built_in">str</span>(lineno)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(gpu_profile_fn, <span class="string">&#x27;a+&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(<span class="string">f&quot;At <span class="subst">&#123;where_str:&lt;<span class="number">50</span>&#125;</span>&quot;</span> </span><br><span class="line">                            <span class="string">f&quot;Total Used Memory:<span class="subst">&#123;meminfo.used/<span class="number">1024</span>**<span class="number">2</span>:&lt;<span class="number">7.1</span>f&#125;</span>Mb\n&quot;</span>)</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> print_tensor_sizes <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">                        <span class="keyword">for</span> tensor <span class="keyword">in</span> get_tensors():</span><br><span class="line">                            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(tensor, <span class="string">&#x27;dbg_alloc_where&#x27;</span>):</span><br><span class="line">                                tensor.dbg_alloc_where = where_str</span><br><span class="line">                        new_tensor_sizes = &#123;(<span class="built_in">type</span>(x), <span class="built_in">tuple</span>(x.size()), np.prod(np.array(x.size()))*<span class="number">4</span>/<span class="number">1024</span>**<span class="number">2</span>,  </span><br><span class="line">                                             x.dbg_alloc_where) <span class="keyword">for</span> x <span class="keyword">in</span> get_tensors()&#125;</span><br><span class="line">                        <span class="keyword">for</span> t, s, m, loc <span class="keyword">in</span> new_tensor_sizes - last_tensor_sizes:</span><br><span class="line">                            f.write(<span class="string">f&#x27;+ <span class="subst">&#123;loc:&lt;<span class="number">50</span>&#125;</span> <span class="subst">&#123;<span class="built_in">str</span>(s):&lt;<span class="number">20</span>&#125;</span> <span class="subst">&#123;<span class="built_in">str</span>(m)[:<span class="number">4</span>]&#125;</span> M <span class="subst">&#123;<span class="built_in">str</span>(t):&lt;<span class="number">10</span>&#125;</span>\n&#x27;</span>)</span><br><span class="line">                        <span class="keyword">for</span> t, s, m, loc <span class="keyword">in</span> last_tensor_sizes - new_tensor_sizes:</span><br><span class="line">                            f.write(<span class="string">f&#x27;- <span class="subst">&#123;loc:&lt;<span class="number">50</span>&#125;</span> <span class="subst">&#123;<span class="built_in">str</span>(s):&lt;<span class="number">20</span>&#125;</span> <span class="subst">&#123;<span class="built_in">str</span>(m)[:<span class="number">4</span>]&#125;</span> M <span class="subst">&#123;<span class="built_in">str</span>(t):&lt;<span class="number">10</span>&#125;</span>\n&#x27;</span>)</span><br><span class="line">                        last_tensor_sizes = new_tensor_sizes</span><br><span class="line">                pynvml.nvmlShutdown()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># save details about line _to be_ executed</span></span><br><span class="line">            lineno = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            func_name = frame.f_code.co_name</span><br><span class="line">            filename = frame.f_globals[<span class="string">&quot;__file__&quot;</span>]</span><br><span class="line">            <span class="keyword">if</span> (filename.endswith(<span class="string">&quot;.pyc&quot;</span>) <span class="keyword">or</span>  </span><br><span class="line">                    filename.endswith(<span class="string">&quot;.pyo&quot;</span>)):</span><br><span class="line">                filename = filename[:-<span class="number">1</span>]</span><br><span class="line">            module_name = frame.f_globals[<span class="string">&quot;__name__&quot;</span>]</span><br><span class="line">            lineno = frame.f_lineno</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> gpu_profile</span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;A exception occured: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(e))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gpu_profile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_tensors</span>():</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> gc.get_objects():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> torch.is_tensor(obj):</span><br><span class="line">                tensor = obj</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> tensor.is_cuda:</span><br><span class="line">                <span class="keyword">yield</span> tensor</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;A exception occured: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(e))</span><br></pre></td></tr></table></figure>
<p>需要注意的是,linecache中的getlines只能读取缓冲过的文件,如果这个文件没有运行过则返回无效值。Python 的垃圾收集机制会在变量没有应引用的时候立马进行回收,但是为什么模型中计算的中间变量在执行结束后还会存在呢。既然都没有引用了为什么还会占用空间?</p>
<p>一种可能的情况是这些引用不在Python代码中,而是在神经网络层的运行中为了backward被保存为gradient,这些引用都在计算图中,我们在程序中是无法看到的:</p>
<br>
<h2 id="后记"><a class="markdownIt-Anchor" href="#后记"></a> 后记</h2>
<p>实际中我们会有些只使用一次的模型，为了节省显存，我们需要一边计算一遍清除中间变量，使用del进行操作。限于篇幅这里不进行讲解，下一篇会进行说明。</p>

                    </article>
                    


    <blockquote id="date-expire-notification" class="post-expired-notify">本文最后更新于 <span id="date-expire-num"></span> 天前，文中所描述的信息可能已发生改变</blockquote>
    <script>
    (function() {
        var dateUpdate = Date.parse("2024-01-11");
        var nowDate = new Date();
        var a = nowDate.getTime();
        var b = a - dateUpdate;
        var daysUpdateExpire = Math.floor(b/(24*3600*1000));
        if (daysUpdateExpire >= 120) {
            document.getElementById('date-expire-num').innerHTML = daysUpdateExpire;
        } else {
            document.getElementById('date-expire-notification').style.display = 'none';
        }
    })();
    </script>


<p class="post-footer-info mb-0 pt-0">本文发表于&nbsp;<time datetime="2024-01-10T16:25:17.000Z" itemprop="datePublished">2024-01-11</time>

</p>
<p class="post-footer-info mb-0 pt-2">

<span class="post-categories-list mt-2">

<a class="post-categories-list-item" href='/categories/Accumulate/'>Accumulate</a>

</span>



<span class="post-tags-list mt-2">

<a class="post-tags-list-item" href="/tags/Accumulate/" rel="tag">#&nbsp;Accumulate</a>

<a class="post-tags-list-item" href="/tags/pytorch/" rel="tag">#&nbsp;pytorch</a>

<a class="post-tags-list-item" href="/tags/memory/" rel="tag">#&nbsp;memory</a>

</span>


</p>

                </div>
                <div class="post-nav px-2 bg-gray">
<ul class="pagination">
    <!-- Prev Nav -->
    
        <li class="page-item page-prev">
            <a href="/2024/01/11/Pytorch%E4%B8%AD%E7%9A%84%E6%98%BE%E5%AD%98%E5%88%A9%E7%94%A8%E9%97%AE%E9%A2%98/" rel="prev">
                <div class="page-item-title"><i class="icon icon-back" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Pytorch中的显存利用问题</div>
            </a>
        </li>
    

    <!-- Next Nav -->
    
        <li class="page-item page-next">
            <a href="/2024/01/10/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E4%B8%AD%E9%97%B4%E5%8F%98%E9%87%8F%E7%9A%84%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E5%A4%A7%E5%B0%8F/" rel="next">
                <div class="page-item-title"><i class="icon icon-forward" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">计算模型以及中间变量的显存占用大小</div>
            </a>
        </li>
    
</ul>
</div>

                
                    <!-- # Comment # -->
                    
                        <div class="card-footer post-comment">
                            <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
        this.page.url = 'https://abinzzz.github.io/2024/01/11/%E5%9C%A8Pytorch%E4%B8%AD%E7%B2%BE%E7%BB%86%E5%8C%96%E5%88%A9%E7%94%A8%E6%98%BE%E5%AD%98/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://abinzzz.github.io/2024/01/11/%E5%9C%A8Pytorch%E4%B8%AD%E7%B2%BE%E7%BB%86%E5%8C%96%E5%88%A9%E7%94%A8%E6%98%BE%E5%AD%98/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>
<script id="disqus-thread-script">
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//robin02.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

                        </div>
                    
                
            </div>
        </div>
    </div>
</div>

            <!-- ### Footer ### -->
            <footer class="text-center">
    <!-- footer copyright -->
    
        <p class="footer-copyright mb-0">Copyright&nbsp;©&nbsp;<span id="copyright-year"></span>
            <a class="footer-copyright-a" href="https://abinzzz.github.io">blog</a>
        </p>

    <!-- footer custom text -->
    <p class="footer-text mb-0">
    
    </p>
    <!-- footer develop info -->
    <p class="footer-develop mb-0">
        
    <!-- Busuanzi User Views -->
    <span id="busuanzi_container_site_uv" hidden>
        <span></span>
        <span id="busuanzi_value_site_uv"></span>
        <span>Viewers</span>
        
            <span>|</span>
        
    </span>




        
        Powered by&nbsp;<!--
         --><a href="https://hexo.io" target="_blank" class="footer-develop-a" rel="external nofollow noopener noreferrer">Hexo</a><span class="footer-develop-divider"></span>Theme&nbsp;-&nbsp;<!--
         --><a href="https://github.com/SukkaW/hexo-theme-suka" target="_blank" class="footer-develop-a" rel="external noopener">Suka</a>
    </p>
</footer>


        <!-- ### Import File ### -->
        <!-- ### Footer JS Import ### -->

<script>

    
window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 50
};

(function() {
    var copyrightNow = new Date().getFullYear();
    var copyrightContent = document.getElementById('copyright-year');
    var copyrightSince = 2023;
    if (copyrightSince === copyrightNow) {
        copyrightContent.textContent = copyrightNow;
    } else {
        copyrightContent.textContent = copyrightSince + ' - ' + copyrightNow;
    }
})();
console.log('\n %c Suka Theme (hexo-theme-suka) | © SukkaW | Verision 1.3.3 %c https://github.com/SukkaW/hexo-theme-suka \n', 'color: #fff; background: #444; padding:5px 0;', 'background: #bbb; padding:5px 0;');

</script>

<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@8.9.0" async></script>
    <script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" async></script>


<!-- Offset -->




<!-- Comment -->

    
        <script id="dsq-count-scr" src="https://robin02.disqus.com/count.js" async></script>

    


<!-- ### Custom Footer ### -->

    </body>

</html>