
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        CSS 2024:DF-Forward |
        
        blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="基本信息 标题: DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass 作者:  Minxin Du - The Chinese University of Hong Kong Xiang Yue - The Ohio State University">
<meta property="og:type" content="article">
<meta property="og:title" content="CSS 2024:DF-Forward">
<meta property="og:url" content="https://abinzzz.github.io/2024/09/02/CSS-2024-DF-Forward/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="基本信息 标题: DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass 作者:  Minxin Du - The Chinese University of Hong Kong Xiang Yue - The Ohio State University">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.imgdb.cn/item/66d94d82d9c307b7e9636f93.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/66d9608fd9c307b7e97f7a2f.png">
<meta property="og:image" content="https://pic.imgdb.cn/item/66d96545d9c307b7e98404b7.png">
<meta property="article:published_time" content="2024-09-02T11:14:48.000Z">
<meta property="article:modified_time" content="2024-09-05T08:01:16.411Z">
<meta property="article:author" content="ab">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/66d94d82d9c307b7e9636f93.png">
    
        <link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://pic.imgdb.cn/item/66d6bebfd9c307b7e94b1ea2.png" data-sizes="auto" alt="CSS 2024:DF-Forward" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>CSS 2024:DF-Forward</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-CSS-2024-DF-Forward" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2024/09/02/CSS-2024-DF-Forward/" class="article-date-link">
        <time datetime="2024-09-02T11:14:48.000Z"
              itemprop="datePublished">2024-09-02</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/paper/">paper</a><a class="article-category-link" href="/categories/paper/FL/">FL</a><a class="article-category-link" href="/categories/paper/FL/Privacy/">Privacy</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <h2 id="基本信息"><a class="markdownIt-Anchor" href="#基本信息"></a> 基本信息</h2>
<p><strong>标题</strong>: DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass</p>
<p><strong>作者</strong>:</p>
<ol>
<li>Minxin Du - The Chinese University of Hong Kong</li>
<li>Xiang Yue - The Ohio State University</li>
<li>Sherman S. M. Chow - The Chinese University of Hong Kong</li>
<li>Tianhao Wang - University of Virginia</li>
<li>Chenyu Huang - Independent</li>
<li>Huan Sun - The Ohio State University</li>
</ol>
<p><strong>出版信息</strong>:</p>
<ul>
<li><strong>会议</strong>: 2023 ACM SIGSAC Conference on Computer and Communications Security (CCS '23)</li>
<li><strong>日期</strong>: 2023年11月26-30日</li>
<li><strong>地点</strong>: 丹麦哥本哈根</li>
<li><strong>出版社</strong>: ACM, New York, NY, USA</li>
<li><strong>页数</strong>: 18页</li>
<li><strong>DOI</strong>: <a target="_blank" rel="noopener" href="https://doi.org/10.1145/3576915.3616592">10.1145/3576915.3616592</a></li>
</ul>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>差分隐私随机梯度下降（DP-SGD）在反向传播过程中向梯度添加噪声，从而保护训练数据免受隐私泄露，特别是防止成员推断攻击。然而，它<strong>无法覆盖推理阶段的威胁</strong>，例如嵌入反转和敏感属性推断。此外，在微调大规模预训练语言模型（LMs）时，DP-SGD在存储和计算上成本很高。(<strong>因为dp-sgd只在训练阶段通过梯度增加噪声来保护训练数据隐私，主要是为了防止成员推断攻击</strong>)</p>
<p>我们提出了DP-Forward方法，该方法<strong>直接在语言模型的前向传播过程中扰动嵌入矩阵</strong>。它满足训练和推理数据的严格本地差分隐私（DP）要求。为了用最小的矩阵值噪声实现这一点，我们设计了一种解析矩阵高斯机制（aMGM），通过从矩阵高斯分布中绘制可能非独立同分布的噪声来实现。我们随后研究了在语言模型的不同隐藏（子）层中使用aMGM噪声进行扰动的效果。在三个典型任务上的实验结果显示，DP-Forward的效用几乎达到了非隐私基线，并且在中等隐私水平下，其表现优于DP-SGD，提升幅度最高可达7.7个百分点。此外，与使用最新高速库的DP-SGD相比，DP-Forward节省了3倍的时间和内存成本，并且将嵌入反转和敏感属性推断的平均成功率分别降低了最高88个百分点和41个百分点，而DP-SGD在这方面表现不佳。</p>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<p>Transformer深度学习架构[68]目前在计算机视觉领域越来越受欢迎，并已广泛应用于自然语言处理（NLP）。基于Transformer的语言模型（LMs），如BERT[20]和GPT[59, 60]，在几乎所有的NLP任务中都取得了最先进的性能。它们首先在大规模（公开的）自标记语料库上进行预训练，然后使用更小的、可能是私有的语料库针对各种任务进行微调。这种方法避免了从头开始训练的过程，同时避免了任务特定语料库不足的情况，提高了模型的通用性。</p>
<p>用于提升微调语言模型效用的训练数据可能包含敏感信息。语言模型可能会（无意中）记住这些数据[12]，从而容易受到成员推断攻击（MIAs）的影响，这种攻击可以识别某个样本是否在训练集中。更糟糕的是，仅通过对GPT-2的黑箱访问就可以提取出原始训练文本（例如，社会安全号码SSNs）[13]。此外，还可以通过提取攻击[13]，从在临床语料库上训练的BERT模型中恢复出个人健康信息（例如，病人-病情对）[42]。</p>
<p>差分隐私（DP）[22]已成为保护个人隐私的实际标准。为了防止对个人训练数据的MIAs攻击，可以使用差分隐私随机梯度下降（DP-SGD）[1]。该方法对批次中的每个样本的梯度进行裁剪，并向聚合梯度添加随机高斯噪声。它比早期只关注凸问题的尝试[17, 18]更加通用，已经在现代机器学习框架中实现，如PyTorch和TensorFlow。可以将其应用于微调基于LM的NLP管道，同时确保样本级别的隐私，假设每个个体贡献一个样本，通常是一个序列-标签对。</p>
<p>然而，DP-SGD通常需要一个可信的第三方来管理用户的敏感训练数据。虽然可以通过安全聚合[15]以分布式方式[9, 50]完成，但这增加了额外的成本和信任假设，且其核心仍然是中心差分隐私（CDP）。以每个样本为单位计算梯度（例如，BERT-Base中有超过1.1亿参数）显然成本很高。此外，由于“维度诅咒”，维护通过添加噪声的聚合梯度训练的管道的效用也很困难。最近的一项研究[79, 表4]显示，在中等隐私条件下，针对四个NLP任务微调语言模型的平均准确率为65.7%（而没有差分隐私时为91.8%）。最后，推理阶段的嵌入没有被训练期间添加的噪声扰动，这使得推理查询容易受到各种恢复攻击的影响[56, 64]，这些攻击范围从敏感属性（例如，作者身份）到原始文本。</p>
<h3 id="11-通过扰动嵌入实现自然隐私"><a class="markdownIt-Anchor" href="#11-通过扰动嵌入实现自然隐私"></a> 1.1 通过扰动嵌入实现自然隐私</h3>
<p>我们提出了一种完全不同的方法，称为DP-Forward，它扰动前向传播信号：用户可以在共享用于训练的（带标签的）序列之前，本地向其嵌入添加噪声，这与在反向传播中扰动梯度（可能由不受信任方进行）的方法不同。该方法旨在提供可证明的本地差分隐私（LDP）保障，从而能够抵御比DP-SGD更强的对手。</p>
<p>我们的方法也自然适用于联邦学习（FL）场景，这种场景不收集用户数据，但存在显著的区别——联邦学习通常共享无噪声的本地模型更新。需要注意的是，任何对噪声嵌入的后续计算（例如梯度计算）由于LDP的自由后处理性质都不会导致额外的隐私损失。有人可能会通过向用户的每个样本梯度中添加“足够多”的噪声来强制DP-SGD提供LDP，但这可能会在相似的隐私水平下生成不可用的模型。</p>
<p>------------------------------ 分析·Begin ------------------------------<br />
<strong>自由后处理性质</strong>：任何对已经添加噪声的数据的后续计算，不会导致额外的隐私泄露</p>
<p>------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<p>DP-Forward还通过向用户的测试序列嵌入添加噪声，确保推理阶段与训练阶段一样满足LDP。此外，作为一个“附带”好处，它能够有效缓解除成员推断攻击（MIA）外的其他新兴嵌入隐私风险【56, 64】。</p>
<p>显然，DP-Forward的设计目标与我们的总体目标自然一致：LDP（对比于CDP），更直接地保护原始数据（而不是梯度）以应对新威胁【56, 64】，并且能够像常规的非隐私训练一样高效（允许对噪声嵌入进行批处理）。不幸的是，支持这些目标的基础尚不可用。<strong>一个专门用于扰动前向传播信号的机制是必不可少的</strong>。</p>
<p>具体来说，我们需要为通过基于语言模型的管道的前向传播获得的训练/推理文本序列的嵌入导出噪声，这些嵌入作为实数和矩阵值函数。可以采用经典的高斯机制（GM）【23】来添加从单变量高斯分布中抽取的独立同分布（i.i.d.）噪声。然而，GM仅根据差分隐私的一个充分条件校准其噪声方差，并且其<strong>方差公式不适用于低隐私环境</strong>【7】。另一种选择是矩阵变量高斯（MVG）机制【14】，专为矩阵值数据设计：它利用可能的非独立同分布噪声从矩阵高斯分布中扰动较少的重要行/列。尽管它可能在效用上优于GM【14】，但由于基于充分条件，它仍然是<strong>次优</strong>的。</p>
<p>------------------------------ 分析·Begin ------------------------------<br />
<strong>高斯机制GM</strong>：通过在数据上添加高斯噪声来达到隐私保护<br />
<strong>矩阵变量高斯机制MCG</strong>：专门为矩阵数值设计的差分隐私方法<br />
<strong>低隐私环境</strong>：对隐私保护要求比较低的场景</p>
<p>------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<p>为了优化MVG，我们提出了一种解析矩阵高斯机制（aMGM），通过整合解析GM（aGM）【7】的必要且充分条件来校准非独立同分布的噪声。我们的挑战在于<strong>处理两个协方差矩阵，而不是一个单一的方差</strong>。我们推导出一个仅涉及两个最小奇异值的约束（见第4.2节），这表明对于像DP-Forward这样的通用应用来说，独立同分布的噪声（如aGM中）可能已经是最优的。</p>
<p>基于Transformer的管道包含一个输入嵌入层、编码器和任务层。这些层在训练和后续推理过程中显著地操作文本输入的嵌入。我们研究了在任务层之前的任何隐藏（子）层输出的嵌入中添加aMGM噪声（如图1所示）。为了确保序列级LDP，我们需要估计任何两个序列的“噪声前”函数的L2-敏感性【23】。这是一个非平凡的问题，因为这些函数可能包括不同的（子）层，甚至可能不是Lipschitz连续的【39】。我们的策略是将函数输出归一化，使其具有固定的Frobenius（或L2）范数，类似于梯度裁剪【1】。这种方法对更深的子层尤其有效，能够达到与非隐私基线相当的任务准确性（见第5节）。对于前几个（子）层，我们还做了两种特化处理，将LDP放宽到令牌级别，详细内容见附录A.2，以提高准确性。</p>
<h3 id="12-我们的贡献"><a class="markdownIt-Anchor" href="#12-我们的贡献"></a> 1.2 我们的贡献</h3>
<p>出于对语言模型微调和推理阶段的隐私问题的普遍关注，以及DP-SGD的固有缺陷的考虑，我们开始对一种直观但很少被研究的方法进行正式研究，并探索其与基于Transformer的NLP管道的集成。具体来说：</p>
<ol>
<li>
<p><strong>提出DP-Forward微调方法</strong>：我们提出了一种DP-Forward微调方法，通过扰动每个用户（带标签）序列的前向传播嵌入来保护隐私。这种方法比DP-SGD扰动聚合梯度提供了更直接的保护。它的可证明保证（定理1）是一种新的序列级LDP概念（SeqLDP，定义4），具有更严格的(𝜖,𝛿)-LDP保证，仅对序列有效。此外，DP-Forward可以自然地扩展到推理阶段，确保测试序列在没有标签的情况下符合标准LDP（定理3），而DP-SGD则无法做到这一点。</p>
</li>
<li>
<p><strong>提出aMGM机制</strong>：为了实现DP-Forward的最优输出扰动机制，我们提出了aMGM机制，这对任何矩阵值函数都有独立的意义。通过利用aGM【7】中差分隐私的必要且充分条件，aMGM可以从类似于MVG【14】的矩阵高斯分布中提取可能的非独立同分布噪声，同时在处理高维数据时产生数量级更小的噪声（见第5.3节）。</p>
</li>
<li>
<p><strong>在三种典型NLP任务上的实验</strong>：我们在第5节进行了三种典型NLP任务的实验，展示了关键超参数（如序列长度）如何影响任务准确性。为了在隐私和效用之间公平比较DP-SGD，我们做了以下两点：i) 使用随机响应【70】扰动标签，使DP-Forward微调为序列-标签对提供标准LDP（定理2）。ii) 通过洗牌【25】将具有标准LDP的DP-Forward转换为（DP-SGD提供的）示例级CDP。我们在深层DP-Forward实例化中获得的准确性提升高达7.7个百分点（pp），相比于DP-SGD或其最近的改进【78, 79】（见第7.3节），在相似的隐私水平下。此外，即使使用最新的Opacus库【77】，DP-SGD也会消耗超过3倍的时间和GPU内存成本。</p>
</li>
<li>
<p><strong>评估三类隐私威胁</strong>：我们评估了三类隐私威胁。与DP-SGD类似，DP-Forward（包括附录A.3中的两个令牌级设计）可以有效防御序列级成员推断攻击（MIAs），但只有DP-Forward可以阻止推理阶段嵌入上的两类威胁。具体来说，第6节显示，DP-SGD在两种嵌入反转攻击中完全失败，而DP-Forward将其成功率显著降低了多达88个百分点（pp）。对于基于神经网络的属性推断攻击，DP-SGD平均仅将成功率降低了15个百分点，而DP-Forward实现了约41个百分点的降低，使攻击的预测效果类似于将所有标签都分配给多数类。</p>
</li>
</ol>
<p>简而言之，DP-Forward是训练（和测试）深度学习模型（例如基于巨型语言模型的模型）比DP-SGD更好的替代方案。</p>
<h2 id="2-预备知识和符号"><a class="markdownIt-Anchor" href="#2-预备知识和符号"></a> 2. 预备知识和符号</h2>
<h3 id="21-bert中的transformer编码器"><a class="markdownIt-Anchor" href="#21-bert中的transformer编码器"></a> 2.1 BERT中的Transformer编码器</h3>
<p>包括BERT [20] 和 GPT [59] 在内的现代基于Transformer的语言模型（LMs）首先在大量未标注的（公共）语料库上进行预训练，以学习上下文化的文本表示。之后，它们可以使用更小、更特定于任务的数据集微调，以适应各种下游NLP任务（例如，情感分析、问题回答等）。</p>
<p>我们考虑BERT（如图1所示），它由L个相同的层（即双向Transformer编码器【68】）堆叠而成。每一层有两个子层：点积多头注意力（MHA）【68】和前馈神经网络（FFN）。每个子层都有一个额外的残差连接，接着是层归一化【6】。</p>
<p>设 ( X = \langle x_i \rangle_{i=1}^n ) 为长度为n的输入序列（例如，字符、单词、子词、q-grams），其中 ( x_i ) 来自词汇表 ( \mathcal{V} )。输入嵌入层首先将每个 ( x_i ) 映射到 ( \mathbb{R}^d ) 中的表示，表示为token、segment和position嵌入的总和。我们重用 ( X ) 来表示 ( \mathbb{R}^{n \times d} ) 中的隐藏嵌入矩阵。在MHA层中，我们为每一个注意力头（Att）推导出查询、键和值矩阵 ( Q, K, V \in \mathbb{R}^{n \times d/h} ) （h是d的除数），这些矩阵通过与头部特定的权重 ( W^Q, W^K, W^V \in \mathbb{R}^{d \times d/h} ) 相乘来得到。其输出是：</p>
<p>[<br />
\text{Att}_i(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d/h}}\right)V, \quad \forall i \in [1, h].<br />
]</p>
<p>softmax(·)的输入是一个 ( n \times n ) 的成对点积矩阵。最终，MHA将所有头的输出串联（用 || 表示）成 ( \mathbb{R}^{n \times d} ) 中的一个矩阵，然后与投影矩阵 ( W^O \in \mathbb{R}^{d \times d} ) 右乘：</p>
<p>[<br />
\text{MHA}(X) = [\text{Att}_1 || \cdots || \text{Att}_h]W^O.<br />
]</p>
<p>FFN由两个线性映射组成，中间有一个ReLU激活函数。它分别且独立地对每个 ( x_i, i \in [1, n] ) 进行操作：</p>
<p>[<br />
\text{FFN}(x_i) = \text{ReLU}(0, x_iW_1 + b_1)W_2 + b_2,<br />
]</p>
<p>其中 ( W_1, W_2, b_1 )，和 ( b_2 ) 是可训练的矩阵/向量值参数。其在 ( X ) 上的输出为：</p>
<p>[<br />
\text{FFN}(X) = [\text{FFN}(x_1)^T || \cdots || \text{FFN}(x_n)^T].<br />
]</p>
<p>子层的残差连接为 ( X + \text{MHA}(X)/\text{FFN}(X) )。</p>
<p>层归一化 ( \text{LN}(x_i) ) 将所有 ( x_i ) 正则化为均值为零、方差为一的单位向量，在每个超小时间步上执行。</p>
<p>在最终的FFN编码器的输出中，隐藏的嵌入矩阵被缩减为 ( \mathbb{R}^{1 \times d} ) 中的向量。常见的缩减方法包括均值池化【61】（计算 (\frac{1}{n} \sum_{i=1}^n x_i/n)）或取特殊token [CLS] 的最后嵌入作为分类【20】。</p>
<p>BERT的预训练基于两种自监督任务：掩码语言模型（MLM）和下一句预测【20】。我们采用MLM：它随机掩盖输入序列中的一些tokens，用于通过最小化交叉熵损失来预测这些被掩盖的tokens。</p>
<p>[<br />
L_{\text{MLM}} = -\sum_{i \in I} \log \Pr[x_i’ | X \setminus {x_i; \theta}], \quad \text{其中 } X’ = X \setminus {x_i : i \in I},<br />
]</p>
<p>这里 θ 表示BERT transformer编码器的所有参数。</p>
<h3 id="22-本地差分隐私"><a class="markdownIt-Anchor" href="#22-本地差分隐私"></a> 2.2 （本地）差分隐私</h3>
<p>差分隐私（DP）【22】是一种严格的、可量化的隐私概念。它有两种流行的模型：中心化和本地化。在中心化差分隐私中，一个可信的数据管理员通过一个随机机制访问和处理来自多个用户的原始数据。形式化定义如下：</p>
<p><strong>定义1（中心化差分隐私）</strong>：对于隐私参数 ( \epsilon \geq 0 ) 和 ( 0 \leq \delta \leq 1 )，若 ((\epsilon, \delta))-DP 满足以下条件，则对于所有相邻的数据集 ( X ) 和 ( X’ )（用 ( X \simeq X’ ) 表示）和所有输出的子集 ( O )，有：</p>
<p>[<br />
\Pr[\mathcal{M}(X) \in O] \leq e^\epsilon \Pr[\mathcal{M}(X’) \in O] + \delta.<br />
]</p>
<p>如果 ( \delta = 0 )，我们称之为 ( \epsilon )-DP。</p>
<p>这个邻接概念依赖于应用（将在第3.1节讨论）。通常，它涉及“替换一个”关系：可以通过替换单个个人的数据点（例如，序列-标签对）从 ( X ) 获得 ( X’ )。这种方法为数据集中任何个体提供合理的可否认性。在对比中，本地差分隐私（LDP）【38】取消了可信数据管理员，允许个人在使用机制 ( \mathcal{M} ) 在被发送给不受信任的聚合器进行分析之前本地扰动他们的数据。</p>
<p><strong>定义2（本地差分隐私）</strong>：对于 ( \epsilon \geq 0 ), ( 0 \leq \delta \leq 1 )，若 ((\epsilon, \delta))-LDP 满足以下条件，对于任何两个输入 ( X, X’ ) 和输出的任意可能子集 ( O ) 有：</p>
<p>[<br />
\Pr[\mathcal{M}(X) \in O] \leq e^\epsilon \Pr[\mathcal{M}(X’) \in O] + \delta.<br />
]</p>
<p>如果 ( \delta = 0 )，我们称之为 ( \epsilon )-LDP。</p>
<p>隐私损失随机变量（PLRV）。对于一对特定输入 ( X \simeq X’ )，观测到输出 ( O ) 所引起的隐私损失（或“实际的 (\epsilon) 值”）【7】是两个概率的对数比：</p>
<p>[<br />
L_{\mathcal{M}, X, X’}(O) = \ln \frac{\Pr[\mathcal{M}(X) = O]}{\Pr[\mathcal{M}(X’) = O]}。<br />
]</p>
<p>当 ( O ) 根据 ( \mathcal{M}(X) ) 变化时，我们得到PLRV ( L_{\mathcal{M}, X, X’} )。一种处理DP的有用方法是分析PLRV的尾部界限【23】，我们在第4.2节中利用这种方法来构建我们提出的机制。</p>
<p>DP有两个理想属性：自由后处理和组合性。前者意味着对 ((\epsilon, \delta))-DP机制输出的进一步计算不会引起额外的隐私损失。后者允许我们构建更复杂的机制，叠加更简单的机制：例如，针对同一输入运行k次 ((\epsilon, \delta))-DP机制至少为 ((k\epsilon, k\delta))-DP。在考虑只有一个数据行的数据集时，这两个属性也适用于LDP。</p>
<p>------------------------------ 分析·Begin ------------------------------<br />
<strong>自由后处理</strong>：自由后处理意味着一旦数据通过差分隐私机制处理，并获得输出，对这些输出的任何进一步计算或处理都不会导致额外的隐私计算。也就是说，如果一个数据集已经通过差分隐私机制得到了保护，那么基于这些结果的后续计算都不会增加额外的隐私风险。</p>
<p><strong>组合性</strong>：允许我们通过组合多个差分隐私机制来构建更复杂的隐私保护机制，<br />
------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<p>一个针对矩阵值函数的输出扰动机制 ( \mathcal{M} ) 通过在其输出上添加从随机变量中抽取的随机噪声来计算：</p>
<p><strong>高斯机制（GM）</strong>：对于 ((\epsilon, \delta))-DP，典型的实例 ( \mathcal{M} ) 是经典的GM【23】，它添加噪声 (Z \in \mathbb{R}^{n \times d} ) 每个条目从单变量高斯分布 ( \mathcal{N}(0, \sigma^2) ) 中独立同分布抽取。方差 ( \sigma^2 = 2 \ln(1.25/\delta) S_2<sup>2(f)/\epsilon</sup>2 ) 与L2-敏感性成正比：</p>
<p>[<br />
S_2(f) = \sup_{X \simeq X’} |f(X) - f(X’)|_F,<br />
]</p>
<p>其中 (|\cdot|_F) 表示矩阵的Frobenius范数【34】。</p>
<p>表1总结了本文中使用的首字母缩写。</p>
<hr />
<p>表1：首字母缩写（新提出的用*标记）</p>
<table>
<thead>
<tr>
<th>缩写</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>NLP</td>
<td>自然语言处理（Natural Language Processing）</td>
</tr>
<tr>
<td>LM</td>
<td>语言模型（Language Model）</td>
</tr>
<tr>
<td>BERT</td>
<td>来自Transformers的双向编码器表示（Bidirectional Encoder Representations from Transformers）</td>
</tr>
<tr>
<td>MLM</td>
<td>掩码语言建模（Masked Language Modeling）</td>
</tr>
<tr>
<td>MHA</td>
<td>多头注意力（Multi-Head Attention）</td>
</tr>
<tr>
<td>FFN</td>
<td>前馈神经网络（Feed-Forward Network）</td>
</tr>
<tr>
<td>MIA</td>
<td>成员推断攻击（Membership Inference Attack）</td>
</tr>
<tr>
<td>DP-SGD</td>
<td>差分隐私随机梯度下降（Differentially Private Stochastic Gradient Descent）</td>
</tr>
<tr>
<td>PLRV</td>
<td>隐私损失随机变量（Privacy Loss Random Variable）</td>
</tr>
<tr>
<td>©DP</td>
<td>（中心）差分隐私（(Central) Differential Privacy）</td>
</tr>
<tr>
<td>LDP</td>
<td>本地差分隐私（Local Differential Privacy）</td>
</tr>
<tr>
<td>Seq(L)DP*</td>
<td>序列（本地）差分隐私（Sequence (Local) Differential Privacy）</td>
</tr>
<tr>
<td>GM</td>
<td>高斯机制（Gaussian Mechanism）</td>
</tr>
<tr>
<td>RR</td>
<td>随机响应（Randomized Response）</td>
</tr>
<tr>
<td>MVG</td>
<td>矩阵高斯机制（Matrix-Variate Gaussian (Mechanism)）</td>
</tr>
<tr>
<td>aGM</td>
<td>解析高斯机制（Analytic Gaussian Mechanism）</td>
</tr>
<tr>
<td>aMGM*</td>
<td>解析矩阵高斯机制（Analytic Matrix Gaussian Mechanism）</td>
</tr>
</tbody>
</table>
<h2 id="3df-forward"><a class="markdownIt-Anchor" href="#3df-forward"></a> 3.DF-forward</h2>
<p>我们研究了基于BERT的管道模型，作为分类任务中表现优越的示例。DP-Forward方法也可以应用于其他（基于Transformer的）自然语言处理（NLP）或计算机视觉模型，这些模型在前向传播过程中涉及矩阵值计算。</p>
<p>假设每个用户持有一个序列-标签对（X, y）或仅持有X，用于在一个不可信的服务提供者处进行微调或测试。即使共享经过编辑的X（去除常见的个人身份信息）或其特征（一个不可读的实值嵌入矩阵）也可能会导致信息泄露。</p>
<p>对于DP-Forward训练，<strong>用户会在共享嵌入矩阵前对其进行局部扰动</strong>，以确保满足（新的）局部差分隐私（LDP）的要求。如果标签被认为是敏感的，也应对其进行扰动（参见第3.4节）。我们探讨了将管道分为噪声前函数f(·)和噪声后处理p(·)的不同选项（参见第3.2节）：用户可以访问f(·)来生成嵌入矩阵，并通过输出扰动机制M（例如GM）添加噪声；服务提供者在带噪声的（标记）嵌入上运行p(·)以进行微调（参见第3.3节）或预训练（参见第3.6节）。分析不同管道部分的S2(f)是一个挑战，我们通过对f(·)进行标准化来解决这个问题。</p>
<p>DP-Forward也可以自然地用于保护推理序列（参见第3.5节），这与DP-SGD不同。它利用了免费的后处理（即推理基于有噪声的嵌入进行），在管道中加入额外的“即插即用”噪声层，所需的更改最小。</p>
<p>------------------------------ 分析·Begin ------------------------------<br />
<strong>SeqDP</strong>:保证进改变数据集中一个序列，输出的变化也在可控范围内</p>
<p>------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<h3 id="31-序列局部差分隐私的概念"><a class="markdownIt-Anchor" href="#31-序列局部差分隐私的概念"></a> 3.1 序列（局部）差分隐私的概念</h3>
<p>嵌入函数 (f(X)) 编码了输入序列 (X) 的语义信息，其中每个序列包含 (n) 个标记（参见第2.1节）。微调（或后续推理）NLP 管道实际上是对 (f(X)) 进行处理。DP-Forward 微调通过对每个 (X) 使用输出扰动机制 (\mathcal{M}) 来保护每个 (X)，与此相对，DP-SGD 则是对 (X) 和标签 (y) 上的梯度 (f’(X, y)) 的聚合进行扰动。简单来说，（(\epsilon, \delta)-LDP）适用于 (X)，而DP-SGD适用于(X, y)的中央差分隐私（CDP）。</p>
<p>仅针对序列的保护是有意义的，因为序列通常传递（隐式）敏感信息（例如，作者身份），而标签（例如，表示正/负的单个位）可以是公开的。对于如何实现对 (X, y) 的“完整”LDP，请参见第3.4节。为了弥合DP-SGD和DP-Forward在理论保证上的差距，我们首先在中央设置中定义了序列DP（SeqDP）。</p>
<p><strong>定义3（SeqDP）</strong>：对于 (\epsilon \geq 0, 0 \leq \delta \leq 1)，(\mathcal{M}) 满足 ((\epsilon, \delta))-SeqDP，如果对于所有仅在某个索引处的序列不同的相邻数据集 (X, X’)（即 (X_i, y_i \in X) 和 (X’_i, y_i \in X’)，以及任何可能的输出子集 (O)，都有：<br />
[<br />
\Pr[\mathcal{M}(X) \in O] \leq e^\epsilon \Pr[\mathcal{M}(X’) \in O] + \delta.<br />
]</p>
<p><strong>3.1.1 标签DP</strong>：最近提出的标签DP概念[26, 31]最初在PAC学习中研究[16]。它<strong>只保护标签</strong>（而不是相应的输入/图像）：((\epsilon, \delta))-DP仅适用于标签。</p>
<p>我们的SeqDP比标签DP“更安全”或至少“补充”了标签DP，因为标签通常依赖于其序列（但反之不然），即使标签受到保护（通过任何标签DP机制），也很可能从原始序列中恢复出真实标签。后续研究[72]显示，即使在模型泛化的情况下，当（(\epsilon, \delta)）任意小时，标签DP下的标签保护也不可能。此外，标签可能缺失（例如，在推理或自监督学习中），在这种情况下，SeqDP升级为标准的（(\epsilon, \delta)-DP），而标签DP则完全不适用。</p>
<p><strong>3.1.2 序列局部DP（SeqLDP）</strong>：我们进一步定义了SeqLDP，这是序列DP的局部对应概念。注意，上述关于标签DP与SeqDP的讨论同样适用于SeqLDP中的标签DP。</p>
<p><strong>定义4（SeqLDP）</strong>：对于 (\epsilon \geq 0, 0 \leq \delta \leq 1)，(\mathcal{M}) 满足 ((\epsilon, \delta))-SeqLDP，如果对于所有相同 (y) 的 (X, X’)，以及任何可能的输出子集 (O)，都有：<br />
[<br />
\Pr[\mathcal{M}(X, y) \in O] \leq e^\epsilon \Pr[\mathcal{M}(X’, y) \in O] + \delta.<br />
]</p>
<p>理论上，SeqLDP仍然是一个强概念（类似于标准的LDP）。它旨在对序列提供信息理论保护，并限制任何 (X, X’)（最多相差 (n) 个标记）之间的不可区分性，因此决定了有噪声嵌入的“有用性”。</p>
<p><strong>3.1.3 序列级SeqLDP与标记级SeqLDP</strong>：在实践中，作为一个在（理想的理论保证和实用效用）之间平衡的强概念，为SeqLDP实现一个有意义的 (\epsilon) 范围是一个挑战。对于 ((\epsilon, \delta))-SeqLDP，向 (f(·)) 的输出添加高斯噪声需要对 (f) 的 (L_2)-敏感性 (S_2(f)) 进行约束，(\forall X, X’)。我们的方法是对输出进行标准化（参见第3.2节的附加好处），类似于DP-SGD中的梯度裁剪。当 (f(·)) 具有更多层（在相同的有意义 (\epsilon) 范围内）时，它通常表现得更好，因为较少的（可训练）参数/层的 (p(·)) 受到有噪声输出的“影响”。</p>
<p>不幸的是，当 (f(·)) 包含前几层时，例如，只有输入嵌入层可供用户使用（比如，为了节省用户端的存储和计算开销），这会导致较差的效用。作为一个全面的研究，我们依靠行级标准化和Lipschitz常数的组合来维持这些情况的效用。与一般的标准化相比，它旨在标记级别上实现较弱的SeqLDP，这是一种在“保护层次”中更细的粒度，保护任何在单个标记（与数据集）上不同的相邻序列（与数据集）。详细信息请参见附录A。</p>
<h3 id="32-我们的序列ldp方法"><a class="markdownIt-Anchor" href="#32-我们的序列ldp方法"></a> 3.2 我们的序列LDP方法</h3>
<p>在本文中，除了附录A外，DP-Forward方法对任何函数 (f(·)) 都应用了一般标准化方法，以实现序列级别的局部差分隐私（SeqLDP）。</p>
<p>假设 (f(·)) 是一个任意深度的前向传递过程，可以从BERT模型中的第一个（输入嵌入）层开始，到最后一个（任务）层之前的所有层。相应地，假设 (p(·)) 是剩余的层，从最后的任务层本身到第一个（输入嵌入）层之前的所有层。每个序列 (X) 在编码器或任务层之前的输出处都变成一个嵌入矩阵 (f(X) \in \mathbb{R}^{n \times d}) 或 ( \mathbb{R}^{1 \times d} )。为了满足 ((\epsilon, \delta))-SeqLDP，我们采用了一个合适的输出扰动机制 (\mathcal{M})，比如GM（高斯机制），并假设数据集中只有一个标记序列。</p>
<p>由于 (\mathcal{M}) 可以作用于任何隐藏层的输出，因此估计 (S_2(f)) 并不简单。特别是，MHA（多头注意力）本身，甚至包括更多的层，不是Lipschitz连续的，这意味着其输出可以因输入的细微变化而任意变化。为了解决这个问题，我们的方法是对函数输出进行标准化或裁剪：</p>
<p>[<br />
|f(·)|_F = C \text{ 或 } f(·)/\max(1, \frac{|f(·)|_F}{C}),<br />
]</p>
<p>如同在DP-SGD中一样，其中 (C) 是一个可调参数。我们接着有 (S_2(f) = 2C)。这种标准化使得任务效用对参数 (C) 的选择变得不那么“敏感”，因为信号和噪声随 (C) 成比例增加，而如果 (f(·)) 未被裁剪，信号可能保持不变。它还有许多其他好处，如稳定训练，避免过拟合，加速收敛等【2】。因此，我们在实验中采用了标准化。然后可以校准高斯噪声 (Z) 并为后噪声层 (p(·)) 提供 (f(X) + Z)。</p>
<p>请注意，为了避免 (p(·)) 重新访问 (X)（图1中虚线箭头）并维持免费的后处理，我们在将噪声添加到第一个MHA层的输出时删除了残差连接。这可能导致不稳定性（例如，梯度消失）【79】，但可以通过预训练一个新的没有这种残差连接的BERT来减轻，从而与后续的微调/推理保持一致。</p>
<p>使用GM的DP-Forward在维度 (d) 较大时（例如，对于BERT-Base，(d = 768)）会遇到“维度诅咒”的问题。为了缓解这一问题，我们可以添加两个线性映射 (M_1, M_2 \in \mathbb{R}^{d \times d’})，其中 (d’ \ll d)，使得 (f(·)) 和 (p(·)) 分别具有 (M_1) 和 (M_2)。两个映射都随机初始化，并像其他权重一样使用梯度进行更新。原始嵌入矩阵首先右乘 (M_1)，得到 (\mathbb{R}^{n \times d’}) 或 (\mathbb{R}^{1 \times d’})，然后进行标准化。我们的隐私保证不会受到影响，因为 (S_2(f)) 保持不变。然后使用 (M_2) 恢复维度以与原始管道兼容；由于免费的后处理，(M_2) 不会产生额外的隐私损失。然而，这需要专门的努力来修改管道；降维后的嵌入矩阵也可能丢失有用信息，从而降低任务效用。因此，我们使 (M_1) 和 (M_2) 成为可选的（参见第5.2节）。</p>
<p>------------------------------ 分析·Begin ------------------------------<br />
<strong>Lipschitz连续</strong>是一种数学概念，用于描述函数的平滑性和稳定性。具体来说，如果一个函数满足Lipschitz连续条件，那么它不会对输入的小变化产生过大的输出变化。</p>
<p><strong>定义</strong>：一个函数 ( f: X \to Y ) 被称为<strong>Lipschitz连续</strong>的，如果存在一个常数 (L \geq 0)（称为Lipschitz常数），使得对于所有 ( x_1, x_2 \in X )，都有：</p>
<p>[<br />
|f(x_1) - f(x_2)| \leq L |x_1 - x_2|<br />
]</p>
<p>这里：</p>
<ul>
<li>( | \cdot | ) 表示某种度量（通常是欧几里得范数）。</li>
<li>( L ) 是Lipschitz常数，它是函数的最大变化率的上界。</li>
</ul>
<p>------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<h3 id="33-dp-forward微调"><a class="markdownIt-Anchor" href="#33-dp-forward微调"></a> 3.3 DP-Forward微调</h3>
<p>假设我们使用一个原始的、公共的BERT检查点进行微调。在第 (i) 步（ (i \geq 1) ）的前向传播中，它为一批用户提供最新的 (f^{(i-1)}(·))，模拟常规的小批量随机梯度下降（SGD）。初始函数 (f^{(0)}) 来自原始检查点。用户被随机选择（不放回），其数量是固定的。用户在批处理中各自独立地计算它们的有噪声嵌入 (f^{(i-1)}(X) + Z) 以确保序列局部差分隐私（SeqLDP）（定理1）。然后，他们将这些嵌入和未扰动的标签 (y) 发送给服务提供者，后者运行 (p^{(i-1)}(·)) 在有噪声的嵌入（(f^{(i-1)}(X) + Z)）和标签 (y) 上来计算批次损失；在SeqLDP下的任何嵌入后处理不会对 (X) 造成额外的隐私损害。这里的 (p^{(0)}) 包括其余的原始BERT部分和随机初始化的任务层。</p>
<p>在反向传播期间，服务提供者可以通过后噪声层的梯度（由损失和有噪声嵌入得到）从 (p^{(i-1)}(·)) 更新到 (p^{(i)}(·))。为了避免访问用户的原始输入 (X)，需要冻结前噪声层 (f^{(i-1)}(·)) 作为 (f^{(0)})。参数冻结与最近的零样本或上下文学习范式兼容【52】。当模型巨大且完全微调昂贵时，这很有用。然而，冻结的层数越多，效用可能越差（即使在非隐私设置中也是如此）。</p>
<p>有两种安全更新 (f^{(i-1)}(·)) 的通用方法：</p>
<ol>
<li>假设一个额外的可信方（如在DP-SGD中），但这变成了中央差分隐私（DP）。</li>
<li>用户可以首先在本地的 (X) 上计算内部 (f^{(i-1)}(·)) 的梯度，然后求助于服务提供者的安全聚合【9】来进行全局更新。然而，这很昂贵。为了获得更好的效用，我们在实验中直接更新 (f^{(i-1)}(·))，要求在不同的轮次之间考虑隐私退化（如详细描述的那样）。关于如何在效用、隐私和其他方面找到平衡的专门方法，将作为未来的工作。</li>
</ol>
<p><strong>定理1</strong>：设 (f(·)) 是BERT管道中的前噪声函数，(\mathcal{M}) 是具有参数 (\epsilon \geq 0, 0 \leq \delta \leq 1) 的高斯机制（GM）。运行在标准化/裁剪的 (f(·)) 上的DP-Forward微调，确保了 ((\epsilon, \delta))-SeqLDP。</p>
<p>证明遵循GM的证明方法【23】。关键在于 (S_2(f))，对于所有 (X, X’)，由输出标准化给出，独立于输入。</p>
<p><strong>隐私会计</strong>：一个epoch是指对私有训练语料库的完整遍历。每个 (X) 在每个epoch中使用一次。多个epoch的数量 (k) 是一个超参数，通常很小。在同一个 (X) 上重复应用GM需要估计总的隐私损失，因为组合性（除非冻结 (f) 以重新使用 (f(X) + Z)）。众所周知的时刻会计方法【1】（或其对Rényi DP的推广【53】）只提供了一个较松的上界，当存在无界矩时甚至不适用。高斯DP【10】提出了基于中心极限定理的会计方法，但这会导致通过较低的界限显著低估隐私损失。我们转而使用最近的一个数值会计方法【32】，它通过逼近任意准确度的真实隐私消耗来超越RDP或GDP。该方法通过截断并离散化具有其概率密度函数的机制的PLRVs（概率比随机变量）来进行计算【32】。</p>
<p>------------------------------ 分析·Begin ------------------------------<br />
<strong>前噪声层</strong>：指模型中位于添加噪声操作之前的层，这些层直接接受用户的原始输入数据，并生成嵌入表示，所以fine-tune的时候不会更新这些层。<br />
<strong>后噪声层</strong>：指模型中位于添加噪声操作之后的层。</p>
<p>服务器是只更新前噪声层，但是客户端是所有层都更新的，这样做是为了避免服务器学到</p>
<p>------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<h3 id="34-dp-forward结合洗牌与dp-sgd"><a class="markdownIt-Anchor" href="#34-dp-forward结合洗牌与dp-sgd"></a> 3.4 DP-Forward结合洗牌与DP-SGD</h3>
<p>DP-Forward在微调时保证了序列局部差分隐私（SeqLDP），而DP-SGD则提供了中央差分隐私（central DP，针对序列-标签对）。为了在隐私和效用之间进行公平的比较，我们做了两个改进。首先，我们还通过合适的机制对标签进行扰动，以实现标准的局部差分隐私（LDP），即将保护范围从序列扩展到序列-标签对。其次，我们使用洗牌【25】将这种标签保护的DP-Forward与LDP结合，以便声称其达到了与DP-SGD相同的样本级中央差分隐私（CDP）。</p>
<p><strong>离散标签扰动</strong>：对于大多数自然语言处理（NLP）任务，例如GLUE基准测试中的二分类/多分类任务，标签空间的大小（即 (|y|)）通常较小。对于离散数据，使用随机响应（Randomized Response，RR）【70】是一种简单而有效的解决方案。这种方法通过以下概率扰动真实标签 (y) 到自己或其他任意标签：<br />
[<br />
\Pr[y = \hat{y}] = \frac{e<sup>\epsilon}{(e</sup>\epsilon + |y| - 1)}<br />
]<br />
或对于所有 (\hat{y} \neq y) 的情况，均匀选择其他标签，其中 (y) 表示标签空间。</p>
<p>当 (|y|) 很大时，我们可以使用先验知识来“修剪” (y) 到更小的 (|y’|) 【31】。这个先验可以是公开可用的（例如，类似于用户数据的辅助语料库）或者通过多阶段训练逐步从均匀分布中提炼出来【31】。然后，通过最大化输出正确概率（即 (\Pr[y = \hat{y}])）来估计最优 (|y’|)。通过（带先验辅助的）随机响应【31】，我们可以实现完全的LDP。</p>
<p><strong>定理2</strong>：设 (f(·)) 是BERT管道中的前噪声函数，(\mathcal{M}) 是具有参数 (\epsilon_1 \geq 0, 0 \leq \delta \leq 1) 的高斯机制（GM），并且 (\mathcal{M}<em>{RR}) 是具有参数 (\epsilon_2 \geq 0) 的（带先验辅助的）随机响应（RR）。如果DP-Forward微调通过 (\mathcal{M}) 和 (\mathcal{M}</em>{RR}) 分别扰动 (f(X)) 和 (y)，则确保了 ((\epsilon_1 + \epsilon_2, \delta))-LDP。</p>
<p>证明遵循基本组合定理【23】。</p>
<p><strong>通过洗牌放大隐私</strong>：如果将有噪声的嵌入-标签对正确地进行洗牌，那么DP-Forward可以声称具有样本级中央差分隐私（CDP）（如同在DP-SGD中），这“放大”了LDP的保证，达到了 (\Theta(\sqrt{N})) 的水平，其中 (N) 是用户的总数（不需要额外的噪声添加）【25】。我们接下来展示，DP-Forward在类似的隐私条件下，如何从信噪比（SNR）的角度相对于DP-SGD表现出色。</p>
<p>假设进行一个epoch，归一化因子为C。对于DP-SGD，批量大小为 (b)；子抽样概率和训练步骤的数量分别为 (b/N) 和 (N/b)。如果每一步是 ((\epsilon, \delta))-DP，那么使用强组合和子抽样的隐私放大，总的隐私损失是 ((O(\epsilon \sqrt{b/N}), \delta))-DP【1】。</p>
<p>DP-Forward结合洗牌也可以看作是合成了N个子抽样，每个子抽样的比例为1【66】。这相当于 ((O(\epsilon \sqrt{1/N}), \delta))-DP，这也是从 ((\epsilon, \delta))-LDP中“放大”的。为了更容易分析信噪比，我们省略了随机响应的 (\epsilon_2)，因为总体 (\epsilon) 主要由子抽样的高斯噪声控制。因此，我们的高斯噪声方差在每一步中比DP-SGD的要小 (b \times)；在嵌入中的每个条目的信噪比（SNR）与梯度聚合相比，可以估算为DP-Forward的 (O(C/\sqrt{nd})) 对比DP-SGD的 (O(C/\sqrt{d’}))，其中 (d’) 是梯度维度，远大于 (nd)（嵌入矩阵的大小）。</p>
<p>------------------------------ 分析·Begin ------------------------------<br />
服务器接受到多个用户的噪声嵌入和标签后，进行洗牌，也就是说打乱这些数据的顺序，不知道某个数据点来源于哪个客户端。</p>
<p>CDP：中央差分隐私，数据集中所有用户的数据都集中在一个中央服务器上，通过向整个数据集添加噪声来保护隐私。</p>
<p>LDP：每个用户在本地对数据进行扰动来保护隐私。<br />
------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<h3 id="35-dp-forward推理"><a class="markdownIt-Anchor" href="#35-dp-forward推理"></a> 3.5 DP-Forward推理</h3>
<p>在仅提供微调后的流水线部分函数 (f(\cdot)) 的情况下，用户可以从他们的测试序列中导出噪声嵌入矩阵，然后在服务提供商处进行推理，同时确保 ((\epsilon, \delta))-LDP（局部差分隐私）。使用与噪声微调对齐的噪声进行推理对任务准确性也有好处。</p>
<p>本地推理（如在DP-SGD中）不添加噪声，会迫使服务提供商透露其整个流水线，可能会损失其知识产权（intellectual property），并增加 (f(\cdot)) 和 (p(\cdot)) 的时间和存储成本。</p>
<p><strong>定理3</strong>：设 (f(\cdot)) 是微调后的前噪声层（BERT基础管道中的），(\mathcal{M}) 是具有参数 (\epsilon \geq 0, 0 \leq \delta \leq 1) 的高斯机制（GM）。DP-Forward推理在标准化/裁剪后的 (f(\cdot)) 上运行(\mathcal{M})，确保了 ((\epsilon, \delta))-LDP。</p>
<p>证明来源于GM【23】的基本组合。不同于DP-Forward微调，LDP对测试序列同样适用，因为标签是不存在的。</p>
<h3 id="36-dp-forward预训练"><a class="markdownIt-Anchor" href="#36-dp-forward预训练"></a> 3.6 DP-Forward预训练</h3>
<p>直接使用原始的BERT可能无法“匹配”DP-Forward微调/推理，这会降低任务效用。在公共可用文本（例如Wikipedia）上使用DP-Forward进行BERT的预训练，除了使用用户共享的私有数据外，还可以使未来的操作更“适应”噪声。这要求我们修改原始的MLM（掩码语言模型）目标如下方公式（1）所示：</p>
<p>[<br />
L^<em><em>{MLM} = - \sum</em>{i \in I} \log Pr[x_i | \mathcal{M}(f(\hat{X})); \theta^</em>],<br />
]</p>
<p>其中 (\theta^*) 表示“有噪声”BERT的参数。这赋予了有噪声的BERT一些“去噪”能力，因为目标是从噪声嵌入(\mathcal{M}(f(\hat{X})))中预测原始的掩码标记。这实际上不会违反隐私，因为后处理是免费的；每个序列的LDP得到了保证，因为预训练是自监督的（无标签）。这种有噪声的预训练也可以外包给专用的GPU集群，从而可以作为服务提供“去噪BERT”。</p>
<p>去噪作为后处理并不新颖，但大多数已有的方法需要先验知识，例如贝叶斯先验。aGM将其视为一种不寻常的估计问题，因为只观察到单个有噪声的输出，对于每个输入，这可以通过适当的估计方法解决，例如贝叶斯先验【7】。另一种尝试【41】训练了一个独立的有噪声自编码器，其学习到的特征是 (f(X) = X)，在一个图像分类网络前堆叠一个去噪网络，来去除噪声。</p>
<h3 id="41-矩阵变量高斯mvg机制"><a class="markdownIt-Anchor" href="#41-矩阵变量高斯mvg机制"></a> 4.1 矩阵变量高斯（MVG）机制</h3>
<p>与经典的高斯机制（GM）不同，MVG采用的是可能非独立同分布的噪声 (Z \in \mathbb{R}^{n \times d})，该噪声来源于零均值矩阵高斯分布 (\text{MN}_{n,d}(0, \Sigma, \Psi))，其中 (\Sigma \in \mathbb{R}^{n \times n}) 和 (\Psi \in \mathbb{R}^{d \times d}) 分别是行和列的协方差矩阵。直观上，它对“更重要”的行或列添加更少的噪声，以获得更好的实用性。</p>
<p><strong>定义 5（矩阵高斯分布）</strong>：随机变量 (Z) 服从 (\text{MN}_{n,d}(0, \Sigma, \Psi)) 时的概率密度函数（PDF）为：<br />
[<br />
\text{Pr}(Z | 0, \Sigma, \Psi) = \frac{\exp\left( -\frac{1}{2} |U^{-1} Z V<sup>{-\top}|</sup>2_F \right)}{(2\pi)^{nd/2} |\Psi|^{d/2} |\Sigma|^{n/2}},<br />
]<br />
其中 (U \in \mathbb{R}^{n \times n}) 和 (V \in \mathbb{R}^{d \times d}) 是可逆矩阵，满足 (\Sigma = UU^\top) 和 (\Psi = VV^\top)，(| \cdot |) 表示矩阵的行列式【34】。</p>
<p>该定义等价于用矩阵迹给出的常规形式。它推广了GM中使用的单变量高斯分布；当 (\Sigma) 和 (\Psi) 是对角矩阵且数值相同时，(Z) 是独立同分布的。下面引用了MVG机制关于((\epsilon, \delta))-DP的主要定理。</p>
<p><strong>定理 4（MVG机制与((\epsilon, \delta))-DP [14]）</strong>：令<br />
[<br />
\sigma(\Sigma^{-1}) = [\sigma_1(\Sigma^{-1}), \ldots, \sigma_n(\Sigma<sup>{-1})]</sup>\top,<br />
]<br />
[<br />
\sigma(\Psi^{-1}) = [\sigma_1(\Psi^{-1}), \ldots, \sigma_d(\Psi<sup>{-1})]</sup>\top<br />
]<br />
分别为(\Sigma^{-1}) 和 (\Psi^{-1}) 的（按降序排列的）奇异值向量。如果噪声来自矩阵高斯分布 (\text{MN}_{n,d}(0, \Sigma, \Psi))，则MVG机制满足((\epsilon, \delta))-DP，当且仅当：<br />
[<br />
|\sigma(\Sigma^{-1})|<em>2 \cdot |\sigma(\Psi^{-1})|<em>2 \leq \frac{\left(-\beta + \sqrt{\beta^2 + 8\alpha\epsilon}\right)<sup>2}{4\alpha</sup>2},<br />
]<br />
其中 (\alpha = [H_r + H</em>{r,1/2}] \gamma^2 + 2H_r \gamma S_2(f), \beta = 2(nd)^{1/4} H_r S_2(f) \zeta(\delta))，(H_r)（或(H</em>{r,1/2})）是阶数为(r)（或1/2）的广义调和数，(\gamma) 是 (\sup_X ||f(X)||_F)，且 (\zeta(\delta) = 2\sqrt{-nd \ln \delta - 2 \ln \delta + nd})。</p>
<p>为了说明MVG机制的工作原理，我们引用了一个示例【14】：在一个肝病数据集【49】（包含6个特征和248个样本，即 (f \in \mathbb{R}^{248 \times 6})）上执行回归任务。MVG基于某些先验知识，将“ALT”和教师标签视为两个最具指示性的特征，因此为这些特征添加了较少的噪声【14】。为了报告最佳的实验结果，它尝试了不同的精度预算（或噪声方差）分配策略，以确保总预算（定理4）不超支。例如，它将总预算的 (\tau &gt; 50%)（一个可调参数）分配给两个重要特征，其余分配给其他四个特征。与使用独立同分布高斯噪声的GM相比，MVG在相同的隐私级别下可以将均方根误差（RMSE）最多提高0.003【14】。</p>
<p><strong>MVG的次优性</strong>：定理4对两个奇异值向量(\sigma(\Sigma<sup>{-1}))和(\sigma(\Psi</sup>{-1}))的 (L_2)-范数乘积给出了一个上界，假设 (||f(X)||_F) 被一个常数 (\gamma) 对任何(X)有界。该上界随(\beta)的增加单调递减，且依赖于(nd)，当(nd \to \infty)时趋向于0，使得噪声方差的总和变大。在高隐私范围(\epsilon \to 0)时也存在类似的情况。</p>
<p>至少有两个松弛导致了次优性。首先是由于一个充分但非必要的条件用于 ((\epsilon, \delta))-DP【23】：(\text{Pr}[\mathcal{L}<em>{M,X,X’} \geq \epsilon] \leq \delta)，这也用于经典的GM。借助Laurent-Massart定理【40】，MVG进一步将其转化为(\text{Pr}[\mathcal{L}</em>{M,X’,X} \leq \epsilon] = 1)，适用于所有可能输出的一个子集。第二个松弛存在于一个宽松的基于矩阵迹的隐私分析中；后续研究【75】从定义5和一个矩阵范数不等式中得到了一个更紧的界。</p>
<h3 id="算法"><a class="markdownIt-Anchor" href="#算法"></a> 算法</h3>
<p><img src="https://pic.imgdb.cn/item/66d94d82d9c307b7e9636f93.png" alt="" /></p>
<p>------------------------------ 分析·Begin ------------------------------<br />
通过给定的隐私参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi></mrow><annotation encoding="application/x-tex">\xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.04601em;">ξ</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span></span></span></span>,计算出一个上界</p>
<p>MVG相比于GM能够在某行情况下表现得更好吗，但是它并不是最优的。</p>
<p>------------------------------ 分析·End<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1em"/></mrow><annotation encoding="application/x-tex">\quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:1em;"></span></span></span></span>------------------------------</p>
<h2 id="5-实验"><a class="markdownIt-Anchor" href="#5-实验"></a> 5 实验</h2>
<h3 id="51-实验设置"><a class="markdownIt-Anchor" href="#51-实验设置"></a> 5.1 实验设置</h3>
<p>我们使用了三个在NLP/DP文献和GLUE基准测试中广泛使用的典型数据集/任务【45, 78-80】和GLUE基准【69】：1）斯坦福情感树库（SST-2），2）互联网电影数据库（IMDb）【47】用于单句和多句电影评论的二元情感分类，以及3）Quora问题对（QQP）用于在Quora.com上进行语义等价性测试。这些测试集没有任何标签；我们使用原始的开发集作为测试集。表2总结了它们的特征。它们都有隐私风险；例如，帖子的风格特征可能会泄露作者的身份。我们使用任务的准确率（相对于真实标签）作为效用指标。</p>
<p><strong>基线</strong>：<br />
我们在DP-Forward中通过经典高斯机制（GM），MVG【14】和aMGM来实例化机制(M)。如果没有特别说明，所有结果均基于aMGM。对于MVG，我们采用其单模态类型，适用于非对称函数，如预噪声层(f(\cdot))。具体而言，我们使行向的噪声方向性一致，并为每一行分配相同的精度预算，假设tokens具有相同的重要性。</p>
<p>默认情况下，我们报告使用DP-Forward微调的任务的DP-Forward推理的准确性（与“DP-Forward微调 + 非隐私推理”相比，约有2个百分点的提升）。我们还使用最新的Opacus【77】实现DP-SGD微调，但不会对其推理添加任何噪声。另一个基线是非隐私设置（在微调和推理中都没有隐私）。</p>
<p><strong>实现</strong>：<br />
我们在配备Tesla P100 GPU的集群上运行实验。我们用Python实现了所有机制和基线。我们使用来自Huggingface transformers库的原始BERT检查点<code>bert-base-uncased</code>【27】进行微调（第3.3节），或者进一步在WikiCorpus上进行预训练（第3.6节）。</p>
<p>在整个实验中使用的超参数如下：设训练轮数 (k = 3)，学习率 (\eta = 2e-5)，批量大小 (b = 32)，归一化/剪辑因子 (C = 1)。我们保持其他参数不变（例如，没有权重衰减，没有学习率衰减），默认遵循文献【69】。隐私参数 (\delta) 固定为 (1e-5)【78】。</p>
<h3 id="52-配置矩阵维度"><a class="markdownIt-Anchor" href="#52-配置矩阵维度"></a> 5.2 配置矩阵维度</h3>
<p><strong>序列长度</strong> (n) 是可变的。尽管隐藏层维度 (d) 固定为768（对应于BERT-Base），我们可以采用两个线性映射来“调节”它（见第3.2节）。由于我们对大小为 (n \times d) 的嵌入矩阵进行了归一化，使其具有固定的范数 (C)，因此每个条目的信号幅度取决于 ( (n, d) )。相反，在给定固定的 (C) 和隐私参数的情况下，噪声方差是相同的。影响准确性的信噪比（SNR）可以基于 ( (n, d) ) 进行配置。</p>
<p><img src="https://pic.imgdb.cn/item/66d9608fd9c307b7e97f7a2f.png" alt="" /></p>
<p>图2展示了在使用DP-Forward微调的SST-2上评估准确率，其中 (n) 的取值范围从16到256。我们研究在五个隐藏层的输出上添加aMGM噪声。结果表明，最佳准确率通常在 (n = 64) 或 (128) 时实现，所以我们在后续实验中选择 (n = 128)（这对于大多数序列来说是足够的）。</p>
<p><img src="https://pic.imgdb.cn/item/66d96545d9c307b7e98404b7.png" alt="" /><br />
我们在有噪声的输出嵌入上微调SST-2，在不同的 (\epsilon) 和减小的 (d) 下进行实验。表3总结了结果。减小 (d) 在固定 (C) 和 (n) 下会导致较大的信噪比（SNR），但也可能丢失有用的信息，降低准确性。在相同的 (\epsilon) 下，大多数准确性的变化在不同的 (d) 选择下均在2个百分点以内。综合考虑，我们在后续实验中使用原始的 (d = 768)，因为这样无需添加额外的变化（包括两个线性映射），便于流水线处理。</p>
<p><strong>这张图展示了在不同编码器子层中添加噪声，对模型性能的影响。</strong></p>
<h3 id="53-使用序列局部差分隐私sequence-ldp进行微调"><a class="markdownIt-Anchor" href="#53-使用序列局部差分隐私sequence-ldp进行微调"></a> 5.3 使用序列局部差分隐私（Sequence LDP）进行微调</h3>
<p>我们的方法也支持在微调过程中扰动子层的输出。我们以六个编码器为例进行研究，结果如图3所示。总体来说，DP-Forward在更深的编码器中表现更好，因为较少的参数会在微调过程中直接受到噪声的影响。另一个观察结果是，即使在同一个编码器内部，扰动不同子层的输出也可能导致准确性的大幅变化；例如，使用编码器1的最后一个子层的有噪声输出，相较于使用第一个子层的有噪声输出可以带来约20个百分点的增益。</p>
<p>接下来，我们评估在不同的 (\epsilon) 值下的隐私-准确性权衡，并使用经典GM、MVG【14】和aMGM进行实例化比较。请注意，我们仍然使用GM方差(\sigma^2 = 2\ln(1.25/\delta)S_2<sup>2(f)/\epsilon</sup>2)来进行经验评估，尽管它在单次运行中不能扩展到(\epsilon &gt; 1)以确保理论上的差分隐私保证。</p>
<p>对于GM和aMGM为基础的实例化，表4显示了所有三个任务的准确性随着(\epsilon)的增加而增加。由于aMGM在所有(\epsilon)值的选择中产生的噪声较小，我们的方法的准确性比基于GM的方法要好。尽管随着(\epsilon)的减小，GM和aMGM之间的噪声方差差距扩大，但在高隐私环境（(\epsilon &lt; 1)）中，无法进行有效模型的精细微调。基于MVG的方法对于所有三个任务表现得像随机猜测，因为其噪声方差与(n \cdot d)成正比，这在高维设置中甚至比经典GM更大（见第4.1节）。例如，在相同的参数设置下（如(n = 128, d = 768, \epsilon = 8)），MVG生成的噪声的方差比aMGM大数个数量级（例如，(&gt;10^8) vs. (~0.6)），即使假设(\sup |f(\cdot)|_F = 1)。</p>
<p>我们要指出的是，使用的局部(\epsilon)值并不大。大多数认为这样的(\epsilon)值在统计分析中属于低隐私范围的经典LDP方法形成鲜明对比。我们旨在对具有高维信号和有限训练数据的LM-based pipeline进行微调，这要复杂得多。许多先前的工作【28, 29, 58, 80】使用更大的(\epsilon)值来确保甚至对弱token-level LDP变体的有效隐私保护，而其他工作【51】将(\epsilon \leq 10)和(\epsilon \leq 20)分别分类为强和中等隐私。对于序列级LDP，如我们所用的，它们提供有效的保护，同时仍然保持适用性，如第6节所述。</p>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2024/09/02/CSS-2024-DF-Forward/" data-id="cm17vq3770002blv60gdy4xkz" data-title="CSS 2024:DF-Forward"
               class="article-share-link">分享</a>
            
            
            
            

        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://pic.imgdb.cn/item/66eaea6df21886ccc0f74e79.png" data-sizes="auto" alt="IELTS:Writing Task 1"
                         class="lazyload">
                
                <a href="/2024/09/18/IELTS-Writing-Task-1/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        IELTS:Writing Task 1
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://pbs.twimg.com/media/GO1SQD0W0AAmWnk?format=jpg&amp;name=medium" data-sizes="auto" alt="PETS 2024:SIGMA"
                         class="lazyload">
                
                <a href="/2024/09/01/PETS-2024-SIGMA/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        PETS 2024:SIGMA
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="ab" class="lazyload">
            <div class="sidebar-author-name">ab</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">304</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">24</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">299</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe style="border-radius:12px" src="https://open.spotify.com/embed/episode/0Si54sXCWmTfhvA8dSwNoR?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Competition/">Competition</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Future/">Future</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/Application-Season/">Application Season</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/IELTS/">IELTS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/FL/">FL</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/FL/Privacy/">Privacy</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/Multimudal/">Multimudal</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS224N/">CS224N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS231N/">CS231N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/Missing-Semester-of-CS/">Missing Semester of CS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 11.54px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/2/" style="font-size: 11.54px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 10.77px;">3</a> <a href="/tags/4/" style="font-size: 10px;">4</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/Accumulate/" style="font-size: 17.69px;">Accumulate</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.54px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/AimGraduate/" style="font-size: 15.38px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 12.31px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10.77px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 12.31px;">CNN</a> <a href="/tags/CS224N/" style="font-size: 10.77px;">CS224N</a> <a href="/tags/CS231N/" style="font-size: 10px;">CS231N</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 13.85px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/ComPetition/" style="font-size: 10px;">ComPetition</a> <a href="/tags/Competition/" style="font-size: 16.15px;">Competition</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.77px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Filter/" style="font-size: 10px;">Filter</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/Future/" style="font-size: 15.38px;">Future</a> <a href="/tags/GB/" style="font-size: 10px;">GB</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GiB/" style="font-size: 10px;">GiB</a> <a href="/tags/Git/" style="font-size: 10.77px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.92px;">GoAbroad</a> <a href="/tags/Graduate/" style="font-size: 10px;">Graduate</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/IELTS/" style="font-size: 13.08px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/LayerNorm/" style="font-size: 10px;">LayerNorm</a> <a href="/tags/Lec01/" style="font-size: 11.54px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.77px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 12.31px;">Linux</a> <a href="/tags/Listening/" style="font-size: 10px;">Listening</a> <a href="/tags/M2/" style="font-size: 10.77px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 13.08px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 15.38px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.77px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 11.54px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NECCS/" style="font-size: 10px;">NECCS</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.77px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Reading/" style="font-size: 10px;">Reading</a> <a href="/tags/ResNet/" style="font-size: 10.77px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SNN/" style="font-size: 13.08px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Script/" style="font-size: 10px;">Script</a> <a href="/tags/Shell/" style="font-size: 10.77px;">Shell</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-Neural-Network-for-Ultra-low-latency-and-High-accurate-Object-Detection/" style="font-size: 10px;">Spiking Neural Network for Ultra-low-latency and High-accurate Object Detection</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.77px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.77px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/Undergraduate/" style="font-size: 10px;">Undergraduate</a> <a href="/tags/University/" style="font-size: 13.08px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 10px;">ViT</a> <a href="/tags/Vim/" style="font-size: 10px;">Vim</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.77px;">Yuxiao Dong</a> <a href="/tags/alexnet/" style="font-size: 10px;">alexnet</a> <a href="/tags/anygpt/" style="font-size: 10px;">anygpt</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 12.31px;">bert</a> <a href="/tags/blip2/" style="font-size: 10px;">blip2</a> <a href="/tags/bug/" style="font-size: 16.92px;">bug</a> <a href="/tags/cat/" style="font-size: 10px;">cat</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/chmod/" style="font-size: 10px;">chmod</a> <a href="/tags/chrome/" style="font-size: 10px;">chrome</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/code/" style="font-size: 10.77px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/commit/" style="font-size: 10px;">commit</a> <a href="/tags/competition/" style="font-size: 10px;">competition</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/copilot/" style="font-size: 10.77px;">copilot</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10.77px;">cuda</a> <a href="/tags/d2l/" style="font-size: 14.62px;">d2l</a> <a href="/tags/database/" style="font-size: 10px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.77px;">deep neural network</a> <a href="/tags/delete/" style="font-size: 10px;">delete</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/dowhy/" style="font-size: 10.77px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/embedding/" style="font-size: 10px;">embedding</a> <a href="/tags/explainer/" style="font-size: 10.77px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/github/" style="font-size: 13.08px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 11.54px;">gpu</a> <a href="/tags/hexo/" style="font-size: 10.77px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/import/" style="font-size: 10px;">import</a> <a href="/tags/instructor/" style="font-size: 12.31px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 12.31px;">intern00</a> <a href="/tags/interns/" style="font-size: 10px;">interns</a> <a href="/tags/internship/" style="font-size: 19.23px;">internship</a> <a href="/tags/interview/" style="font-size: 10px;">interview</a> <a href="/tags/introduction/" style="font-size: 10px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/linux/" style="font-size: 11.54px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lr/" style="font-size: 10px;">lr</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 13.08px;">mac</a> <a href="/tags/memory/" style="font-size: 12.31px;">memory</a> <a href="/tags/mentor/" style="font-size: 10.77px;">mentor</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.77px;">neuromorphic computing</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/olive/" style="font-size: 10px;">olive</a> <a href="/tags/os/" style="font-size: 12.31px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/paper/" style="font-size: 20px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 12.31px;">prml</a> <a href="/tags/profile/" style="font-size: 10px;">profile</a> <a href="/tags/project/" style="font-size: 13.85px;">project</a> <a href="/tags/prompt/" style="font-size: 10px;">prompt</a> <a href="/tags/pycharm/" style="font-size: 10px;">pycharm</a> <a href="/tags/python/" style="font-size: 10.77px;">python</a> <a href="/tags/pytorch/" style="font-size: 16.15px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/register/" style="font-size: 10px;">register</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 10px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/server/" style="font-size: 10px;">server</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a> <a href="/tags/sora/" style="font-size: 10px;">sora</a> <a href="/tags/spike/" style="font-size: 10.77px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.77px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikebert/" style="font-size: 10px;">spikebert</a> <a href="/tags/spikingjelly/" style="font-size: 13.08px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.77px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.77px;">ssh</a> <a href="/tags/sta/" style="font-size: 10px;">sta</a> <a href="/tags/terminal/" style="font-size: 10px;">terminal</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.77px;">tips</a> <a href="/tags/tittle/" style="font-size: 10px;">tittle</a> <a href="/tags/tmux/" style="font-size: 10px;">tmux</a> <a href="/tags/tool/" style="font-size: 18.46px;">tool</a> <a href="/tags/transformer/" style="font-size: 13.85px;">transformer</a> <a href="/tags/transformers/" style="font-size: 10px;">transformers</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10.77px;">vscode</a> <a href="/tags/wakatime/" style="font-size: 10px;">wakatime</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/yeild/" style="font-size: 10px;">yeild</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 18.46px;">专业知识</a> <a href="/tags/%E4%B8%93%E7%A1%95/" style="font-size: 10px;">专硕</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/" style="font-size: 10px;">中国大学生计算机设计大赛</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E4%BB%A3%E7%90%86/" style="font-size: 10px;">代理</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">公选课</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.77px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7/" style="font-size: 10px;">写作技巧</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" style="font-size: 10px;">博弈论</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">大作业</a> <a href="/tags/%E5%A4%A7%E5%88%9B/" style="font-size: 10px;">大创</a> <a href="/tags/%E5%A4%A7%E8%8B%B1%E8%B5%9B/" style="font-size: 10px;">大英赛</a> <a href="/tags/%E5%AD%A6%E7%A1%95/" style="font-size: 10px;">学硕</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.77px;">审稿意见</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%82%84%E6%82%84%E8%AF%9D/" style="font-size: 10px;">悄悄话</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%99%E8%82%B2%E8%AE%B8%E5%8F%AF/" style="font-size: 10px;">教育许可</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.77px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 10px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.77px;">服务器</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">朴素贝叶斯</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 12.31px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.77px;">李宏毅</a> <a href="/tags/%E6%9D%8E%E6%B2%90/" style="font-size: 10px;">李沐</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%A7%91%E8%BD%AF/" style="font-size: 10px;">科软</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E7%BB%A7%E6%89%BF/" style="font-size: 10px;">继承</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3/" style="font-size: 10px;">脑机接口</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%8A%82%E8%83%BD%E5%87%8F%E6%8E%92/" style="font-size: 11.54px;">节能减排</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%AD%E4%B9%89%E7%A9%BA%E9%97%B4/" style="font-size: 10px;">语义空间</a> <a href="/tags/%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">课程</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 10.77px;">调研</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%87%8F%E5%8C%96/" style="font-size: 10px;">量化</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%B8%BF%E9%9B%81%E6%9D%AF/" style="font-size: 10px;">鸿雁杯</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">九月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">八月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">七月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">六月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">五月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">四月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                ab
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    708.7k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    43:41
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="ab" class="lazyload">
            <div class="sidebar-author-name">ab</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">304</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">24</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">299</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

