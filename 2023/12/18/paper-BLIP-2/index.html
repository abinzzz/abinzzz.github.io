
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        paper:BLIP-2 |
        
        Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models 第一作者：A">
<meta property="og:type" content="article">
<meta property="og:title" content="paper:BLIP-2">
<meta property="og:url" content="https://abinzzz.github.io/2023/12/18/paper-BLIP-2/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models 第一作者：A">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/GBnUgmnbEAAX3-r?format=png&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/GBnVs4WaIAA8lpx?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GBnWD66boAA9bAK?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GBkssmPasAAU6iL?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/GBkoaBbaQAA_yp5?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GBkvC-UakAABQe6?format=jpg&amp;name=medium">
<meta property="article:published_time" content="2023-12-17T18:41:27.000Z">
<meta property="article:modified_time" content="2023-12-18T08:51:27.927Z">
<meta property="article:author" content="あまのひな">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="BLIP-2">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/GBnUgmnbEAAX3-r?format=png&amp;name=small">
    
        <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://pbs.twimg.com/media/GBTN7RnW0AA6G9K?format=jpg&amp;name=medium" data-sizes="auto" alt="paper:BLIP-2" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>paper:BLIP-2</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-paper-BLIP-2" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/12/18/paper-BLIP-2/" class="article-date-link">
        <time datetime="2023-12-17T18:41:27.000Z"
              itemprop="datePublished">2023-12-18</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/paper/">paper</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h1 id="blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models"><a class="markdownIt-Anchor" href="#blip-2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models"></a> BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h1>
<p>第一作者：<a target="_blank" rel="noopener" href="https://sites.google.com/site/junnanlics/">Author：Junnan Li</a></p>
<p>发布日期：2023.1</p>
<br>
<h2 id="参考链接"><a class="markdownIt-Anchor" href="#参考链接"></a> 参考链接</h2>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.12597.pdf">paper: BLIP-2</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">code: BLIP-2</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1fA411Z772/">朱老师：多模态论文串讲</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Salesforce/BLIP2">HF上的Demo</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41994006/article/details/129221701">BLIP2-图像文本预训练论文解读</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624647342">BLIP2论文简洁摘要</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/606364639">BLIP2: 下一代多模态模型的雏形</a></li>
</ul>
<br>
<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<h3 id="从泰坦尼克号说起"><a class="markdownIt-Anchor" href="#从泰坦尼克号说起"></a> <code>从泰坦尼克号说起</code></h3>
<p><img src="https://pbs.twimg.com/media/GBnUgmnbEAAX3-r?format=png&amp;name=small" alt="" /></p>
<p>开始前介绍论文前我们先来讨论下，实现图片中的问答，需要什么能力呢？</p>
<ul>
<li>图片里发生了什么：一位男士在船头搂着一位女士。（感知-CV模型的能力）</li>
<li>问题问的什么：电影的结尾是什么？（感知-NLP模型的能力）</li>
<li>图片和电影有什么关系：这是泰坦尼克号里的经典镜头。（对齐融合-多模态模型的能力）</li>
<li>电影的结尾是什么：泰坦尼克号沉没了。（推理-LLM模型的能力</li>
</ul>
<br>
<h3 id="对不同模型扮演的角色理解"><a class="markdownIt-Anchor" href="#对不同模型扮演的角色理解"></a> <code>对不同模型扮演的角色理解</code></h3>
<p>从上面的问题可以看出，为了解决这个问题，需要几个模型配合一下。其实自从多模态模型（特别是图文多模态模型）出现，模态之间怎么配合就是个问题。</p>
<p>19年20年的时候，ViLBERT和Uniter采用了Object-Text对来提升模型对图片的理解能力。Object的引入，不可避免的需要一个笨重的检测器，去检测各种框，使得图像模态显得比较笨重。而且检测器模型不可避免的会存在漏检的问题，可以参考后来Open-Vocabulary一些工作，比如ViLD。这一阶段，显然对图像的理解是多模态的重头戏，文本更多是辅助图像任务的理解。</p>
<p><img src="https://pbs.twimg.com/media/GBnVs4WaIAA8lpx?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>到了21年22年，去掉检测器成了主流，ViLT，ALBEF，VLMo，BLIP 等等都抛弃了检测器，彻底摆脱了CNN网络的舒服，全面拥抱Transformer，当然这也得益于本身ViT模型在CV领域的大放光彩，让两个模态的有机融合成为了可能。在这一阶段，文本模态感觉已经可以和图像模态平起平坐了。从在各项具体下游任务（VQA、VG、ITR）的实际表现上来说，已经比较令人满意了。但总感觉差点味道，就是复杂推理。比如VQA上的问题，大多数是简单的逻辑计算或识别，感觉还不够智能。</p>
<p>那么如何实现更加复杂的推理呢？众所周知，NLP领域一直领先于CV领域的发展。得益于更丰富的语料库，NLP领域的已经拥有了一些具有初步推理能力模型的研究，特别是LLM大模型的出现。（今天谷歌刚刚发布了22B的ViT，而在NLP领域这个规模的模型应该已经不算新闻了。）我对于LLM能力有多强的理解，其实也是ChatGPT之后才有明确的感知。</p>
<p><img src="https://pbs.twimg.com/media/GBnWD66boAA9bAK?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>23年1月，BLIP2出来了，引入了LLM。从图像上看，BLIP2大概由这么几个部分组成，图像（Image）输入了图像编码器（Image Encoder），得到的结果与文本（Text）在Q-Former（BERT初始化）里进行融合，最后送入LLM模型。</p>
<ul>
<li>图像和文本：自然信号；</li>
<li>图像编码器（Image Encoder）：传感器（图像）；</li>
<li>Q-Former：传感器（文本）+ 融合算法（Query）；</li>
<li>LLM：处理器。</li>
</ul>
<p>之前的模型大多都关注在了传感器和融合算法的设计上，但忽略了处理器的重要作用。BERT模型虽然能理解文本，但却没有世界观的概念，没有庞大的背景知识库，只能作一个传感器。只有LLM模型，才能实现这一角色，统一起各个模态的信号，从一个宏观的角度去看待这个问题。这里引用一段原文中的话。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Powered by LLMs (e.g. OPT (Zhang et al., 2022), FlanT5 (Chung et al., 2022)), BLIP-2 can be prompted to perform zero-shot image-to-text generation that follows natural language instructions, which enables emerging capabilities such as visual knowledge reasoning, visual conversation, etc.</span><br></pre></td></tr></table></figure>
<p>目前看，或许LLM就是下一代多模态模型的关键一环。</p>
<br>
<h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2>
<p>训练大尺度视觉语言预训练模型成本比较高，BLIP-2，基于现有的图像编码器预训练模型，大规模语言模型进行预训练视觉语言模型；BLIP-2通过轻量级两阶段预训练模型Querying Transformer缩小模态之间gap，第一阶段从冻结图像编码器学习视觉语言表征，第二阶段基于冻结语言模型，进行视觉到语言生成学习；</p>
<br>
<h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2>
<p><strong>目前存在的问题</strong>: 端到端训练视觉语言模型需要大尺度模型及大规模数据，该过程成本大，本文提出方法基于现有高质量视觉模型及语言大模型进行联合训练，为减少计算量及防止遗忘，作者对预训练模型进行frozen，为了将两任务对齐，作者提出Querying Transformer (Q- Former) 预训练，如图1，其将有用视觉特征传递至LLM输出目标文本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">端到端训练是一种机器学习训练方法，它直接从输入数据到最终输出目标进行训练，不需要人工干预或特定特征的提取。</span><br></pre></td></tr></table></figure>
<p><img src="https://pbs.twimg.com/media/GBkssmPasAAU6iL?format=png&amp;name=900x900" alt="" /></p>
<p>BLIP-2优势如下：</p>
<ul>
<li>高效利用frozen预训练视觉及语言模型；</li>
<li>由于大规模语言模型能力，BLIP-2可以根据提示进行zero-shot图像到文本生成；- 由于使用frozen单模态预训练模型，BLIP-2与现有SOTA方案相比，计算更加高效；</li>
</ul>
<br>
<p>为了对齐视觉特征到LLM文本空间，作者提出Q-Former，进行两阶段预训练：</p>
<ul>
<li>图像编码器frozen进行学习视觉语言表征；</li>
<li>使用frozen LLM进行学习视觉到文本生成；</li>
</ul>
<br>
<h2 id="related-work"><a class="markdownIt-Anchor" href="#related-work"></a> Related Work</h2>
<h4 id="end-to-end-vision-language-pre-training"><a class="markdownIt-Anchor" href="#end-to-end-vision-language-pre-training"></a> <code>End-to-end Vision-Language Pre-training</code></h4>
<p>端到端的视觉-语言预训练（VLP）的目标是学习多模态基础模型，以提高在各种视觉和语言任务上的性能。文章提到了几种不同的模型架构，包括双编码器、融合编码器、编码器-解码器以及统一的变换器架构。此外，还介绍了几种预训练目标，如图像-文本对比学习、图像-文本匹配和（遮蔽的）语言建模。大多数VLP方法使用大规模的图像-文本对数据集进行端到端预训练，随着模型大小的增加，预训练的计算成本极高。而且，端到端预训练的模型难以利用现有的单模态预训练模型，例如大型语言模型（LLMs）。</p>
<br>
<h4 id="modular-vision-language-pre-training"><a class="markdownIt-Anchor" href="#modular-vision-language-pre-training"></a> <code>Modular Vision-Language Pre-training</code></h4>
<p>这些方法利用现成的预训练模型，并在视觉-语言预训练（VLP）过程中保持这些模型不变（冻结状态）。一些方法冻结图像编码器，另一些方法冻结语言模型，用于视觉到语言的生成任务。关键挑战是将视觉特征与文本空间对齐。例如，Frozen方法微调图像编码器的输出作为大型语言模型（LLM）的软提示，而Flamingo方法在LLM中插入新的交叉注意力层。与现有方法不同，BLIP-2可以有效且高效地利用冻结的图像编码器和LLM，以较低的计算成本在各种视觉-语言任务上实现更强的性能。</p>
<br>
<h2 id="method"><a class="markdownIt-Anchor" href="#method"></a> Method</h2>
<p>BLIP-2是一种新的视觉-语言预训练方法，它基于冻结的单模态预训练模型。为了弥合模态间的差距，提出了一个分两阶段预训练的查询变换器（Q-Former）：第一阶段是利用冻结的图像编码器进行视觉-语言表示学习，第二阶段是利用冻结的大型语言模型（LLM）进行视觉到语言的生成学习。</p>
<br>
<h3 id="model-architecture如何统一多模态的表征"><a class="markdownIt-Anchor" href="#model-architecture如何统一多模态的表征"></a> <code>Model Architecture(如何统一多模态的表征)</code></h3>
<p>LLM本质上是个语言模型，自然无法直接接受其他模态的信息。所以如何把各个模态的信息，统一到LLM能理解的特征空间，就是第一步要解决的问题。为此，作者提出了Q-Former。</p>
<p>为了融合特征，那Transformer架构是最合适不过的了。熟悉ALBEF或者BLIP的同学或许发现，Q-Former的结构和ALBEF其实很像，如果看代码的话，可以发现就是在ALBEF基础上改的。</p>
<p>相较于ALBEF，最大的不同，就是Learned Query的引入。可以看到这些Query通过Cross-Attention与图像的特征交互，通过Self-Attention与文本的特征交互。这样做的好处有两个：（1）这些Query是基于两种模态信息得到的；（2）无论多大的视觉Backbone，最后都是Query长度的特征输出，大大降低了计算量。比如在实际实验中，ViT-L/14的模型的输出的特征是257x1024的大小，最后也是32x768的Query特征。</p>
<p>Q-Former作为一个可训练模块，旨在弥合冻结的图像编码器和冻结的大型语言模型（LLM）之间的差距。它从图像编码器中提取固定数量的输出特征，这些特征与输入图像的分辨率无关。</p>
<p>Q-Former包括两个自共享self-attention层的transformer子模块：</p>
<ul>
<li>图像transformer（Q-Former左半部分）与frozen image encoder相互作用提取视觉特征；</li>
<li>文本transformer（Q-Former右半部分）可作为文本编码器，也可作为文本解码器。</li>
</ul>
<p><img src="https://pbs.twimg.com/media/GBkoaBbaQAA_yp5?format=jpg&amp;name=medium" alt="" /></p>
<br>
<h3 id="bootstrap-vision-language-representation-learning-from-a-frozen-image-encoder"><a class="markdownIt-Anchor" href="#bootstrap-vision-language-representation-learning-from-a-frozen-image-encoder"></a> <code>Bootstrap Vision-Language Representation Learning from a Frozen Image Encoder</code></h3>
<p>query通过学习提升与text相关视觉表征，受BLIP启发，作者通过3个目标函数，共享相同输入格式及模型参数，每个目标函数通过不同attention mask策略控制query与text之间相互影响</p>
<p><strong>图像-文本对比学习（ITC）</strong>：ITC学习对齐图像表征与文本表征，通过比较成对与非成对的图像-文本相似度实现；计算过程如下：<br />
计算image transformer输出query表征Z ZZ（与可学习query长度相同）与text transformer输出文本表征 t tt 中【CLS】token相似性，选取最大值作为图像文本对相似度，为<strong>防止信息泄露，作者使用单模态self-attention mask，query与text不能互相可见，防止从文本直接学习</strong>；由于image encoder进行frozen，显存释放，可以使用batch负样本而不用像BLIP中使用队列。</p>
<p><strong>基于图像的文本生成（ITG）损失</strong>：ITG根据输入图像训练Q-Former生成文本，由于Q-Former不允许image encoder与text token直接交互，文本生成所需信息通过query进行提取，通过self-attention进行传递至text token，因此query需要捕获文本相关所有信息，<strong>作者使用多模态因果self-attention mask控制query-text交互，query无法获取text token，当前text token 可获取所有query及其之前text token。作者将【CLS】token替换为【DEC】token 作为解码任务标记；</strong></p>
<p><strong>图像-文本匹配（ITM）</strong>：ITM为了学习精细化图像文本匹配，作者使用bi-dirention self-atttention mask，所有query与text相互可见，因此输出的query embedding Z捕获多模态信息，Z通过二类线性分类器获取logit，logit均值为匹配得分，作者使用《Align before Fuse》中难例负样本挖掘策略创建负样本对。</p>
<p><strong>难例负样本挖掘策略</strong>：<br />
当负样本的图像文本对有相同的语义但在细粒度细节上不同，那么该样本是难样本。作者通过对比相似度寻找batch内的 hard negatives。对于一个batch中的每一幅图像，作者根据对比相似性分布从相同的batch中抽取一个负文本，其中与图像更相似的文本有更高的可能被采样。同样的，作者还为每个文本采样一个hard negative图像。</p>
<br>
<h3 id="bootstrap-vision-to-language-generative-learning-from-a-frozen-llm变成llm认识的样子"><a class="markdownIt-Anchor" href="#bootstrap-vision-to-language-generative-learning-from-a-frozen-llm变成llm认识的样子"></a> <code>Bootstrap Vision-to-Language Generative Learning from a Frozen LLM(变成LLM认识的样子)</code></h3>
<p>通过第一阶段的训练，Query已经浓缩了图片的精华，现在要做的，就是把Query变成LLM认识的样子。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">为什么不让LLM认识Query，而让Query变成LLM认识呢？这里的原因有两：</span><br><span class="line">（1）LLM模型的训练代价有点大；</span><br><span class="line">（2）从 Prompt Learning 的观点来看，目前多模态的数据量不足以保证LLM训练的更好，反而可能会让其丧失泛化性。如果不能让模型适应任务，那就让任务来适应模型。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>作者将Q-Former与LLM相连，后去LLM的语言生成能力。如图3，FC层映射输出的query embedding Z至LLM的text embedding；基于LLM Q-Former提取到的视觉表征作为soft visual prompt，由于Q-Former已经预训练用于提取对文本有用的视觉表征，减轻LLM学习视觉-文本对齐的负担<br />
<img src="https://pbs.twimg.com/media/GBkvC-UakAABQe6?format=jpg&amp;name=medium" alt="" /></p>
<p>这里作者针对两类不同LLM设计了不同的任务：</p>
<ul>
<li>Decoder类型的LLM（如OPT）：以Query做输入，文本做目标；</li>
<li>Encoder-Decoder类型的LLM（如FlanT5）：以Query和一句话的前半段做输入，以后半段做目标；</li>
</ul>
<p>为了适合各模型不同的Embedding维度，作者引入了一个FC层做维度变换</p>
<p>对于encoder-decoder-based LLM，使用prefix language modeling loss预训练，将text分为两部分，text前半部分与视觉表征concat输入LLM编码器，后半部分作为LLM解码器的生成目标。</p>
<br>
<h3 id="model-pre-training"><a class="markdownIt-Anchor" href="#model-pre-training"></a> <code>Model Pre-training</code></h3>
<p>BLIP-2使用与BLIP相同数据，129M图片，包括COCO、Visual Genome、CC3M、CC12M、SBU，其中115M来自 LAION400M，使用CapFilt对网图进行生成caption，具体步骤如下：</p>
<ul>
<li>使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>L</mi><mi>I</mi><msub><mi>P</mi><mrow><mi>l</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">BLIP_{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>生成10个caption；</li>
<li>生成10个caption+原始web caption通过CLIP ViT-L/14模型与对应图像进行相似度排序；</li>
<li>选取top2作为该图的caption，以此作为训练数据；</li>
</ul>
<br>
<p><strong>两个SOTA视觉transformer预训练模型</strong>：ViT-L/14 from CLIP、ViT-G/14 from EVA-CLIP,移除ViT最后一层，使用倒数第二层特征。</p>
<p><strong>LLM模型</strong>：</p>
<ul>
<li>无监督训练的OPT作为decoder-based LLM</li>
<li>基于指令训练的FlanT5作为encoder-decoder-based LLM</li>
</ul>
<br>
<p><strong>预训练设置</strong>:<br />
第一阶段训练250k step，第二阶段训练80k step；ViT和LLM 转为FP16，FlanT5转为BFloat16，作者发现相对于32-bit，性能无下降；由于使用frozen模型，作者预训练比现在大规模VLP方法计算量都小，在16个A100（40G）上，对于ViT-G和FlanT5-XXL第一阶段训练耗时6天，第二阶段少于3天。</p>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/12/18/paper-BLIP-2/" data-id="clq9w74kq0000yd69ecrzbwhv" data-title="paper:BLIP-2"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BLIP-2/" rel="tag">BLIP-2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/" rel="tag">paper</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://pbs.twimg.com/media/GBTN7RnW0AA6G9K?format=jpg&amp;name=medium" data-sizes="auto" alt="paper:多模态论文合集"
                         class="lazyload">
                
                <a href="/2023/12/18/paper-%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        paper:多模态论文合集
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://pbs.twimg.com/media/GBTN7RnW0AA6G9K?format=jpg&amp;name=medium" data-sizes="auto" alt="paper:BLIP"
                         class="lazyload">
                
                <a href="/2023/12/18/paper-BLIP/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        paper:BLIP
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="あまのひな" class="lazyload">
            <div class="sidebar-author-name">あまのひな</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">247</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">23</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">297</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=28854246&auto=1&height=66"></iframe>

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Databse/">Databse</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Missing-Semester-of-CS/">Missing Semester of CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 10.67px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/2/" style="font-size: 12px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 10.67px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/4/" style="font-size: 10.67px;">4</a> <a href="/tags/5/" style="font-size: 10px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 10.67px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 12.67px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.33px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 11.33px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 12px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CV/" style="font-size: 10.67px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 13.33px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 16px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.67px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/Git/" style="font-size: 10.67px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.67px;">GoAbroad</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 10.67px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/Lec01/" style="font-size: 11.33px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.67px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.33px;">Linux</a> <a href="/tags/M2/" style="font-size: 10.67px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.67px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 12.67px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.67px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 10px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 17.33px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OS/" style="font-size: 12.67px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.67px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.33px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.67px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.67px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.67px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/University/" style="font-size: 13.33px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 10.67px;">ViT</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.67px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 12px;">bert</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/bug/" style="font-size: 14px;">bug</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.33px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10.67px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.67px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.67px;">chapter05</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/code/" style="font-size: 11.33px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.33px;">d2l</a> <a href="/tags/database/" style="font-size: 14px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.67px;">deep neural network</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/dowhy/" style="font-size: 10.67px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/explainer/" style="font-size: 10.67px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/github/" style="font-size: 10.67px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 10.67px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/instructor/" style="font-size: 12px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 12px;">intern00</a> <a href="/tags/internship/" style="font-size: 18.67px;">internship</a> <a href="/tags/introduction/" style="font-size: 11.33px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.67px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 11.33px;">mac</a> <a href="/tags/mid/" style="font-size: 10.67px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.67px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.67px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 15.33px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.33px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.33px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 12px;">prml</a> <a href="/tags/pytorch/" style="font-size: 12px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 14.67px;">review</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.33px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/spike/" style="font-size: 10.67px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.67px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikingjelly/" style="font-size: 12.67px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.67px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.67px;">ssh</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.67px;">tips</a> <a href="/tags/tool/" style="font-size: 18px;">tool</a> <a href="/tags/transformer/" style="font-size: 12px;">transformer</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10px;">vscode</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">专业知识</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.67px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">图像描述生成</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.67px;">审稿意见</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">显卡</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.67px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 13.33px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.67px;">服务器</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期中复习</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 10px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.67px;">李宏毅</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">模型训练流程</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 13.33px;">毛概</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.67px;">特征学习</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">第三章</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">评测指标</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.33px;">调研</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">软件概要设计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                あまのひな
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    513.6k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    32:43
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="あまのひな" class="lazyload">
            <div class="sidebar-author-name">あまのひな</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">247</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">23</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">297</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

