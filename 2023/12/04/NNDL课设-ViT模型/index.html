
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        NNDL课设:ViT模型 |
        
        blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });     Code: Vision Transformer + Transformer Decoder for Image Captioning  1.Readme 变换器是一种强大的模型，利用注意力机制。  最初由http">
<meta property="og:type" content="article">
<meta property="og:title" content="NNDL课设:ViT模型">
<meta property="og:url" content="https://abinzzz.github.io/2023/12/04/NNDL%E8%AF%BE%E8%AE%BE-ViT%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });     Code: Vision Transformer + Transformer Decoder for Image Captioning  1.Readme 变换器是一种强大的模型，利用注意力机制。  最初由http">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/GAp9906aAAA_pXl?format=jpg&amp;name=medium">
<meta property="article:published_time" content="2023-12-04T06:35:11.000Z">
<meta property="article:modified_time" content="2023-12-23T06:09:26.561Z">
<meta property="article:author" content="Jerome">
<meta property="article:tag" content="专业知识">
<meta property="article:tag" content="NNDL">
<meta property="article:tag" content="ViT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/GAp9906aAAA_pXl?format=jpg&amp;name=medium">
    
        <link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="NNDL课设:ViT模型" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>NNDL课设:ViT模型</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-NNDL课设-ViT模型" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/12/04/NNDL%E8%AF%BE%E8%AE%BE-ViT%E6%A8%A1%E5%9E%8B/" class="article-date-link">
        <time datetime="2023-12-04T06:35:11.000Z"
              itemprop="datePublished">2023-12-04</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><a class="article-category-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<!-- ## 参考链接
- [Blog: 让Transformer描述图片:一种新型图片转文字（图片字幕）](https://zhuanlan.zhihu.com/p/639823511)
- [Code: Image Captioning Using Vision Transformers](https://github.com/inuwamobarak/Image-captioning-ViT)
- [Code: ViT-for-Image-Captioning](https://github.com/Urezzzer/ViT-for-Image-Captioning)
- [Code: Vision Transformer + Transformer Decoder for Image Captioning](https://github.com/2021-DL-Training-Program/Lab5-Vision-Transformer)
- [Code: transformer-image-captioning](https://github.com/duongttr/Image-Captioning-Transfromers)

<br>

## 数据集

- **Microsoft COCO**：使用最广泛的图像描述生成数据集   -->
<br>
<h1 id="code-vision-transformer-transformer-decoder-for-image-captioning"><a class="markdownIt-Anchor" href="#code-vision-transformer-transformer-decoder-for-image-captioning"></a> Code: Vision Transformer + Transformer Decoder for Image Captioning</h1>
<h2 id="1readme"><a class="markdownIt-Anchor" href="#1readme"></a> 1.Readme</h2>
<p>变换器是一种强大的模型，利用注意力机制。 <br></p>
<p>最初由<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a> 提出，并用于序列到序列任务。<br></p>
<p>后来，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a> 提出了视觉变换器ViT，它直接在图像上应用变换器。<br></p>
<p>在这个任务中，我们使用预训练的<code>google/vit-base-patch16-224</code> 预训练的ViT作为编码器，以及一个变换器解码器。</p>
<br>
<p>你可以通过这个<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1VNbrIE9oFu12QnS_vQAti4r6bIMUBZsB/view?usp=sharing">链接</a> 下载预训练的ViT模型 <br><br />
解压下载的google.zip</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip google.zip</span><br></pre></td></tr></table></figure>
<br>
<p>(1). 下载MSCOCO数据集：<br />
法一：执行脚本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh download.sh</span><br></pre></td></tr></table></figure>
<p>法二：从<a target="_blank" rel="noopener" href="https://cocodataset.org/#download">网站</a>下载<br />
<img src="https://pbs.twimg.com/media/GAp9906aAAA_pXl?format=jpg&amp;name=medium" alt="" /></p>
<p>(2). 数据预处理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python resize.py</span><br></pre></td></tr></table></figure>
<br>
<p>如果出现报错如下报错：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(ab) root@<span class="number">3090</span>_0002:/data10/cyb/Vision-Transformer# python resize.py</span><br><span class="line">Traceback (most recent <span class="keyword">call</span> last):</span><br><span class="line">  File &quot;/data10/cyb/Vision-Transformer/resize.py&quot;, line <span class="number">42</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    main(args)</span><br><span class="line">  File &quot;/data10/cyb/Vision-Transformer/resize.py&quot;, line <span class="number">30</span>, <span class="keyword">in</span> main</span><br><span class="line">    resize_images(image_dir, output_dir, image_size)</span><br><span class="line">  File &quot;/data10/cyb/Vision-Transformer/resize.py&quot;, line <span class="number">20</span>, <span class="keyword">in</span> resize_images</span><br><span class="line">    img = resize_image(img, size)</span><br><span class="line">  File &quot;/data10/cyb/Vision-Transformer/resize.py&quot;, line <span class="number">8</span>, <span class="keyword">in</span> resize_image</span><br><span class="line">    return image.resize(size, Image.ANTIALIAS)</span><br><span class="line"><span class="function">AttributeError: <span class="title">module</span> &#x27;<span class="title">PIL.Image</span>&#x27; <span class="title">has</span> <span class="title">no</span> <span class="title">attribute</span> &#x27;<span class="title">ANTIALIAS</span>&#x27;</span></span><br></pre></td></tr></table></figure>
<p><strong>出错原因</strong>: 原来是在pillow的10.0.0版本中，ANTIALIAS方法被删除了，使用新的方法 <strong>(Image.LANCZOS,Image.Resampling.LANCZOS)</strong> 即可</p>
<p>修改后的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resize_image</span>(<span class="params">image, size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将图像调整为给定的大小&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> image.resize(size, Image.LANCZOS)</span><br></pre></td></tr></table></figure>
<br>
<p>(3). 为标题文本构建词汇表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python build_vocab.py</span><br></pre></td></tr></table></figure>
<p>(4). 安装变换器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure>
<hr />
<p>coco数据集的json文件格式：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;info&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO 2014 Dataset&quot;</span><span class="punctuation">,</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://cocodataset.org&quot;</span><span class="punctuation">,</span><span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1.0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;year&quot;</span><span class="punctuation">:</span> <span class="number">2014</span><span class="punctuation">,</span><span class="attr">&quot;contributor&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO Consortium&quot;</span><span class="punctuation">,</span><span class="attr">&quot;date_created&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2017/09/01&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line"> <span class="attr">&quot;images&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">           <span class="punctuation">&#123;</span><span class="attr">&quot;license&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span><span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO_val2014_000000391895.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;coco_url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://images.cocodataset.org/val2014/COCO_val2014_000000391895.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;height&quot;</span><span class="punctuation">:</span> <span class="number">360</span><span class="punctuation">,</span><span class="attr">&quot;width&quot;</span><span class="punctuation">:</span> <span class="number">640</span><span class="punctuation">,</span><span class="attr">&quot;date_captured&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2013-11-14 11:18:45&quot;</span><span class="punctuation">,</span><span class="attr">&quot;flickr_url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">391895</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span><span class="attr">&quot;license&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span><span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO_val2014_000000522418.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;coco_url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://images.cocodataset.org/val2014/COCO_val2014_000000522418.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;height&quot;</span><span class="punctuation">:</span> <span class="number">480</span><span class="punctuation">,</span><span class="attr">&quot;width&quot;</span><span class="punctuation">:</span> <span class="number">640</span><span class="punctuation">,</span><span class="attr">&quot;date_captured&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2013-11-14 11:38:44&quot;</span><span class="punctuation">,</span><span class="attr">&quot;flickr_url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://farm1.staticflickr.com/1/127244861_ab0c0381e7_z.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">522418</span><span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;licenses&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://creativecommons.org/licenses/by-nc-sa/2.0/&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attribution-NonCommercial-ShareAlike License&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span>    </span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://creativecommons.org/licenses/by-nc/2.0/&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attribution-NonCommercial License&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://creativecommons.org/licenses/by-nc-nd/2.0/&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attribution-NonCommercial-NoDerivs License&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://creativecommons.org/licenses/by/2.0/&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attribution License&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://creativecommons.org/licenses/by-sa/2.0/&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attribution-ShareAlike License&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://creativecommons.org/licenses/by-nd/2.0/&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Attribution-NoDerivs License&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://flickr.com/commons/usage/&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;No known copyright restrictions&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">             <span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://www.usa.gov/copyright.shtml&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;United States Government Work&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">&quot;annotations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">203564</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">37</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A bicycle replica with a clock as the front wheel.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">179765</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">38</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A black Honda motorcycle parked in front of a garage.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">322141</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">49</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A room with blue walls and a white sink and door.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">16977</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">89</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A car that seems to be parked illegally behind a legally parked car&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">106140</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">98</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A large passenger airplane flying through the air.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">106140</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">101</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;There is a GOL plane taking off in a partly cloudy sky.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">322141</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">109</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Blue and white color scheme in a small bathroom.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">322141</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">121</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;This is a blue and white bathroom with a wall sink and a lifesaver on the wall.&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">               <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<br>
<p>coco数据集的简化：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;info&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO 2014 Dataset&quot;</span><span class="punctuation">,</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://cocodataset.org&quot;</span><span class="punctuation">,</span><span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1.0&quot;</span><span class="punctuation">,</span><span class="attr">&quot;year&quot;</span><span class="punctuation">:</span> <span class="number">2014</span><span class="punctuation">,</span><span class="attr">&quot;contributor&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO Consortium&quot;</span><span class="punctuation">,</span><span class="attr">&quot;date_created&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2017/09/01&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line"> <span class="attr">&quot;images&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">           <span class="punctuation">&#123;</span><span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO_val2014_000000391895.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">391895</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span><span class="attr">&quot;file_name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;COCO_val2014_000000522418.jpg&quot;</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">522418</span><span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;annotations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">203564</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">37</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A bicycle replica with a clock as the front wheel.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span><span class="attr">&quot;image_id&quot;</span><span class="punctuation">:</span> <span class="number">179765</span><span class="punctuation">,</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">38</span><span class="punctuation">,</span><span class="attr">&quot;caption&quot;</span><span class="punctuation">:</span> <span class="string">&quot;A black Honda motorcycle parked in front of a garage.&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">                </span><br><span class="line">               <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>我们的deepfashion-mutimodal数据集的json文件：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"> <span class="attr">&quot;WOMEN-Jackets_Coats-id_00005611-01_4_full.jpg&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The upper clothing has long sleeves, cotton fabric and solid color patterns. The neckline of it is v-shape. The lower clothing is of long length. The fabric is denim and it has solid color patterns. This lady also wears an outer clothing, with cotton fabric and complicated patterns. This female is wearing a ring on her finger. This female has neckwear.&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;WOMEN-Tees_Tanks-id_00005033-03_4_full.jpg&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Her tank shirt has no sleeves, chiffon fabric and graphic patterns. It has a round neckline. The person wears a long pants. The pants are with denim fabric and solid color patterns. The lady wears a ring.&quot;</span><span class="punctuation">,</span></span><br><span class="line"> <span class="attr">&quot;WOMEN-Rompers_Jumpsuits-id_00000245-01_1_front.jpg&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Her tank top has no sleeves, cotton fabric and solid color patterns. It has a v-shape neckline. This woman wears a long trousers. The trousers are with cotton fabric and solid color patterns. There is a ring on her finger. The lady wears a belt. There is an accessory on her wrist.&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<hr />
<p>(5). 训练图像描述</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python captioning_DIY.py</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>选取一个图像进行测试</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python sample.py --image_path &lt;任意图像路径&gt;</span><br><span class="line">python sample.py --image_path ./data/train2014/COCO_train2014_000000581921.jpg</span><br></pre></td></tr></table></figure>
<br>
<h2 id="2resizepy"><a class="markdownIt-Anchor" href="#2resizepy"></a> 2.<code>resize.py</code></h2>
<p><strong>resize_image</strong>:将图像调整为给定的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resize_image</span>(<span class="params">image, size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Resize an image to the given size.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> image.resize(size, Image.ANTIALIAS)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>resize_images</strong>:调整’image_dir’中的图像大小并保存到’output_dir’中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resize_images</span>(<span class="params">image_dir, output_dir, size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Resize the images in &#x27;image_dir&#x27; and save into &#x27;output_dir&#x27;.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(output_dir):</span><br><span class="line">        os.makedirs(output_dir)</span><br><span class="line"></span><br><span class="line">    images = os.listdir(image_dir)</span><br><span class="line">    num_images = <span class="built_in">len</span>(images)</span><br><span class="line">    <span class="keyword">for</span> i, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(image_dir, image), <span class="string">&#x27;r+b&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">with</span> Image.<span class="built_in">open</span>(f) <span class="keyword">as</span> img:</span><br><span class="line">                img = resize_image(img, size)</span><br><span class="line">                img.save(os.path.join(output_dir, image), img.<span class="built_in">format</span>)</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;[&#123;&#125;/&#123;&#125;] Resized the images and saved into &#x27;&#123;&#125;&#x27;.&quot;</span></span><br><span class="line">                   .<span class="built_in">format</span>(i+<span class="number">1</span>, num_images, output_dir))</span><br></pre></td></tr></table></figure>
<br>
<p><strong>main</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    image_dir = args.image_dir</span><br><span class="line">    output_dir = args.output_dir</span><br><span class="line">    image_size = [args.image_size, args.image_size]</span><br><span class="line">    resize_images(image_dir, output_dir, image_size)</span><br></pre></td></tr></table></figure>
<br>
<h2 id="3build_vocabpy"><a class="markdownIt-Anchor" href="#3build_vocabpy"></a> 3.<code>build_vocab.py</code></h2>
<p><strong>Vocabulary</strong>: 简单词汇包装器</p>
<ul>
<li>向词汇表中添加新单词</li>
<li>当调用一个 Vocabulary 实例并传入一个单词时，该方法会返回该单词对应的索引</li>
<li>词汇表的大小</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocabulary</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;简单词汇包装器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.word2idx = &#123;&#125;<span class="comment">#单词到索引的映射</span></span><br><span class="line">        self.idx2word = &#123;&#125;<span class="comment">#索引到单词的映射</span></span><br><span class="line">        self.idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_word</span>(<span class="params">self, word</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;向词汇表中添加新单词&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            self.word2idx[word] = self.idx</span><br><span class="line">            self.idx2word[self.idx] = word</span><br><span class="line">            self.idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, word</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;当调用一个 Vocabulary 实例并传入一个单词时，该方法会返回该单词对应的索引&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word <span class="keyword">in</span> self.word2idx:</span><br><span class="line">            <span class="keyword">return</span> self.word2idx[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> self.word2idx[word]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;词汇表的大小&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.word2idx)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>build_vocab</strong>:构建一个词汇表（Vocabulary）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_vocab</span>(<span class="params">json, threshold</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建一个词汇表（Vocabulary）&quot;&quot;&quot;</span></span><br><span class="line">    coco = COCO(json)<span class="comment">#使用 COCO 工具读取 JSON 文件，这通常包含了图像的注释信息，如图像的字幕。</span></span><br><span class="line">    counter = Counter()<span class="comment">#使用 Counter 对象来跟踪每个单词出现的频率</span></span><br><span class="line">    ids = coco.anns.keys()</span><br><span class="line">    <span class="keyword">for</span> i, <span class="built_in">id</span> <span class="keyword">in</span> <span class="built_in">enumerate</span>(ids):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        遍历 JSON 文件中的每个注释（字幕）</span></span><br><span class="line"><span class="string">        使用 nltk.tokenize.word_tokenize 对每个字幕进行分词，将字幕分解成单词列表</span></span><br><span class="line"><span class="string">        使用 counter.update(tokens) 更新这些单词的频率计数</span></span><br><span class="line"><span class="string">        每处理1000个字幕，打印一条进度信息</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        caption = <span class="built_in">str</span>(coco.anns[<span class="built_in">id</span>][<span class="string">&#x27;caption&#x27;</span>])</span><br><span class="line">        tokens = nltk.tokenize.word_tokenize(caption.lower())</span><br><span class="line">        counter.update(tokens)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[&#123;&#125;/&#123;&#125;] Tokenized the captions.&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(ids)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果单词频率小于“threshold”，则该单词被丢弃</span></span><br><span class="line">    words = [word <span class="keyword">for</span> word, cnt <span class="keyword">in</span> counter.items() <span class="keyword">if</span> cnt &gt;= threshold]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#创建一个 Vocabulary 实例，并添加一些特殊标记（如 &#x27;&lt;pad&gt;&#x27;, &#x27;&lt;start&gt;&#x27;, &#x27;&lt;end&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;）</span></span><br><span class="line">    vocab = Vocabulary()</span><br><span class="line">    vocab.add_word(<span class="string">&#x27;&lt;pad&gt;&#x27;</span>)</span><br><span class="line">    vocab.add_word(<span class="string">&#x27;&lt;start&gt;&#x27;</span>)</span><br><span class="line">    vocab.add_word(<span class="string">&#x27;&lt;end&gt;&#x27;</span>)</span><br><span class="line">    vocab.add_word(<span class="string">&#x27;&lt;unk&gt;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将筛选后的词汇添加到 Vocabulary 实例中</span></span><br><span class="line">    <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(words):</span><br><span class="line">        vocab.add_word(word)</span><br><span class="line">    <span class="keyword">return</span> vocab</span><br></pre></td></tr></table></figure>
<br>
<p><strong>main</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    vocab = build_vocab(json=args.caption_path, threshold=args.threshold)<span class="comment">#建立词汇表</span></span><br><span class="line">    vocab_path = args.vocab_path<span class="comment">#存储词汇表的路径</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(vocab_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        使用 pickle 模块的 dump 方法将 vocab 对象序列化并写入之前打开的文件 f 中</span></span><br><span class="line"><span class="string">        这样可以将词汇表对象持久化保存到文件系统中</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        pickle.dump(vocab, f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Total vocabulary size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(vocab)))<span class="comment">#打印词汇表大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Saved the vocabulary wrapper to &#x27;&#123;&#125;&#x27;&quot;</span>.<span class="built_in">format</span>(vocab_path))<span class="comment">#打印词汇表路径</span></span><br></pre></td></tr></table></figure>
<br>
<h2 id="4dataloaderpy"><a class="markdownIt-Anchor" href="#4dataloaderpy"></a> 4.<code>dataloader.py</code></h2>
<p><strong>CocoDataset</strong>: COCO自定义数据集继承torch.utils.data.DataLoader</p>
<ul>
<li>初始化</li>
<li>根据提供的索引 index 获取数据集中的一个数据对(图像和标题)</li>
<li>获取注释ID序列的长度</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CocoDataset</span>(data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;COCO自定义数据集继承torch.utils.data.DataLoader。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root, json, vocab, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;设置图像、标题和词汇包装的路径</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            root:  存放图像文件的目录路径</span></span><br><span class="line"><span class="string">            json: 包含 COCO 数据集注释的 JSON 文件的路径</span></span><br><span class="line"><span class="string">            vocab:  一个词汇表对象，用于处理文本数据（如标题）</span></span><br><span class="line"><span class="string">            transform: 可选参数，用于进行图像转换处理</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.coco = COCO(json)</span><br><span class="line">        self.ids = <span class="built_in">list</span>(self.coco.anns.keys())</span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据提供的索引 index 获取数据集中的一个数据对(图像和标题)&quot;&quot;&quot;</span></span><br><span class="line">        coco = self.coco</span><br><span class="line">        vocab = self.vocab</span><br><span class="line">        ann_id = self.ids[index]</span><br><span class="line">        caption = coco.anns[ann_id][<span class="string">&#x27;caption&#x27;</span>]<span class="comment">#使用先前获取的注释 ID (ann_id) 从数据集的注释中提取出对应的标题（caption）</span></span><br><span class="line">        img_id = coco.anns[ann_id][<span class="string">&#x27;image_id&#x27;</span>]<span class="comment">#从相同的注释条目中获取与标题关联的图像 ID</span></span><br><span class="line">        path = coco.loadImgs(img_id)[<span class="number">0</span>][<span class="string">&#x27;file_name&#x27;</span>]<span class="comment">#使用 loadImgs 方法根据图像 ID 加载图像，并获取图像文件的名称</span></span><br><span class="line"></span><br><span class="line">        image = Image.<span class="built_in">open</span>(os.path.join(self.root, path)).convert(<span class="string">&#x27;RGB&#x27;</span>)<span class="comment">#使用 PIL 库打开图像文件</span></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将标题(字符串)转换为字id.</span></span><br><span class="line">        tokens = nltk.tokenize.word_tokenize(<span class="built_in">str</span>(caption).lower())<span class="comment">#使用 nltk 库对标题文本进行分词处理，并将所有文本转换为小写。</span></span><br><span class="line">        caption = []</span><br><span class="line">        caption.append(vocab(<span class="string">&#x27;&lt;start&gt;&#x27;</span>))</span><br><span class="line">        caption.extend([vocab(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens])</span><br><span class="line">        caption.append(vocab(<span class="string">&#x27;&lt;end&gt;&#x27;</span>))</span><br><span class="line">        target = torch.Tensor(caption)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> image, target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.ids)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>collate_fn</strong>：自定义的批处理函数，用于处理数据集中的不同长度的标题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从元组列表中创建小批量张量(图片，标题)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    处理不同长度的标题，将它们转换为一个统一的批处理格式</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data: list of tuple (image, caption).</span></span><br><span class="line"><span class="string">            - image: torch tensor of shape (3, 256, 256).</span></span><br><span class="line"><span class="string">            - caption: torch tensor of shape (?); variable length.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        images: torch tensor of shape (batch_size, 3, 256, 256).</span></span><br><span class="line"><span class="string">        targets: torch tensor of shape (batch_size, padded_length).</span></span><br><span class="line"><span class="string">        lengths: list; valid length for each padded caption.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 按标题长度(降序)对数据列表排序</span></span><br><span class="line">    data.sort(key=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x[<span class="number">1</span>]), reverse=<span class="literal">True</span>)</span><br><span class="line">    images, captions = <span class="built_in">zip</span>(*data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 合并图像(从3D张量元组到4D张量元组)。</span></span><br><span class="line">    images = torch.stack(images, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 合并标题(从1D张量元组到2D张量元组)。</span></span><br><span class="line">    lengths = [<span class="built_in">len</span>(cap) <span class="keyword">for</span> cap <span class="keyword">in</span> captions]</span><br><span class="line">    targets = torch.zeros(<span class="built_in">len</span>(captions), <span class="built_in">max</span>(lengths)).long()</span><br><span class="line">    <span class="keyword">for</span> i, cap <span class="keyword">in</span> <span class="built_in">enumerate</span>(captions):</span><br><span class="line">        end = lengths[i]</span><br><span class="line">        targets[i, :end] = cap[:end]</span><br><span class="line">    <span class="keyword">return</span> images, targets, lengths</span><br></pre></td></tr></table></figure>
<br>
<p><strong>get_loader</strong>: 创建并返回一个用于加载 COCO 数据集的 torch.utils.data.DataLoader 实例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_loader</span>(<span class="params">root, json, vocab, transform, batch_size, shuffle, num_workers</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;创建并返回一个用于加载 COCO 数据集的 torch.utils.data.DataLoader 实例&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># COCO caption dataset</span></span><br><span class="line">    coco = CocoDataset(root=root,</span><br><span class="line">                       json=json,</span><br><span class="line">                       vocab=vocab,</span><br><span class="line">                       transform=transform)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Data loader for COCO dataset</span></span><br><span class="line">    <span class="comment"># This will return (images, captions, lengths) for each iteration.</span></span><br><span class="line">    <span class="comment"># images: a tensor of shape (batch_size, 3, 224, 224).</span></span><br><span class="line">    <span class="comment"># captions: a tensor of shape (batch_size, padded_length).</span></span><br><span class="line">    <span class="comment"># lengths: a list indicating valid length for each caption. length is (batch_size).</span></span><br><span class="line">    data_loader = torch.utils.data.DataLoader(dataset=coco,</span><br><span class="line">                                              batch_size=batch_size,</span><br><span class="line">                                              shuffle=shuffle,</span><br><span class="line">                                              num_workers=num_workers,</span><br><span class="line">                                              collate_fn=collate_fn)</span><br><span class="line">    <span class="keyword">return</span> data_loader</span><br></pre></td></tr></table></figure>
<br>
<h2 id="5git_vitpy"><a class="markdownIt-Anchor" href="#5git_vitpy"></a> 5.<code>git_vit.py</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.vit = ViTModel.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line">        self.hid_dim=<span class="number">768</span></span><br><span class="line">        self.proj = nn.Linear(<span class="number">768</span>, <span class="number">768</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src</span>):</span><br><span class="line">        <span class="comment">#return = [batch size, patch len, hid dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.proj(self.vit(pixel_values=src).last_hidden_state)</span><br><span class="line">model = Encoder()</span><br></pre></td></tr></table></figure>
<br>
<h2 id="6captioning_diypy"><a class="markdownIt-Anchor" href="#6captioning_diypy"></a> 6.<code>captioning_DIY.py</code></h2>
<p><strong>PositionalEncoding</strong>: Transformer 网络添加位置编码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer 网络添加位置编码&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)<span class="comment">#模型的维度和位置编码的最大长度</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)<span class="comment">#从 0 到 max_len 的连续值，表示位置索引</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))<span class="comment">#缩放项</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)<span class="comment">#为位置矩阵的偶数部分赋值正弦函数值</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)<span class="comment">#为位置矩阵的奇数部分赋值余弦函数值。</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)<span class="comment">#调整位置编码矩阵的形状以便后续操作</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#将输入 x（通常是序列的嵌入表示）与位置编码相加，以便每个位置的嵌入都有唯一的表示</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>Encoder</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.vit = ViTModel.from_pretrained(<span class="string">&#x27;google/vit-base-patch16-224&#x27;</span>)</span><br><span class="line">        self.hid_dim=<span class="number">768</span></span><br><span class="line">        self.proj = nn.Linear(<span class="number">768</span>, <span class="number">768</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src</span>):</span><br><span class="line">        <span class="comment">#return = [batch size, patch len, hid dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.proj(self.vit(pixel_values=src).last_hidden_state)</span><br><span class="line">model = Encoder()</span><br></pre></td></tr></table></figure>
<br>
<p><strong>MultiHeadAttentionLayer</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttentionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力机制,允许模型在计算注意力时同时关注来自不同位置的不同表示子空间&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hid_dim, n_heads, dropout, device</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span> hid_dim % n_heads == <span class="number">0</span><span class="comment">#确保隐藏维度可以均匀地分配到每个头</span></span><br><span class="line">        </span><br><span class="line">        self.hid_dim = hid_dim</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.head_dim = hid_dim // n_heads<span class="comment">#每个头的维度</span></span><br><span class="line">        </span><br><span class="line">        self.fc_q = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc_k = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc_v = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        </span><br><span class="line">        self.fc_o = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        </span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size = query.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: Linear transformations for query, key, and value</span></span><br><span class="line">        Q = self.fc_q(query)  <span class="comment"># [batch size, query len, hid dim]</span></span><br><span class="line">        K = self.fc_k(key)  <span class="comment"># [batch size, key len, hid dim]</span></span><br><span class="line">        V = self.fc_v(value)  <span class="comment"># [batch size, value len, hid dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: Split the embeddings into `self.n_heads` heads</span></span><br><span class="line">        Q = Q.view(batch_size, -<span class="number">1</span>, self.n_heads, self.head_dim).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        K = K.view(batch_size, -<span class="number">1</span>, self.n_heads, self.head_dim).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        V = V.view(batch_size, -<span class="number">1</span>, self.n_heads, self.head_dim).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3: Compute the energy (attention weights)</span></span><br><span class="line">        energy = torch.matmul(Q, K.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)) / self.scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4: Apply mask (if any)</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            energy = energy.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 5: Normalize attention weights</span></span><br><span class="line">        attention = torch.softmax(energy, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 6: Apply attention to the value vector</span></span><br><span class="line">        x = torch.matmul(self.dropout(attention), V)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 7: Concatenate heads and apply final linear layer</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>, self.hid_dim)</span><br><span class="line">        x = self.fc_o(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, attention</span><br></pre></td></tr></table></figure>
<br>
<p><strong>PositionwiseFeedforwardLayer</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedforwardLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对每个位置的特征独立地应用相同的全连接层变换&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hid_dim, pf_dim, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.fc_1 = nn.Linear(hid_dim, pf_dim)</span><br><span class="line">        self.fc_2 = nn.Linear(pf_dim, hid_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        </span><br><span class="line">        x = self.dropout(torch.relu(self.fc_1(x)))</span><br><span class="line">        x = self.fc_2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<br>
<p><strong>Decoder</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = <span class="number">100</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">        self.tok_embedding = nn.Embedding(output_dim, hid_dim)<span class="comment">#创建一个嵌入层，用于将目标序列的令牌转换为固定维度的向量</span></span><br><span class="line">        self.pos_embedding = nn.Embedding(max_length, hid_dim)<span class="comment">#创建一个位置嵌入层，用于给目标序列的每个位置编码一个固定维度的向量</span></span><br><span class="line">        <span class="comment"># self.pos_encoding = PositionalEncoding(hid_dim, max_length)</span></span><br><span class="line"></span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">        self.fc_out = nn.Linear(hid_dim, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, trg, enc_src, trg_mask, src_mask</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment">#trg = [batch size, trg len] #目标序列</span></span><br><span class="line">        <span class="comment">#enc_src = [batch size, src len, hid dim] #编码去输出</span></span><br><span class="line">        <span class="comment">#trg_mask = [batch size, 1, trg len, trg len] #目标序列掩码</span></span><br><span class="line">        <span class="comment">#src_mask = [batch size, 1, 1, src len] #源序列掩码</span></span><br><span class="line"></span><br><span class="line">        batch_size = trg.shape[<span class="number">0</span>]</span><br><span class="line">        trg_len = trg.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, trg_len).unsqueeze(<span class="number">0</span>).repeat(batch_size, <span class="number">1</span>).to(self.device)<span class="comment">#生成位置序列，并将其复制到每个样本</span></span><br><span class="line">        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))<span class="comment">#对目标序列的令牌应用嵌入，缩放，加上位置嵌入，然后应用 dropout</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#每个解码器层进行迭代</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            trg, attention = layer(trg, enc_src, trg_mask, src_mask)</span><br><span class="line">        output = self.fc_out(trg)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#return output, attention</span></span><br><span class="line">        <span class="keyword">return</span> output,attention</span><br></pre></td></tr></table></figure>
<br>
<p><strong>DecoderLayer</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 hid_dim, </span></span><br><span class="line"><span class="params">                 n_heads, </span></span><br><span class="line"><span class="params">                 pf_dim, </span></span><br><span class="line"><span class="params">                 dropout, </span></span><br><span class="line"><span class="params">                 device</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)</span><br><span class="line">        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)</span><br><span class="line">        self.ff_layer_norm = nn.LayerNorm(hid_dim)</span><br><span class="line">        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)</span><br><span class="line">        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)</span><br><span class="line">        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, trg, enc_src, trg_mask, src_mask</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#trg = [batch size, trg len, hid dim]</span></span><br><span class="line">        <span class="comment">#enc_src = [batch size, src len, hid dim]</span></span><br><span class="line">        <span class="comment">#trg_mask = [batch size, 1, trg len, trg len]</span></span><br><span class="line">        <span class="comment">#src_mask = [batch size, 1, 1, src len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># self attention</span></span><br><span class="line">        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)</span><br><span class="line">        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))  <span class="comment"># residual connection and layer norm</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder attention</span></span><br><span class="line">        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)</span><br><span class="line">        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))  <span class="comment"># residual connection and layer norm</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># positionwise feedforward</span></span><br><span class="line">        _trg = self.positionwise_feedforward(trg)</span><br><span class="line">        trg = self.ff_layer_norm(trg + self.dropout(_trg))  <span class="comment"># residual connection and layer norm</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trg, attention</span><br></pre></td></tr></table></figure>
<br>
<p><strong>Img2Seq</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Img2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, trg_pad_idx, device</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.trg_pad_idx = trg_pad_idx<span class="comment">#目标填充索引</span></span><br><span class="line">        self.device = device</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_src_mask</span>(<span class="params">self, src</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        创建源数据（src）的掩码</span></span><br><span class="line"><span class="string">        创建一个与源数据同样形状的全1张量，然后添加两个维度，并将其传输到与源数据相同的设备上</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        src_mask = torch.ones(src.size(<span class="number">0</span>), src.size(<span class="number">1</span>)).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>).to(src.device)</span><br><span class="line">        <span class="keyword">return</span> src_mask</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_trg_mask</span>(<span class="params">self, trg</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        创建目标数据（trg）的掩码</span></span><br><span class="line"><span class="string">        首先创建一个用于标识目标序列中非填充元素的掩码，然后创建一个下三角矩阵，用于确保解码器只能看到之前的元素（这对于序列生成任务很重要）。</span></span><br><span class="line"><span class="string">        最后，它返回这两个掩码的逻辑与结果。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        trg_len = trg.shape[<span class="number">1</span>]</span><br><span class="line">        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).<span class="built_in">bool</span>()</span><br><span class="line">        trg_mask = trg_pad_mask &amp; trg_sub_mask</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> trg_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, trg</span>):</span><br><span class="line">                </span><br><span class="line">        trg_mask = self.make_trg_mask(trg)</span><br><span class="line">        enc_src = self.encoder(src)</span><br><span class="line">        src_mask = self.make_src_mask(enc_src)    </span><br><span class="line">        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, attention</span><br></pre></td></tr></table></figure>
<br>
<p><strong>train</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, iterator, optimizer, criterion, clip, log_step=<span class="number">10</span></span>):</span><br><span class="line">    </span><br><span class="line">    model.train()</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>) </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    p_bar = tqdm(<span class="built_in">enumerate</span>(iterator), total=<span class="built_in">len</span>(iterator))<span class="comment">#创建一个进度条</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> p_bar:</span><br><span class="line">        </span><br><span class="line">        src, tgt, l = batch</span><br><span class="line">        src = src.to(device)</span><br><span class="line">        trg = tgt.to(device)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()<span class="comment">#清除梯度</span></span><br><span class="line">        </span><br><span class="line">        output, _ = model(src, trg[:,:-<span class="number">1</span>])<span class="comment">#通过模型前向传播计算输出。目标数据trg去掉最后一个元素，因为在序列生成中，预测的是下一个单词</span></span><br><span class="line"></span><br><span class="line">        output_dim = output.shape[-<span class="number">1</span>]</span><br><span class="line">            </span><br><span class="line">        output = output.contiguous().view(-<span class="number">1</span>, output_dim)</span><br><span class="line">        trg = trg[:,<span class="number">1</span>:].contiguous().view(-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        loss = criterion(output, trg)</span><br><span class="line">        </span><br><span class="line">        loss.backward()<span class="comment">#反向传播</span></span><br><span class="line">        </span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)<span class="comment">#裁剪梯度 防止爆炸</span></span><br><span class="line">        </span><br><span class="line">        optimizer.step()<span class="comment">#更新模型权重</span></span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % log_step == <span class="number">0</span>:</span><br><span class="line">            p_bar.set_description(<span class="string">f&#x27;STEP <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> | Loss: <span class="subst">&#123;(epoch_loss/(i+<span class="number">1</span>)):<span class="number">.3</span>f&#125;</span> | Train PPL: <span class="subst">&#123;math.exp(epoch_loss/(i+<span class="number">1</span>)):<span class="number">7.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / <span class="built_in">len</span>(iterator)</span><br></pre></td></tr></table></figure>
<br>
<h2 id="技术路线"><a class="markdownIt-Anchor" href="#技术路线"></a> 技术路线</h2>
<!-- 在这个任务中，我们使用预训练的google/vit-base-patch16-224 预训练的ViT作为编码器，以及一个transformer解码器。 -->
<p><strong>数据预处理</strong>：</p>
<ul>
<li><strong>文本描述</strong>：创建一个 Vocabulary 类，用于对每个描述进行分词处理，进而将这些描述拆解为单词的列表形式。同时，该类负责统计各单词出现的频次，并依据设定的频率阈值筛选单词——频率低于该阈值的单词将被排除。此外，每个 Vocabulary 实例中都会包含几个特殊的标记符，例如 &lt; pad &gt;、&lt; start &gt;、&lt; end &gt;和 &lt; unk &gt;，以支持更加灵活和有效的文本处理。</li>
<li><strong>图片</strong>：将图像裁剪到 224x224 像素，随机进行水平翻转，从 PIL 格式转化为 PyTorch 张量，最后对图像的每个颜色通道进行标准化处理。</li>
</ul>
<p><strong>图像编码器</strong>：预训练的vit-base-patch16-224</p>
<p><strong>文本解码器</strong>：将目标序列通过嵌入层进行处理，然后经过6个DecoderLayer(attention + feedforward)，最后通过线性层，生成每个可能单词的预测分数。</p>
<br>
<h2 id="模块分布"><a class="markdownIt-Anchor" href="#模块分布"></a> 模块分布</h2>
<ol>
<li>
<p><strong>位置编码 (<code>PositionalEncoding</code>)</strong>: 这个类用于生成位置编码，这些编码随后会添加到输入序列的嵌入中。它使用正弦和余弦函数的变体来为每个位置生成一个唯一的编码。</p>
</li>
<li>
<p><strong>编码器 (<code>Encoder</code>)</strong>: 编码器使用了一个预训练的Vision Transformer（ViT）来处理图像输入。它将图像转换成一个序列化的表示形式，这是用于后续的注意力机制的。</p>
</li>
<li>
<p><strong>多头注意力机制 (<code>MultiHeadAttentionLayer</code>)</strong>: 这个类实现了多头注意力机制，其中模型可以并行地关注输入的不同部分。这是Transformer架构的核心特性之一。</p>
</li>
<li>
<p><strong>位置特定的前馈层 (<code>PositionwiseFeedforwardLayer</code>)</strong>: 这个层对每个位置的特征独立地应用一个全连接网络，通常包含一个ReLU激活函数。</p>
</li>
<li>
<p><strong>解码器 (<code>Decoder</code>)</strong>: 解码器负责生成输出序列。它使用多头注意力机制和位置特定的前馈层，并利用编码器的输出来帮助生成序列。</p>
</li>
</ol>
<hr />
<h2 id="build_vocabpy"><a class="markdownIt-Anchor" href="#build_vocabpy"></a> build_vocab.py</h2>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/12/04/NNDL%E8%AF%BE%E8%AE%BE-ViT%E6%A8%A1%E5%9E%8B/" data-id="cls1iheag008d98699aacf8pz" data-title="NNDL课设:ViT模型"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NNDL/" rel="tag">NNDL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ViT/" rel="tag">ViT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" rel="tag">专业知识</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="Django外部网络访问"
                         class="lazyload">
                
                <a href="/2023/12/04/Django%E5%A4%96%E9%83%A8%E7%BD%91%E7%BB%9C%E8%AE%BF%E9%97%AE/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        Django外部网络访问
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="nvidia-smi"
                         class="lazyload">
                
                <a href="/2023/12/03/nvidia-smi/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        nvidia-smi
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="Jerome" class="lazyload">
            <div class="sidebar-author-name">Jerome</div>
            <div class="sidebar-description">Indeed, I am quite the oddity.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">367</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">34</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">396</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/74X2u8JMVooG2QbjRxXwR8?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Competition/">Competition</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Future/">Future</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/IELTS/">IELTS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/Multimudal/">Multimudal</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS224N/">CS224N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS231N/">CS231N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/Missing-Semester-of-CS/">Missing Semester of CS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Computer-Vision/">Computer Vision</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%96%87%E5%8C%96%E8%AE%A1%E7%AE%97/">文化计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/">智能信息网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/">语音信息处理</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%91%A8%E8%AE%B0/">周记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 12.78px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/17/" style="font-size: 10px;">17</a> <a href="/tags/2/" style="font-size: 13.89px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 12.22px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/3-11/" style="font-size: 10px;">3.11</a> <a href="/tags/3-4/" style="font-size: 10px;">3.4</a> <a href="/tags/4/" style="font-size: 11.11px;">4</a> <a href="/tags/5/" style="font-size: 10.56px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 11.11px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 17.78px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.11px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 13.89px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 11.67px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10.56px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 11.67px;">CNN</a> <a href="/tags/CS224N/" style="font-size: 10.56px;">CS224N</a> <a href="/tags/CS231N/" style="font-size: 10px;">CS231N</a> <a href="/tags/CV/" style="font-size: 12.78px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 12.78px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/ComPetition/" style="font-size: 10px;">ComPetition</a> <a href="/tags/Competition/" style="font-size: 12.78px;">Competition</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 16.11px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.56px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Filter/" style="font-size: 10px;">Filter</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/Future/" style="font-size: 13.33px;">Future</a> <a href="/tags/GB/" style="font-size: 10px;">GB</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GiB/" style="font-size: 10px;">GiB</a> <a href="/tags/Git/" style="font-size: 10.56px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 17.22px;">GoAbroad</a> <a href="/tags/Graduate/" style="font-size: 10px;">Graduate</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 12.22px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/LayerNorm/" style="font-size: 10px;">LayerNorm</a> <a href="/tags/Lec01/" style="font-size: 11.11px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.56px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.67px;">Linux</a> <a href="/tags/Listening/" style="font-size: 10px;">Listening</a> <a href="/tags/M2/" style="font-size: 10.56px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.22px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 13.89px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.56px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 11.11px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NECCS/" style="font-size: 10px;">NECCS</a> <a href="/tags/NLP/" style="font-size: 12.22px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 16.67px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/OS/" style="font-size: 13.89px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.56px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Reading/" style="font-size: 10px;">Reading</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10.56px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.11px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.22px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Script/" style="font-size: 10px;">Script</a> <a href="/tags/Shell/" style="font-size: 10.56px;">Shell</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-Neural-Network-for-Ultra-low-latency-and-High-accurate-Object-Detection/" style="font-size: 10px;">Spiking Neural Network for Ultra-low-latency and High-accurate Object Detection</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.56px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.56px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/Undergraduate/" style="font-size: 10px;">Undergraduate</a> <a href="/tags/University/" style="font-size: 12.78px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 11.11px;">ViT</a> <a href="/tags/Vim/" style="font-size: 10px;">Vim</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.56px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/alexnet/" style="font-size: 10px;">alexnet</a> <a href="/tags/anygpt/" style="font-size: 10px;">anygpt</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 11.67px;">bert</a> <a href="/tags/blip2/" style="font-size: 10px;">blip2</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/bug/" style="font-size: 16.67px;">bug</a> <a href="/tags/cat/" style="font-size: 10px;">cat</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.11px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.56px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.56px;">chapter05</a> <a href="/tags/chapter6/" style="font-size: 10px;">chapter6</a> <a href="/tags/chapter7/" style="font-size: 10px;">chapter7</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/chmod/" style="font-size: 10px;">chmod</a> <a href="/tags/chrome/" style="font-size: 10px;">chrome</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/code/" style="font-size: 11.67px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/commit/" style="font-size: 10px;">commit</a> <a href="/tags/competition/" style="font-size: 10px;">competition</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/copilot/" style="font-size: 10.56px;">copilot</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10.56px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.33px;">d2l</a> <a href="/tags/database/" style="font-size: 13.89px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.56px;">deep neural network</a> <a href="/tags/delete/" style="font-size: 10px;">delete</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/dowhy/" style="font-size: 10.56px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10.56px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/embedding/" style="font-size: 10px;">embedding</a> <a href="/tags/explainer/" style="font-size: 10.56px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/github/" style="font-size: 12.22px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 11.11px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10.56px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/import/" style="font-size: 10px;">import</a> <a href="/tags/instructor/" style="font-size: 11.67px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 11.67px;">intern00</a> <a href="/tags/interns/" style="font-size: 10px;">interns</a> <a href="/tags/internship/" style="font-size: 18.89px;">internship</a> <a href="/tags/interview/" style="font-size: 10px;">interview</a> <a href="/tags/introduction/" style="font-size: 11.11px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/jmbook/" style="font-size: 10.56px;">jmbook</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab/" style="font-size: 10px;">lab</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.56px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 11.11px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lr/" style="font-size: 10px;">lr</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 12.22px;">mac</a> <a href="/tags/memory/" style="font-size: 11.67px;">memory</a> <a href="/tags/mentor/" style="font-size: 10.56px;">mentor</a> <a href="/tags/mid/" style="font-size: 10.56px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/multimudal/" style="font-size: 10px;">multimudal</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.56px;">neuromorphic computing</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/nndl/" style="font-size: 10.56px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 15px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.11px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.44px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 11.67px;">prml</a> <a href="/tags/profile/" style="font-size: 10px;">profile</a> <a href="/tags/project/" style="font-size: 12.78px;">project</a> <a href="/tags/pycharm/" style="font-size: 10px;">pycharm</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/pytorch/" style="font-size: 14.44px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10.56px;">reading</a> <a href="/tags/register/" style="font-size: 10px;">register</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 15px;">review</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.56px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/server/" style="font-size: 10px;">server</a> <a href="/tags/sgns/" style="font-size: 10px;">sgns</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/softmax/" style="font-size: 10px;">softmax</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/sora/" style="font-size: 10px;">sora</a> <a href="/tags/spike/" style="font-size: 10.56px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.56px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikebert/" style="font-size: 10px;">spikebert</a> <a href="/tags/spikingjelly/" style="font-size: 12.22px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.56px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.56px;">ssh</a> <a href="/tags/sta/" style="font-size: 10px;">sta</a> <a href="/tags/terminal/" style="font-size: 10px;">terminal</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.56px;">tips</a> <a href="/tags/tittle/" style="font-size: 10px;">tittle</a> <a href="/tags/tmux/" style="font-size: 10px;">tmux</a> <a href="/tags/tool/" style="font-size: 18.33px;">tool</a> <a href="/tags/transformer/" style="font-size: 12.78px;">transformer</a> <a href="/tags/transformers/" style="font-size: 10px;">transformers</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10.56px;">vscode</a> <a href="/tags/wakatime/" style="font-size: 10px;">wakatime</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/yeild/" style="font-size: 10px;">yeild</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">专业知识</a> <a href="/tags/%E4%B8%93%E7%A1%95/" style="font-size: 10px;">专硕</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E4%BB%A3%E7%90%86/" style="font-size: 10px;">代理</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">公选课</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.56px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%86%99%E4%BD%9C%E6%8A%80%E5%B7%A7/" style="font-size: 10px;">写作技巧</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" style="font-size: 10px;">博弈论</a> <a href="/tags/%E5%91%A8%E8%AE%B0/" style="font-size: 11.11px;">周记</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">图像描述生成</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">大作业</a> <a href="/tags/%E5%A4%A7%E5%88%9B/" style="font-size: 10px;">大创</a> <a href="/tags/%E5%A4%A7%E8%8B%B1%E8%B5%9B/" style="font-size: 10px;">大英赛</a> <a href="/tags/%E5%AD%A6%E7%A1%95/" style="font-size: 10px;">学硕</a> <a href="/tags/%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/" style="font-size: 10px;">实验报告</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.56px;">审稿意见</a> <a href="/tags/%E5%B0%8F%E4%BD%9C%E4%B8%9A/" style="font-size: 10.56px;">小作业</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%82%84%E6%82%84%E8%AF%9D/" style="font-size: 10px;">悄悄话</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%99%E8%82%B2%E8%AE%B8%E5%8F%AF/" style="font-size: 10px;">教育许可</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%96%87%E5%8C%96%E8%AE%A1%E7%AE%97/" style="font-size: 11.11px;">文化计算</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">显卡</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.56px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/" style="font-size: 11.11px;">智能信息网络</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 13.89px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.56px;">服务器</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期中复习</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">朴素贝叶斯</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 11.67px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.56px;">李宏毅</a> <a href="/tags/%E6%9D%8E%E6%B2%90/" style="font-size: 10px;">李沐</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">模型训练流程</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 12.78px;">毛概</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.56px;">特征学习</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 10px;">目标检测</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">第一次作业</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">第三章</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E7%BD%91%E6%98%93/" style="font-size: 10px;">网易</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3/" style="font-size: 10px;">脑机接口</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%8A%82%E8%83%BD%E5%87%8F%E6%8E%92/" style="font-size: 11.11px;">节能减排</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">评测指标</a> <a href="/tags/%E8%AF%AD%E4%B9%89%E7%A9%BA%E9%97%B4/" style="font-size: 10px;">语义空间</a> <a href="/tags/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/" style="font-size: 11.67px;">语音信息处理</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">课程</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.11px;">调研</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">软件概要设计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%87%8F%E5%8C%96/" style="font-size: 10px;">量化</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a> <a href="/tags/%E9%B8%BF%E9%9B%81%E6%9D%AF/" style="font-size: 10px;">鸿雁杯</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">四月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                Jerome
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    738.6k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    46:50
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="Jerome" class="lazyload">
            <div class="sidebar-author-name">Jerome</div>
            <div class="sidebar-description">Indeed, I am quite the oddity.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">367</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">34</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">396</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

