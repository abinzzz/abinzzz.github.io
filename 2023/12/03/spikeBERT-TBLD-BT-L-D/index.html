
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        spikeBERT:TBLD-&gt;BT(L)D |
        
        Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   1. class BertLayer(nn.Module):初始化函数：12345678910111213def __init__(self, config):        super().__init__()">
<meta property="og:type" content="article">
<meta property="og:title" content="spikeBERT:TBLD-&gt;BT(L)D">
<meta property="og:url" content="https://abinzzz.github.io/2023/12/03/spikeBERT-TBLD-BT-L-D/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   1. class BertLayer(nn.Module):初始化函数：12345678910111213def __init__(self, config):        super().__init__()">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-12-03T07:52:14.000Z">
<meta property="article:modified_time" content="2023-12-05T19:02:50.342Z">
<meta property="article:author" content="あまのひな">
<meta property="article:tag" content="internship">
<meta property="article:tag" content="spikeBERT">
<meta property="article:tag" content="改维度">
<meta name="twitter:card" content="summary">
    
        <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="spikeBERT:TBLD-&gt;BT(L)D" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>spikeBERT:TBLD-&gt;BT(L)D</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-spikeBERT-TBLD-BT-L-D" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/12/03/spikeBERT-TBLD-BT-L-D/" class="article-date-link">
        <time datetime="2023-12-03T07:52:14.000Z"
              itemprop="datePublished">2023-12-03</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/internship/">internship</a><a class="article-category-link" href="/categories/internship/spikeBERT/">spikeBERT</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>

<h2 id="1-class-BertLayer-nn-Module"><a href="#1-class-BertLayer-nn-Module" class="headerlink" title="1. class BertLayer(nn.Module):"></a>1. class BertLayer(nn.Module):</h2><p>初始化函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.chunk_size_feed_forward = config.chunk_size_feed_forward</span><br><span class="line">        self.seq_len_dim = <span class="number">1</span></span><br><span class="line">        self.attention = BertAttention(config)</span><br><span class="line">        self.is_decoder = config.is_decoder</span><br><span class="line">        self.add_cross_attention = config.add_cross_attention</span><br><span class="line">        <span class="keyword">if</span> self.add_cross_attention:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.is_decoder:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">f&quot;<span class="subst">&#123;self&#125;</span> should be used as a decoder model if cross attention is added&quot;</span>)</span><br><span class="line">            self.crossattention = BertAttention(config, position_embedding_type=<span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">        self.intermediate = BertIntermediate(config)</span><br><span class="line">        self.output = BertOutput(config)</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p>前向传播函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_hidden_states: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[<span class="type">Tuple</span>[torch.FloatTensor]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.Tensor]:</span><br><span class="line">        <span class="comment"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span></span><br><span class="line">        self_attn_past_key_value = past_key_value[:<span class="number">2</span>] <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self_attention_outputs = self.attention(</span><br><span class="line">            hidden_states,</span><br><span class="line">            attention_mask,</span><br><span class="line">            head_mask,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            past_key_value=self_attn_past_key_value,</span><br><span class="line">        )<span class="comment">#这个就只有一个输出，后面是无效的</span></span><br><span class="line">        attention_output = self_attention_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if decoder, the last output is tuple of self-attn cache</span></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            outputs = self_attention_outputs[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            present_key_value = self_attention_outputs[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = self_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add self attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">        cross_attn_present_key_value = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder <span class="keyword">and</span> encoder_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;crossattention&quot;</span>):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(</span><br><span class="line">                    <span class="string">f&quot;If `encoder_hidden_states` are passed, <span class="subst">&#123;self&#125;</span> has to be instantiated with cross-attention layers&quot;</span></span><br><span class="line">                    <span class="string">&quot; by setting `config.add_cross_attention=True`&quot;</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple</span></span><br><span class="line">            cross_attn_past_key_value = past_key_value[-<span class="number">2</span>:] <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            cross_attention_outputs = self.crossattention(</span><br><span class="line">                attention_output,</span><br><span class="line">                attention_mask,</span><br><span class="line">                head_mask,</span><br><span class="line">                encoder_hidden_states,</span><br><span class="line">                encoder_attention_mask,</span><br><span class="line">                cross_attn_past_key_value,</span><br><span class="line">                output_attentions,</span><br><span class="line">            )</span><br><span class="line">            attention_output = cross_attention_outputs[<span class="number">0</span>]</span><br><span class="line">            outputs = outputs + cross_attention_outputs[<span class="number">1</span>:-<span class="number">1</span>]  <span class="comment"># add cross attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># add cross-attn cache to positions 3,4 of present_key_value tuple</span></span><br><span class="line">            cross_attn_present_key_value = cross_attention_outputs[-<span class="number">1</span>]</span><br><span class="line">            present_key_value = present_key_value + cross_attn_present_key_value</span><br><span class="line"></span><br><span class="line">        layer_output = apply_chunking_to_forward(</span><br><span class="line">            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output</span><br><span class="line">        )<span class="comment">#这部分作用，好像就是扩大了矩阵，增加了过呢更多</span></span><br><span class="line">        outputs = (layer_output,) + outputs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if decoder, return the attn key/values as the last output</span></span><br><span class="line">        <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">            outputs = outputs + (present_key_value,)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># print(layer_output.shape)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p>前馈网络块：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">feed_forward_chunk</span>(<span class="params">self, attention_output</span>):<span class="comment">#是否有必要？？？？</span></span><br><span class="line">        intermediate_output = self.intermediate(attention_output)</span><br><span class="line">        layer_output = self.output(intermediate_output, attention_output)</span><br><span class="line">        <span class="keyword">return</span> layer_output</span><br></pre></td></tr></table></figure></p>
<p><strong>self_attn_past_key_value</strong>: None<br><strong>self_attention_output</strong>: 只有一个元素的tuple<br><strong>attention_output</strong>: tensor[4,32,128,1536]<br><strong>layer_out</strong>: </p>
<ul>
<li>—-Intermediate—-</li>
<li><strong>tensor[4,32,128,1536]</strong>-&gt;(Intermediate=SpikeLinear).PSN-&gt;<strong>tensor[4,32,128,1536]的0,1向量</strong> </li>
<li><strong>tensor[4,32,128,1536]的0,1向量</strong> -&gt;(Intermediate=SpikeLinear).nn.functional.linear + self.bias-&gt;(Intermediate=SpikeLinear).scale-&gt;<strong>tensor[4,32,128,3072]</strong></li>
<li>—-output—-</li>
<li><strong>tensor[4,32,128,3072]</strong>-&gt;output.SpikeLinear.PSN-&gt;<strong>tensor[4,32,128,3072]的0,1向量</strong></li>
<li><strong>tensor[4,32,128,3072]的0,1向量</strong>-&gt;&gt;output.SpikeLinear.nn.functional.linear + self.bias-&gt;&gt;output.SpikeLinear.scale-&gt;<strong>tensor[4,32,128,1536]</strong></li>
<li><strong>tensor[4,32,128,1536]</strong>-&gt;output.dropout-&gt;<strong>tensor[4,32,128,1536]</strong></li>
<li><strong>tensor[4,32,128,1536]</strong>-&gt;output.LayerNorm-&gt;<strong>tensor[4,32,128,1536]</strong>  </li>
</ul>
<p><strong>outputs</strong>: 只有一个layer_out的tuple</p>
<hr>
<p><br></p>
<p><br></p>
<h2 id="2-class-BertEncoder-nn-Module"><a href="#2-class-BertEncoder-nn-Module" class="headerlink" title="2. class BertEncoder(nn.Module):"></a>2. class BertEncoder(nn.Module):</h2><p>初始化函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.num_hidden_layers)])</span><br><span class="line">        self.gradient_checkpointing = <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p>前向传播函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_hidden_states: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        past_key_values: <span class="type">Optional</span>[<span class="type">Tuple</span>[<span class="type">Tuple</span>[torch.FloatTensor]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        use_cache: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        output_hidden_states: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        return_dict: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Union</span>[<span class="type">Tuple</span>[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:</span><br><span class="line">        all_hidden_states = () <span class="keyword">if</span> output_hidden_states <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        all_self_attentions = () <span class="keyword">if</span> output_attentions <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="comment"># all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if self.gradient_checkpointing and self.training:</span></span><br><span class="line">        <span class="comment">#     if use_cache:</span></span><br><span class="line">        <span class="comment">#         logger.warning_once(</span></span><br><span class="line">        <span class="comment">#             &quot;`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...&quot;</span></span><br><span class="line">        <span class="comment">#         )</span></span><br><span class="line">        <span class="comment">#         use_cache = False</span></span><br><span class="line"></span><br><span class="line">        next_decoder_cache = () <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">######################</span></span><br><span class="line">        hidden_states = hidden_states.repeat(<span class="built_in">tuple</span>([<span class="number">4</span>] + torch.ones(<span class="built_in">len</span>(hidden_states.size()), dtype=<span class="built_in">int</span>).tolist())) <span class="comment"># T B L D</span></span><br><span class="line">        <span class="comment"># hidden_states = hidden_states.transpose(0, 1) # B T L D</span></span><br><span class="line">        <span class="comment">######################</span></span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layer):</span><br><span class="line">            <span class="keyword">if</span> output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">            layer_head_mask = head_mask[i] <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            past_key_value = past_key_values[i] <span class="keyword">if</span> past_key_values <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># if self.gradient_checkpointing and self.training:</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># def create_custom_forward(module):</span></span><br><span class="line">                <span class="comment">#     def custom_forward(*inputs):</span></span><br><span class="line">                <span class="comment">#         return module(*inputs, past_key_value, output_attentions)</span></span><br><span class="line"></span><br><span class="line">                <span class="comment">#     return custom_forward</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># layer_outputs = torch.utils.checkpoint.checkpoint(</span></span><br><span class="line">                <span class="comment">#     create_custom_forward(layer_module),</span></span><br><span class="line">                <span class="comment">#     hidden_states,</span></span><br><span class="line">                <span class="comment">#     attention_mask,</span></span><br><span class="line">                <span class="comment">#     layer_head_mask,</span></span><br><span class="line">                <span class="comment">#     encoder_hidden_states,</span></span><br><span class="line">                <span class="comment">#     encoder_attention_mask,</span></span><br><span class="line">                <span class="comment"># )</span></span><br><span class="line">            <span class="comment"># else:</span></span><br><span class="line">            layer_outputs = layer_module(</span><br><span class="line">                hidden_states,</span><br><span class="line">                attention_mask,</span><br><span class="line">                layer_head_mask,</span><br><span class="line">                encoder_hidden_states,</span><br><span class="line">                encoder_attention_mask,</span><br><span class="line">                past_key_value,</span><br><span class="line">                output_attentions,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">if</span> use_cache:</span><br><span class="line">                next_decoder_cache += (layer_outputs[-<span class="number">1</span>],)</span><br><span class="line">            <span class="keyword">if</span> output_attentions:</span><br><span class="line">                all_self_attentions = all_self_attentions + (layer_outputs[<span class="number">1</span>],)<span class="comment">#不执行</span></span><br><span class="line">                <span class="comment"># if self.config.add_cross_attention:</span></span><br><span class="line">                <span class="comment">#     all_cross_attentions = all_cross_attentions + (layer_outputs[2],)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states.mean(<span class="number">0</span>),)</span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line">        hidden_states = hidden_states.mean(<span class="number">0</span>) </span><br><span class="line">        <span class="comment">#########################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> return_dict:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">tuple</span>(</span><br><span class="line">                v</span><br><span class="line">                <span class="keyword">for</span> v <span class="keyword">in</span> [</span><br><span class="line">                    hidden_states,</span><br><span class="line">                    next_decoder_cache,</span><br><span class="line">                    all_hidden_states,</span><br><span class="line">                    all_self_attentions,</span><br><span class="line">                    <span class="literal">None</span>,</span><br><span class="line">                ]</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> BaseModelOutputWithPastAndCrossAttentions(</span><br><span class="line">            last_hidden_state=hidden_states,</span><br><span class="line">            past_key_values=next_decoder_cache,</span><br><span class="line">            hidden_states=all_hidden_states,               <span class="comment">########################## all_hidden_states</span></span><br><span class="line">            attentions=all_self_attentions,</span><br><span class="line">            cross_attentions=<span class="literal">None</span>,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><br><!-- 书接上回
**layer_outputs**=BertLayer.outputs:只有一个元素tensor[4,32,128,1536]的元组 --></p>
<p><br></p>
<p><strong>all_hidden_states</strong>：()<br><strong>all_self_attentions</strong>：None<br><strong>next_decoder_cache</strong>:None<br><strong>hidden_states</strong>：tensor[32,128,1536]-&gt;repeat-&gt;tensor[4,32,128,1536]   </p>
<p>—-BertLayer—-<br><strong>self_attn_past_key_value</strong>: None<br><strong>self_attention_output</strong>: 只有一个元素的tuple<br><strong>attention_output</strong>: tensor[4,32,128,1536]<br><strong>layer_out</strong>: </p>
<ul>
<li>—-Intermediate—-</li>
<li><strong>tensor[4,32,128,1536]</strong>-&gt;(Intermediate=SpikeLinear).PSN-&gt;<strong>tensor[4,32,128,1536]的0,1向量</strong> </li>
<li><strong>tensor[4,32,128,1536]的0,1向量</strong> -&gt;(Intermediate=SpikeLinear).nn.functional.linear + self.bias-&gt;(Intermediate=SpikeLinear).scale-&gt;<strong>tensor[4,32,128,3072]</strong></li>
<li>—-output—-</li>
<li><strong>tensor[4,32,128,3072]</strong>-&gt;output.SpikeLinear.PSN-&gt;<strong>tensor[4,32,128,3072]的0,1向量</strong></li>
<li><strong>tensor[4,32,128,3072]的0,1向量</strong>-&gt;&gt;output.SpikeLinear.nn.functional.linear + self.bias-&gt;&gt;output.SpikeLinear.scale-&gt;<strong>tensor[4,32,128,1536]</strong></li>
<li><strong>tensor[4,32,128,1536]</strong>-&gt;output.dropout-&gt;<strong>tensor[4,32,128,1536]</strong></li>
<li><strong>tensor[4,32,128,1536]</strong>-&gt;output.LayerNorm-&gt;<strong>tensor[4,32,128,1536]</strong>  </li>
</ul>
<p><strong>outputs</strong>: 只有一个layer_out的tuple<br>—-BertLayer—-    </p>
<p><strong>layer_outputs</strong>=BertLayer.outputs:只有一个元素tensor[4,32,128,1536]的元组<br><strong>hidden_states</strong> = layer_outputs的第一个元素tensor[4,32,128,1536]<br><strong>hidden_states</strong>: <strong>tensor[4,32,128,1536]</strong>-&gt;mean(0)-&gt;<strong>tensor[32,128,1536]</strong></p>
<p><br></p>
<h2 id="3-PSN"><a href="#3-PSN" class="headerlink" title="3. PSN"></a>3. PSN</h2><p><strong>神经元的充电状态</strong>:</p>
<script type="math/tex; mode=display">H = WX, \quad W \in \mathbb{R}^{T \times T}, X \in \mathbb{R}^{T \times N}</script><p>这里,$H$ 表示神经元的状态,$W$ 是可学习的权重矩阵,$X$ 是输入。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h_seq = torch.addmm(self.bias, self.weight, x_seq.flatten(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p><strong>脉冲生成</strong>:  </p>
<script type="math/tex; mode=display">S = \Theta(H - B), \quad B \in \mathbb{R}^{T}, S \in \{0, 1\}^{T \times N}</script><p>在这里,$S$ 表示生成的脉冲序列,$\Theta$ 是阈值函数(比如 heaviside 函数),$B$ 是可学习的阈值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spike_seq = self.surrogate_function(h_seq)</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p><strong>完整代码</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PSN</span>(nn.Module, base.MultiStepModule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, T: <span class="built_in">int</span>, surrogate_function: surrogate.SurrogateFunctionBase = surrogate.ATan(<span class="params"></span>)</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param T: the number of time-steps</span></span><br><span class="line"><span class="string">        :type T: int</span></span><br><span class="line"><span class="string">        :param surrogate_function: the function for calculating surrogate gradients of the heaviside step function in backward</span></span><br><span class="line"><span class="string">        :type surrogate_function: Callable</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. admonition:: Note</span></span><br><span class="line"><span class="string">            :class: note</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            The PSN only supports the multi-step mode.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.T = T</span><br><span class="line">        self.surrogate_function = surrogate_function</span><br><span class="line">        weight = torch.zeros([T, T])</span><br><span class="line">        bias = torch.zeros([T, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(weight)</span><br><span class="line">        self.bias = nn.Parameter(bias)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#将权重和偏置初始化</span></span><br><span class="line">        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">        nn.init.constant_(self.bias, -<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x_seq: torch.Tensor</span>):</span><br><span class="line">        <span class="comment"># x_seq.shape = [T, N, *]</span></span><br><span class="line">        <span class="comment">#print(x_seq.shape)#[4,32,128,1536]/[4, 32, 128, 3072]</span></span><br><span class="line">        <span class="comment">#print(x_seq.flatten(1).shape)#[4,32x128x1536]/[4,32x128x3072]</span></span><br><span class="line">        h_seq = torch.addmm(self.bias, self.weight, x_seq.flatten(<span class="number">1</span>))<span class="comment">#执行矩阵乘法和加法</span></span><br><span class="line">        <span class="comment">#h_seq.shape=[4,32x128x1536]/[4,32x128x3072]</span></span><br><span class="line"></span><br><span class="line">        spike_seq = self.surrogate_function(h_seq)<span class="comment">#形状没变，只是转化成了0，1的脉冲的形式</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> spike_seq.view(x_seq.shape)<span class="comment">#将spike_seq的形状改为x_seq的形状</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().extra_repr() + <span class="string">f&#x27;, T=<span class="subst">&#123;self.T&#125;</span>&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p><br></p>
<h2 id="4-MaskedPSN"><a href="#4-MaskedPSN" class="headerlink" title="4. MaskedPSN"></a>4. MaskedPSN</h2><p><strong>神经元的充电状态</strong>:  </p>
<script type="math/tex; mode=display">H = (W \cdot M_k)X, \quad W \in \mathbb{R}^{T \times T}, M_k \in \mathbb{R}^{T \times T}, X \in \mathbb{R}^{T \times N}</script><p>这里 $H$ 表示神经元的状态,$W$ 是可学习的权重矩阵,$M_k$ 是掩码矩阵,$X$ 是输入。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h_seq = torch.addmm(self.bias, self.masked_weight(), x_seq.flatten(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p><strong>脉冲生成</strong>:   </p>
<script type="math/tex; mode=display">S = \Theta(H - B), \quad B \in \mathbb{R}^{T}, S \in \{0, 1\}^{T \times N}</script><p>在这里,$S$ 表示生成的脉冲序列,$\Theta$ 是阈值函数(例如 heaviside 函数),$B$ 是可学习的阈值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spike_seq = self.surrogate_function(h_seq).view(x_seq.shape)</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p><strong>掩码矩阵 $M_k$</strong>  </p>
<ul>
<li>$M_k$ 的定义基于其元素的位置关系:  </li>
</ul>
<script type="math/tex; mode=display">M_k[i][j] = \begin{cases}  
1, & \text{if } j \leq i \leq j + k - 1 \\
0, & \text{otherwise}  
\end{cases}</script><p>这意味着 $M_k$ 的特定元素是基于其在矩阵中的位置来决定的,这种结构允许对权重矩阵 $W$ 进行局部调整或掩码。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mask0 = torch.tril(mask1) * torch.triu(mask1, -(self.k - <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><br></p>
<p><strong>$\lambda$ 和渐进掩码过程</strong>  </p>
<ul>
<li>$\lambda$ 用于调整掩码过程,定义了一个渐进的掩码矩阵 $M_k(\lambda)$:  </li>
</ul>
<script type="math/tex; mode=display">M_k(\lambda) = \lambda \cdot M_k + (1 - \lambda) \cdot J</script><p>这里 $J$ 是一个全一矩阵。通过调整 $\lambda$ 的值,可以在完全掩码 $M_k$ 和全连接(全一矩阵 $J$)之间平滑地过渡。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gen_masked_weight</span>(<span class="params">lambda_: torch.Tensor, mask0: torch.Tensor, mask1: torch.Tensor, weight: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">return</span> (lambda_ * mask0 + (<span class="number">1.</span> - lambda_) * mask1) * weight</span><br></pre></td></tr></table></figure>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedPSN</span>(base.MemoryModule):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line"><span class="meta">    @torch.jit.script</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gen_masked_weight</span>(<span class="params">lambda_: torch.Tensor, mask0: torch.Tensor, mask1: torch.Tensor, weight: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">return</span> (lambda_ * mask0 + (<span class="number">1.</span> - lambda_) * mask1) * weight</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">masked_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.lambda_ &gt;= <span class="number">1.</span>:</span><br><span class="line">            <span class="keyword">return</span> self.weight * self.mask0</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.gen_masked_weight(self.lambda_, self.mask0, self.mask1, self.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k: <span class="built_in">int</span>, T: <span class="built_in">int</span>, lambda_init: <span class="built_in">float</span> = <span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 surrogate_function: surrogate.SurrogateFunctionBase = surrogate.ATan(<span class="params"></span>), step_mode: <span class="built_in">str</span> = <span class="string">&#x27;s&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param k: the order of the Masked PSN</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param T: the number of time-steps</span></span><br><span class="line"><span class="string">        :type T: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param lambda_init: the initial value of :math:`\\lambda` to adjust the progressive masking process</span></span><br><span class="line"><span class="string">        :type lambda_init: float</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param surrogate_function: the function for calculating surrogate gradients of the heaviside step function in backward</span></span><br><span class="line"><span class="string">        :type surrogate_function: Callable</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param step_mode: the step mode, which can be `s` (single-step) or `m` (multi-step)</span></span><br><span class="line"><span class="string">        :type step_mode: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .</span></span><br><span class="line"><span class="string">        .. admonition:: Note</span></span><br><span class="line"><span class="string">            :class: note</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            The masked PSN supports both single-step and multi-step mode. But using the multi-step mode is much faster than the single-step mode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.register_memory(<span class="string">&#x27;time_step&#x27;</span>, <span class="number">0</span>)</span><br><span class="line">        self.register_memory(<span class="string">&#x27;queue&#x27;</span>, [])</span><br><span class="line">        self.step_mode = step_mode</span><br><span class="line">        self.k = k</span><br><span class="line">        self.T = T</span><br><span class="line">        self.surrogate_function = surrogate_function</span><br><span class="line">        weight = torch.zeros([T, T])</span><br><span class="line">        bias = torch.zeros([T, <span class="number">1</span>])</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;_lambda_&#x27;</span>, torch.as_tensor(lambda_init))</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(weight)</span><br><span class="line">        self.bias = nn.Parameter(bias)</span><br><span class="line"></span><br><span class="line">        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">        nn.init.constant_(self.bias, -<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">        mask1 = torch.ones([T, T])</span><br><span class="line">        mask0 = torch.tril(mask1) * torch.triu(mask1, -(self.k - <span class="number">1</span>))</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;mask0&#x27;</span>, mask0)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;mask1&#x27;</span>, mask1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">single_step_forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">if</span> self.lambda_ &lt; <span class="number">1.</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;The masked PSN can not work in single-step mode when k &lt; 1!&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.queue.append(x.flatten())</span><br><span class="line">        <span class="keyword">if</span> self.queue.__len__() &gt; self.k:</span><br><span class="line">            self.queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.time_step + <span class="number">1</span> &gt; self.T:</span><br><span class="line">            <span class="keyword">raise</span> OverflowError(<span class="string">f&quot;The MaskedPSN(T=<span class="subst">&#123;self.T&#125;</span>) has run <span class="subst">&#123;self.time_step + <span class="number">1</span>&#125;</span> time-steps!&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        weight = self.masked_weight()[self.time_step, self.time_step + <span class="number">1</span> - self.queue.__len__(): self.time_step + <span class="number">1</span>]</span><br><span class="line">        x_seq = torch.stack(self.queue)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.dim()):</span><br><span class="line">            weight = weight.unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        h = torch.<span class="built_in">sum</span>(weight * x_seq, <span class="number">0</span>)</span><br><span class="line">        spike = self.surrogate_function(h + self.bias[self.time_step])</span><br><span class="line"></span><br><span class="line">        self.time_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> spike.view(x.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">multi_step_forward</span>(<span class="params">self, x_seq: torch.Tensor</span>):</span><br><span class="line">        <span class="comment"># x_seq.shape = [T, N, *]</span></span><br><span class="line">        <span class="keyword">assert</span> x_seq.shape[<span class="number">0</span>] == self.T</span><br><span class="line">        h_seq = torch.addmm(self.bias, self.masked_weight(), x_seq.flatten(<span class="number">1</span>))</span><br><span class="line">        spike_seq = self.surrogate_function(h_seq).view(x_seq.shape)</span><br><span class="line">        <span class="keyword">return</span> spike_seq</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lambda_</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._lambda_</span><br><span class="line"></span><br><span class="line"><span class="meta">    @lambda_.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lambda_</span>(<span class="params">self, value: <span class="built_in">float</span></span>):</span><br><span class="line">        torch.fill_(self.lambda_, value)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().extra_repr() + <span class="string">f&#x27;, lambda_=<span class="subst">&#123;self.lambda_&#125;</span>, T=<span class="subst">&#123;self.T&#125;</span>&#x27;</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="5-SlidingPSN"><a href="#5-SlidingPSN" class="headerlink" title="5. SlidingPSN"></a>5. SlidingPSN</h2><script type="math/tex; mode=display">H[t] = \sum_{i=0}^{k-1}W_{i}\cdot X[t - k + 1 + i]</script><script type="math/tex; mode=display">S[t] = \Theta(H[t] - B)</script><p>其中,$W$ 是可学习的权重,$B$ 是可学习的阈值,$\Theta$ 是阈值函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SlidingPSN</span>(base.MemoryModule):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">supported_backends</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;gemm&#x27;</span>, <span class="string">&#x27;conv&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gen_gemm_weight</span>(<span class="params">self, T: <span class="built_in">int</span></span>):</span><br><span class="line">        weight = torch.zeros([T, T], device=self.weight.device)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">            end = i + <span class="number">1</span></span><br><span class="line">            start = <span class="built_in">max</span>(<span class="number">0</span>, i + <span class="number">1</span> - self.k)</span><br><span class="line">            length = <span class="built_in">min</span>(end - start, self.k)</span><br><span class="line">            weight[i][start: end] = self.weight[self.k - length: self.k]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weight</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k: <span class="built_in">int</span>, exp_init: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 surrogate_function: surrogate.SurrogateFunctionBase = surrogate.ATan(<span class="params"></span>), step_mode: <span class="built_in">str</span> = <span class="string">&#x27;s&#x27;</span>,</span></span><br><span class="line"><span class="params">                 backend: <span class="built_in">str</span> = <span class="string">&#x27;gemm&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param k: the order of the Sliding PSN</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param exp_init: if ``True``, the weight will be initialized as ``(..., 1/4, 1/2, 1)``. If ``False``, the weight    will be initialized by the kaiming uniform</span></span><br><span class="line"><span class="string">        :type exp_init: bool</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param surrogate_function: the function for calculating surrogate gradients of the heaviside step function in backward</span></span><br><span class="line"><span class="string">        :type surrogate_function: Callable</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param step_mode: the step mode, which can be `s` (single-step) or `m` (multi-step)</span></span><br><span class="line"><span class="string">        :type step_mode: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param backend: backend fot this neuron layer, which can be &quot;gemm&quot; or &quot;conv&quot;. This option only works for the multi-step mode</span></span><br><span class="line"><span class="string">        :type backend: str</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. admonition:: Note</span></span><br><span class="line"><span class="string">            :class: note</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            The Sliding PSN supports both single-step and multi-step mode. But using the multi-step mode is much faster than the single-step mode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.register_memory(<span class="string">&#x27;queue&#x27;</span>, [])</span><br><span class="line">        self.step_mode = step_mode</span><br><span class="line">        self.k = k</span><br><span class="line">        self.surrogate_function = surrogate_function</span><br><span class="line">        self.backend = backend</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> exp_init:</span><br><span class="line">            weight = torch.ones([k])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k - <span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">                weight[i] = weight[i + <span class="number">1</span>] / <span class="number">2.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weight = torch.ones([<span class="number">1</span>, k])</span><br><span class="line">            nn.init.kaiming_uniform_(weight, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">            weight = weight[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(weight)</span><br><span class="line">        self.bias = nn.Parameter(torch.as_tensor(-<span class="number">1.</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">single_step_forward</span>(<span class="params">self, x: torch.Tensor</span>):</span><br><span class="line">        self.queue.append(x.flatten())</span><br><span class="line">        <span class="keyword">if</span> self.queue.__len__() &gt; self.k:</span><br><span class="line">            self.queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        weight = self.weight[self.k - self.queue.__len__(): self.k]</span><br><span class="line">        x_seq = torch.stack(self.queue)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.dim()):</span><br><span class="line">            weight = weight.unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        h = torch.<span class="built_in">sum</span>(weight * x_seq, <span class="number">0</span>)</span><br><span class="line">        spike = self.surrogate_function(h + self.bias)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> spike.view(x.shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">multi_step_forward</span>(<span class="params">self, x_seq: torch.Tensor</span>):</span><br><span class="line">        <span class="keyword">if</span> self.backend == <span class="string">&#x27;gemm&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            weight = self.gen_gemm_weight(x_seq.shape[<span class="number">0</span>])</span><br><span class="line">            h_seq = torch.addmm(self.bias, weight, x_seq.flatten(<span class="number">1</span>)).view(x_seq.shape)</span><br><span class="line">            <span class="keyword">return</span> self.surrogate_function(h_seq)</span><br><span class="line">        <span class="keyword">elif</span> self.backend == <span class="string">&#x27;conv&#x27;</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># x_seq.shape = [T, N, *]</span></span><br><span class="line">            x_seq_shape = x_seq.shape</span><br><span class="line">            <span class="comment"># [T, N, *] -&gt; [T, N] -&gt; [N, T] -&gt; [N, 1, T]</span></span><br><span class="line">            x_seq = x_seq.flatten(<span class="number">1</span>).t().unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            x_seq = F.pad(x_seq, pad=(self.k - <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">            x_seq = F.conv1d(x_seq, self.weight.view(<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>), stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            x_seq = x_seq.squeeze(<span class="number">1</span>).t().view(x_seq_shape)</span><br><span class="line">            <span class="keyword">return</span> self.surrogate_function(x_seq + self.bias)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(self.backend)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extra_repr</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().extra_repr() + <span class="string">f&#x27;, order=<span class="subst">&#123;self.k&#125;</span>&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="6-内存使用情况"><a href="#6-内存使用情况" class="headerlink" title="6. 内存使用情况"></a>6. 内存使用情况</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.PSN(T=<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate <strong>1.50 GiB</strong> (GPU 6; <strong>23.70 GiB total capacity</strong>; <strong>22.28 GiB already allocated</strong>; <strong>231.69 MiB free</strong>; 22.41 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">4</span>,exp_init=<span class="literal">False</span>,step_mode=<span class="string">&#x27;m&#x27;</span>,backend=<span class="string">&#x27;conv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>虽然GPU总容量为23.70 GiB，但在已分配20.84 GiB内存的情况下，仅剩1.41 GiB的空闲内存，无法满足额外的内存需求</p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">5</span>,exp_init=<span class="literal">False</span>,step_mode=<span class="string">&#x27;m&#x27;</span>,backend=<span class="string">&#x27;conv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 4; 23.70 GiB total capacity; 20.87 GiB already allocated; <strong>1.39 GiB free</strong>; 21.00 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">10</span>,exp_init=<span class="literal">False</span>,step_mode=<span class="string">&#x27;m&#x27;</span>,backend=<span class="string">&#x27;conv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 23.70 GiB total capacity; 20.98 GiB already allocated; <strong>1.27 GiB free</strong>; 21.12 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p><strong>暂时可以得出结论：k增大，越消耗内存</strong></p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">3</span>,exp_init=<span class="literal">False</span>,step_mode=<span class="string">&#x27;m&#x27;</span>,backend=<span class="string">&#x27;conv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 3; 23.70 GiB total capacity; 20.82 GiB already allocated; <strong>1.43 GiB free</strong>; 20.96 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p><strong>越来越接近需要分配的内存了</strong></p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">3</span>,exp_init=<span class="literal">False</span>,step_mode=<span class="string">&#x27;m&#x27;</span>,backend=<span class="string">&#x27;gemm&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 7; 23.70 GiB total capacity; 22.27 GiB already allocated; <strong>235.69 MiB free</strong>; 22.41 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p><strong>可以看出后端设为gemm非常消耗内存</strong></p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">3</span>,exp_init=<span class="literal">True</span>,step_mode=<span class="string">&#x27;m&#x27;</span>,backend=<span class="string">&#x27;conv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 3; 23.70 GiB total capacity; 20.82 GiB already allocated; <strong>1.43 GiB free</strong>; 20.96 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p><strong>可也看出exp_init对内存无影响</strong></p>
<p><br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">3</span>,exp_init=<span class="literal">False</span>,step_mode=<span class="string">&#x27;s&#x27;</span>,backend=<span class="string">&#x27;conv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 4; 23.70 GiB total capacity; 22.27 GiB already allocated; <strong>235.69 MiB free</strong>; 22.41 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
<p><strong>可以看出单步模式非常占内存</strong></p>
<p><br></p>
<p>以上都是bz=16的结果，下面是bz=32的结果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.lif =  neuron.SlidingPSN(k=<span class="number">3</span>,exp_init=<span class="literal">True</span>,step_mode=<span class="string">&#x27;m&#x27;</span>,backend=<span class="string">&#x27;conv&#x27;</span>)</span><br></pre></td></tr></table></figure><br>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 3; 23.70 GiB total capacity; 19.34 GiB already allocated; 2.94 GiB free; 19.45 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/12/03/spikeBERT-TBLD-BT-L-D/" data-id="clppf7qpp0009gk699u5i1tzu" data-title="spikeBERT:TBLD-&gt;BT(L)D"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/internship/" rel="tag">internship</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spikeBERT/" rel="tag">spikeBERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" rel="tag">改维度</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="nvidia-smi"
                         class="lazyload">
                
                <a href="/2023/12/03/nvidia-smi/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        nvidia-smi
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="Linux服务器上解压缩文件"
                         class="lazyload">
                
                <a href="/2023/12/03/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        Linux服务器上解压缩文件
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="あまのひな" class="lazyload">
            <div class="sidebar-author-name">あまのひな</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">249</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">23</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">299</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=28854246&auto=1&height=66"></iframe>

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Databse/">Databse</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Missing-Semester-of-CS/">Missing Semester of CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 10.67px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/2/" style="font-size: 12px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 10.67px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/4/" style="font-size: 10.67px;">4</a> <a href="/tags/5/" style="font-size: 10px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 11.33px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 12.67px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.33px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 11.33px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 12px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CV/" style="font-size: 10.67px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 13.33px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 16px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.67px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/Git/" style="font-size: 10.67px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.67px;">GoAbroad</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 10.67px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/Lec01/" style="font-size: 11.33px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.67px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.33px;">Linux</a> <a href="/tags/M2/" style="font-size: 10.67px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.67px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 12.67px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.67px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 10px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 17.33px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OS/" style="font-size: 12.67px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.67px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.33px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.67px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.67px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.67px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/University/" style="font-size: 13.33px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 10.67px;">ViT</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.67px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 12px;">bert</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/bug/" style="font-size: 14px;">bug</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.33px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10.67px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.67px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.67px;">chapter05</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/code/" style="font-size: 11.33px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.33px;">d2l</a> <a href="/tags/database/" style="font-size: 14px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.67px;">deep neural network</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/dowhy/" style="font-size: 10.67px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/explainer/" style="font-size: 10.67px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/github/" style="font-size: 10.67px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 10.67px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/instructor/" style="font-size: 12px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 12px;">intern00</a> <a href="/tags/internship/" style="font-size: 18.67px;">internship</a> <a href="/tags/introduction/" style="font-size: 11.33px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.67px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 11.33px;">mac</a> <a href="/tags/mid/" style="font-size: 10.67px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.67px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.67px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 15.33px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.33px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.33px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 12px;">prml</a> <a href="/tags/pytorch/" style="font-size: 12px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 14.67px;">review</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.33px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/spike/" style="font-size: 10.67px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.67px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikingjelly/" style="font-size: 12.67px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.67px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.67px;">ssh</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.67px;">tips</a> <a href="/tags/tool/" style="font-size: 18px;">tool</a> <a href="/tags/transformer/" style="font-size: 12px;">transformer</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10px;">vscode</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">专业知识</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">公选课</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.67px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">图像描述生成</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">大作业</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.67px;">审稿意见</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">显卡</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.67px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 13.33px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.67px;">服务器</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期中复习</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 10px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.67px;">李宏毅</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">模型训练流程</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 13.33px;">毛概</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.67px;">特征学习</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">第三章</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">评测指标</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.33px;">调研</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">软件概要设计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                あまのひな
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    521.2k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    33:12
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="あまのひな" class="lazyload">
            <div class="sidebar-author-name">あまのひな</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">249</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">23</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">299</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

