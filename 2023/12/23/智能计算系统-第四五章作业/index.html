
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        智能计算系统:第四五章作业 |
        
        blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   第四章  1.使用tensor初始化一个1x3的矩阵M和2x1的矩阵N，然后对两个矩阵做减法(用三种不同的方式，提示:直接相减、torch.nn、inplace原地操作)，最后分析三种方式的不同。  方法：  直接减法">
<meta property="og:type" content="article">
<meta property="og:title" content="智能计算系统:第四五章作业">
<meta property="og:url" content="https://abinzzz.github.io/2023/12/23/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E5%9B%9B%E4%BA%94%E7%AB%A0%E4%BD%9C%E4%B8%9A/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   第四章  1.使用tensor初始化一个1x3的矩阵M和2x1的矩阵N，然后对两个矩阵做减法(用三种不同的方式，提示:直接相减、torch.nn、inplace原地操作)，最后分析三种方式的不同。  方法：  直接减法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://viso.ai/wp-content/uploads/2021/03/performance-benchmark-pytorch-vs-tensorflow-02.jpg">
<meta property="og:image" content="https://viso.ai/wp-content/uploads/2021/03/pytorch-vs-tensorflow-accuracy.jpg">
<meta property="og:image" content="https://pbs.twimg.com/media/GCB2JB-WgAAOhBV?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GCB2XctWoAAjJN-?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GCB2lhbWIAA7rJy?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GCB2yIvXgAA9Zab?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GCB3BjTWkAAR_3Q?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GCB4MsdWMAAJmjU?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GCCJ6zfXIAAqgc_?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/GCCJb33XkAA07LE?format=jpg&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/GCCJb37XkAAXG5w?format=jpg&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/GCCJdZNWgAA15ue?format=jpg&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/GCCJdZTXsAAkolR?format=jpg&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/GCF_bUraEAAUS-9?format=jpg&amp;name=small">
<meta property="article:published_time" content="2023-12-23T08:48:59.000Z">
<meta property="article:modified_time" content="2023-12-24T15:56:16.314Z">
<meta property="article:author" content="Jerome">
<meta property="article:tag" content="专业知识">
<meta property="article:tag" content="chapter05">
<meta property="article:tag" content="chapter04">
<meta property="article:tag" content="智能计算系统">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://viso.ai/wp-content/uploads/2021/03/performance-benchmark-pytorch-vs-tensorflow-02.jpg">
    
        <link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://pbs.twimg.com/media/GBTN7RnW0AA6G9K?format=jpg&amp;name=medium" data-sizes="auto" alt="智能计算系统:第四五章作业" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>智能计算系统:第四五章作业</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-智能计算系统-第四五章作业" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/12/23/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E5%9B%9B%E4%BA%94%E7%AB%A0%E4%BD%9C%E4%B8%9A/" class="article-date-link">
        <time datetime="2023-12-23T08:48:59.000Z"
              itemprop="datePublished">2023-12-23</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><a class="article-category-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h1 id="第四章"><a class="markdownIt-Anchor" href="#第四章"></a> 第四章</h1>
<h2 id="1使用tensor初始化一个1x3的矩阵m和2x1的矩阵n然后对两个矩阵做减法用三种不同的方式提示直接相减-torchnn-inplace原地操作最后分析三种方式的不同"><a class="markdownIt-Anchor" href="#1使用tensor初始化一个1x3的矩阵m和2x1的矩阵n然后对两个矩阵做减法用三种不同的方式提示直接相减-torchnn-inplace原地操作最后分析三种方式的不同"></a> 1.使用tensor初始化一个1x3的矩阵M和2x1的矩阵N，然后对两个矩阵做减法(用三种不同的方式，提示:直接相减、torch.nn、inplace原地操作)，最后分析三种方式的不同。</h2>
<!-- - [【深度学习】深度学习实验一——PyTorch基本操作、Tensor、Logistic 回归、 实现softmax、数据集上进行训练和测试](https://blog.csdn.net/yuzhangfeng/article/details/131908194)
- [包含直接相减、torch.nn，但缺少inplace](https://blog.csdn.net/mynameisgt/article/details/128267333)
- [这个包含了三种方法！！](https://blog.csdn.net/weixin_44645198/article/details/120110641) -->
<p><strong>方法</strong>：</p>
<ul>
<li>直接减法：使用 <code>-</code> 运算符。</li>
<li><code>torch.sub</code> 函数。</li>
<li>原地减法：使用 <code>sub_</code> 方法。</li>
</ul>
<br>
<p><strong>过程：</strong><br />
首先，我们初始化了两个不同形状的张量 <code>tensor_m</code> 和 <code>tensor_n</code>。接着，我们分别使用了上述三种方法对这两个张量进行减法操作。</p>
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor_m = torch.rand(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">tensor_n = torch.rand(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量M:\n&quot;</span>, tensor_m)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量N:\n&quot;</span>, tensor_n)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行并显示不同的减法方法</span></span><br><span class="line"><span class="comment"># 方法1：直接减法</span></span><br><span class="line">result_direct = tensor_m - tensor_n</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;减法方法1 - 直接减法:\n&quot;</span>, result_direct)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：使用torch.sub函数</span></span><br><span class="line">result_torch_sub = torch.sub(tensor_m, tensor_n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;减法方法2 - 使用torch.sub:\n&quot;</span>, result_torch_sub)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法3：在tensor_n的副本上进行原地减法,避免修改原始的tensor_n</span></span><br><span class="line">tensor_n_clone = tensor_n.clone()</span><br><span class="line">tensor_n_clone.sub_(tensor_m)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;减法方法3 - 在副本上inplace原地减法:\n&quot;</span>, tensor_n_clone)</span><br></pre></td></tr></table></figure>
<br>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">张量M:</span><br><span class="line"> tensor([[<span class="number">0.4984</span>, <span class="number">0.7894</span>, <span class="number">0.7699</span>]])</span><br><span class="line">张量N:</span><br><span class="line"> tensor([[<span class="number">0.9350</span>],</span><br><span class="line">        [<span class="number">0.8822</span>]])</span><br><span class="line">减法方法<span class="number">1</span> - 直接减法:</span><br><span class="line"> tensor([[-<span class="number">0.4365</span>, -<span class="number">0.1456</span>, -<span class="number">0.1651</span>],</span><br><span class="line">        [-<span class="number">0.3838</span>, -<span class="number">0.0928</span>, -<span class="number">0.1123</span>]])</span><br><span class="line">减法方法<span class="number">2</span> - 使用torch.sub:</span><br><span class="line"> tensor([[-<span class="number">0.4365</span>, -<span class="number">0.1456</span>, -<span class="number">0.1651</span>],</span><br><span class="line">        [-<span class="number">0.3838</span>, -<span class="number">0.0928</span>, -<span class="number">0.1123</span>]])</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/Users/chenyubin/PycharmProjects/智能计算系统习题/4.1.py&quot;</span>, line <span class="number">20</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    tensor_n_clone.sub_(tensor_m)</span><br><span class="line">RuntimeError: output <span class="keyword">with</span> shape [<span class="number">2</span>, <span class="number">1</span>] doesn<span class="string">&#x27;t match the broadcast shape [2, 3]</span></span><br></pre></td></tr></table></figure>
<br>
<!-- **方法1 - 直接减法**:  
这是最直观的方法，使用 `-` 运算符直接对两个张量进行减法。在这种情况下，PyTorch自动应用了广播（broadcasting）机制。广播是一种在不同形状的张量间进行算术运算的机制，它扩展张量的维度以匹配彼此的形状。`tensor_m` 的形状是 `[1, 3]`，而 `tensor_n` 的形状是 `[2, 1]`。广播机制将这两个张量扩展到一个共同形状 `[2, 3]`，然后执行逐元素的减法。

<br>

**方法2 - 使用 `torch.sub`:**  
`torch.sub` 函数是专门用于减法的函数。它提供了更明确、可读性更强的代码方式。它的行为与直接使用 `-` 运算符相同，同样应用了广播机制，产生了与方法1相同的结果。

<br>

**方法3 - 在副本上原地（inplace）减法:**  
这种方法首先创建了 `tensor_n` 的副本，然后在这个副本上使用 `sub_` 方法进行原地减法。原地操作（如 `sub_`）是直接在原有的数据上进行修改，而不是创建新的数据副本。这样可以节省内存，但需要小心处理，因为它会改变原始数据。在你的例子中，这个方法失败了，因为 `tensor_n_clone.sub_(tensor_m)` 尝试将形状 `[1, 3]` 的 `tensor_m` 减去形状 `[2, 1]` 的 `tensor_n_clone`。PyTorch 的原地操作不支持自动广播，因此这里产生了一个 `RuntimeError` 错误。 -->
<p><strong>结果：</strong><br />
<strong>直接减法</strong>和<strong>使用 <code>torch.sub</code></strong> 方法在功能上是等效的，均成功执行了减法操作。这两种方法利用了PyTorch的广播机制，自动扩展张量维度以匹配形状，最终得到形状为 <code>[2, 3]</code> 的结果张量。<strong>原地减法</strong>尝试在 <code>tensor_n</code> 的副本上直接减去 <code>tensor_m</code>。然而，由于原地操作不支持广播机制，形状不兼容导致实验失败，产生 <code>RuntimeError</code>。</p>
<p><strong>直接减法和使用 <code>torch.sub</code></strong> 是较为简单且安全的选择，因为它们都支持广播机制，适用于形状不同的张量之间的运算。相比之下，<strong>原地减法</strong>在节省内存方面有优势，但由于不支持广播，需要更谨慎地处理张量形状以确保兼容性。</p>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<h2 id="2全面对比pytorch和tensorflow分析pytorch的优势"><a class="markdownIt-Anchor" href="#2全面对比pytorch和tensorflow分析pytorch的优势"></a> 2.全面对比Pytorch和Tensorflow，分析Pytorch的优势。</h2>
<!-- - [PyTorch与TensorFlow全面对比：哪个更适合工业界](https://docs.pingcode.com/ask/47199.html)
- [实例分析](https://cloud.tencent.com/developer/article/2212225)
- 第五章ppt P89 -->
<h2 id="21-主要内容比较"><a class="markdownIt-Anchor" href="#21-主要内容比较"></a> 2.1 <code>主要内容比较</code></h2>
<table>
<thead>
<tr>
<th>深度学习框架</th>
<th>PyTorch</th>
<th>TensorFlow</th>
</tr>
</thead>
<tbody>
<tr>
<td>主要维护单位</td>
<td>Facebook</td>
<td>Google</td>
</tr>
<tr>
<td>前端支持语言</td>
<td>Python, C++</td>
<td>Python, C/C++, Java, Go, JavaScript, R, Julia, Swift</td>
</tr>
<tr>
<td>支持平台</td>
<td>Linux, MacOS, Windows</td>
<td>Linux, MacOS, Windows, iOS, Android</td>
</tr>
<tr>
<td>编程类型</td>
<td>命令式</td>
<td>Graph: 声明式; Eager: 命令式</td>
</tr>
<tr>
<td>辅助工具生态</td>
<td>TorchVision, 官方模型库</td>
<td>TensorBoard, Profiler, TFLite, TF-serving, tfdbg, 官方模型库</td>
</tr>
<tr>
<td>易用性</td>
<td>语法更符合Python，调试更容易</td>
<td>学习曲线较陡，需要更多模板代码</td>
</tr>
<tr>
<td>动态计算图</td>
<td>运行时更容易修改</td>
<td>需要重新编译才能修改</td>
</tr>
<tr>
<td>GPU 支持</td>
<td>设置和使用多GPU更简单</td>
<td>多GPU支持更复杂，需要更多设置，有专门的TF API</td>
</tr>
<tr>
<td>社区支持</td>
<td>相对较新，发展迅速</td>
<td>大型且活跃，资源丰富</td>
</tr>
<tr>
<td>生态系统</td>
<td>与TensorFlow相比库和工具较少</td>
<td>拥有广泛的预构建模型和工具库</td>
</tr>
<tr>
<td>调试</td>
<td>由于Python语法和动态计算图，调试更容易</td>
<td>由于静态计算图，调试可能更具挑战性</td>
</tr>
<tr>
<td>研究应用</td>
<td>由于其灵活性和易用性，常用于研究</td>
<td>由于速度和可扩展性，常用于生产应用</td>
</tr>
<tr>
<td>数学库</td>
<td>使用TorchScript进行张量操作，NumPy进行数值计算</td>
<td>使用自有数学库进行张量操作和数值计算</td>
</tr>
<tr>
<td>Keras 集成</td>
<td>没有原生Keras集成</td>
<td>有原生Keras集成，简化模型构建和训练</td>
</tr>
</tbody>
</table>
<br>
<h2 id="22-性能比较"><a class="markdownIt-Anchor" href="#22-性能比较"></a> 2.2 <code>性能比较</code></h2>
<p>通过对PyTorch和TensorFlow两个深度学习框架进行性能基准测试，以对比和分析这两个框架在单机热切模式下的性能表现。本次性能测试使用了32位浮点数的两个不同的模型，分别为AlexNet、VGG-19、ResNet-50、MobileNet、GNMTv2和NCF模型。测试指标包括各模型的吞吐率，分别以每秒图像数、每秒令牌数和每秒样本数为单位来测量。基准测试的结果表明，PyTorch在性能方面优于TensorFlow。具体而言，PyTorch在处理相同的模型时表现出更高的吞吐率。这一性能优势可能源于PyTorch和TensorFlow都将计算任务主要卸载给了同版本的cuDNN和cuBLAS库。</p>
<p><img src="https://viso.ai/wp-content/uploads/2021/03/performance-benchmark-pytorch-vs-tensorflow-02.jpg" alt="" /></p>
<br>
<h2 id="23-精度比较"><a class="markdownIt-Anchor" href="#23-精度比较"></a> 2.3 <code>精度比较</code></h2>
<p>通过对两个框架下的模型进行训练精度和验证精度的对比，评估模型在记忆训练信息和学习新信息的能力。精度数据收集是在经过20个epoch的训练后进行的。结果显示，PyTorch和TensorFlow在模型精度方面表现相似。两个框架中的模型在经过20个epoch的训练后，平均验证精度约为78%。这表明两个框架都能够准确实现神经网络，并在相同的模型和数据集条件下产生类似的结果。</p>
<p><img src="https://viso.ai/wp-content/uploads/2021/03/pytorch-vs-tensorflow-accuracy.jpg" alt="" /></p>
<br>
<h2 id="24-训练时间和内存使用上图"><a class="markdownIt-Anchor" href="#24-训练时间和内存使用上图"></a> 2.4 <code>训练时间和内存使用(上图)</code></h2>
<p>通过测量并对比两个框架下的模型训练时间以及内存使用情况。结果显示，TensorFlow的平均训练时间（11.19秒）明显高于PyTorch（7.67秒）。在内存使用方面，TensorFlow的使用率（1.7 GB RAM）低于PyTorch（3.5 GB RAM）。两个框架在初始加载数据期间的内存使用更高，分别为TensorFlow的4.8 GB和PyTorch的5 GB。这些结果表明，PyTorch不仅在训练速度上有优势，而且在内存使用上更为高效。</p>
<br>
<h2 id="25-pytorch优势"><a class="markdownIt-Anchor" href="#25-pytorch优势"></a> 2.5 <code>Pytorch优势</code></h2>
<p><strong>以Python为中心的设计</strong>：PyTorch深度集成于Python，提供了与Python代码高度一致的“Python风格”的编程体验。这使得PyTorch尤其受到使用Python作为主要编程语言的数据科学家和机器学习研究人员的青睐。</p>
<p><strong>易于学习和使用</strong>：PyTorch的语法类似于传统的编程语言，如Python，因此相比于其他深度学习框架，它更容易学习和使用。</p>
<p><strong>动态计算图</strong>：PyTorch支持动态计算图，这意味着网络的行为可以在运行时编程方式地更改，从而使得优化模型变得更加简单。</p>
<p><strong>高效的调试工具</strong>：PyTorch可以使用Python中广泛可用的调试工具进行调试，例如pdb和ipdb。</p>
<p><strong>数据并行处理</strong>：PyTorch的数据并行特性允许在多个CPU或GPU核心之间分配计算工作，这种并行处理在PyTorch中实现起来比其他机器学习工具更为简单。</p>
<p><strong>活跃的社区和优秀的文档</strong>：PyTorch拥有一个活跃的社区和论坛，其文档整洁有序，对初学者非常友好，同时与PyTorch的发布版本保持同步。</p>
<p><strong>分布式训练支持</strong>：PyTorch原生支持集体操作的异步执行和端到端通信，既可以从Python访问，也可以从C++访问。</p>
<p><strong>适合研究用途</strong>：由于其灵活性和易用性，PyTorch常用于研究领域，尤其适合自然语言处理（NLP）等任务。</p>
<p><strong>更快的原型设计</strong>：与TensorFlow相比，PyTorch允许更快速的原型设计，尤其在需要快速迭代和实验的研究和开发环境中表现突出。</p>
<p><strong>对ONNX的原生支持</strong>：PyTorch提供对ONNX（开放神经网络交换）格式的原生支持，允许无缝模型导出并与ONNX兼容的平台和工具兼容。</p>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<h2 id="3给出torchnn和torchnnfunctional之间的区别"><a class="markdownIt-Anchor" href="#3给出torchnn和torchnnfunctional之间的区别"></a> 3.给出torch.nn和torch.nn.functional之间的区别。</h2>
<!-- - [Pytorch中torch.nn和torch.nn.functional的区别及实例详解](https://blog.csdn.net/MRZHUGH/article/details/113199202)
- [PyTorch 中，nn 与 nn.functional 有什么区别？](https://www.zhihu.com/question/66782101)
- [【pytorch】torch.nn 与 torch.nn.functional 的区别](https://blog.csdn.net/wangweiwells/article/details/100531264)
- [TensorFlow vs PyTorch的优缺点与区别](https://blog.csdn.net/qq_42804713/article/details/125208925)
- [Pytorch vs Tensorflow: A Head-to-Head Comparison](https://viso.ai/deep-learning/pytorch-vs-tensorflow/) -->
<h3 id="31-两者的区别"><a class="markdownIt-Anchor" href="#31-两者的区别"></a> 3.1 <code>两者的区别</code></h3>
<p><strong>相同之处：</strong></p>
<ul>
<li>两者都继承于 <code>nn.Module</code></li>
<li><code>nn.x</code> 与 <code>nn.functional.x</code> 的实际功能相同，比如 <code>nn.Conv3d</code> 和 <code>nn.functional.conv3d</code> 都是进行 3d 卷积</li>
<li>运行效率几乎相同</li>
</ul>
<p><strong>不同之处:</strong></p>
<ul>
<li><code>nn.x</code> 是 <code>nn.functional.x</code> 的类封装，而 <code>nn.functional.x</code> 是具体的函数接口</li>
<li><code>nn.x</code> 除了具有 <code>nn.functional.x</code> 功能之外，还具有 <code>nn.Module</code> 相关的属性和方法，比如：<code>train()</code>, <code>eval()</code> 等</li>
<li><code>nn.functional.x</code> 直接传入参数调用，<code>nn.x</code> 需要先实例化再传参调用</li>
<li><code>nn.x</code> 能很好的与 <code>nn.Sequential</code> 结合使用，而 <code>nn.functional.x</code> 无法与 <code>nn.Sequential</code> 结合使用</li>
<li><code>nn.x</code> 不需要自定义和管理 weight，而 <code>nn.functional.x</code> 需要自定义 weight，作为传入的参数</li>
</ul>
<br>
<h3 id="32-实例分析"><a class="markdownIt-Anchor" href="#32-实例分析"></a> 3.2 实例分析</h3>
<p>查看两者的doc即可看出区别,即一个侧重数据结构,一个侧重算法运算。其实两个都是完成了同样的功能,只是实现方式有些不同而已:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.conv2d(<span class="built_in">input</span>, weight, bias=<span class="literal">None</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>) → Tensor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=‘zeros’)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>torch.nn</strong> 这个模块下面存的主要是 <strong>Module类</strong>。以 <code>torch.nn.Conv2d</code> 为例, 也就是说 <code>torch.nn.Conv2d</code> 这种&quot;函数&quot;其实是个 <strong>Module类</strong>, 在实例化类后会初始化2d卷积所需要的参数。这些参数会在你做forward和 backward之后根据loss进行更新,所以通常存放在定义模型的 <code>_init_()</code> 中。如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvolutionalModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvolutionalModel, self).__init__()</span><br><span class="line">        <span class="comment"># 初始化卷积层：输入通道3，输出通道6，卷积核大小3，步长1，填充1</span></span><br><span class="line">        self.convolution_layer = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 初始化激活函数为ReLU</span></span><br><span class="line">        self.activation_function = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_tensor</span>):</span><br><span class="line">        <span class="comment"># 应用卷积层</span></span><br><span class="line">        conv_output = self.convolution_layer(input_tensor)</span><br><span class="line">        <span class="comment"># 应用ReLU激活函数</span></span><br><span class="line">        activated_output = self.activation_function(conv_output)</span><br><span class="line">        <span class="keyword">return</span> activated_output</span><br></pre></td></tr></table></figure>
<p>但如果像 <strong>torch.nn.functional</strong> 一样把 <code>nn.Conv2d</code> 写在forward处，就相当于模型每次跑forward的时候,都重新实例化了 <code>nn.Conv2d</code> 和 <code>nn.Conv2d</code> 的参数,导致模型学不到参数。</p>
<br>
<p><strong>torch.nn.functional.x</strong> 为函数,与 <strong>torch.nn</strong> 不同, <code>torch.nn.x</code> 中包含了初始化需要的参数等 attributes 而 <code>torch.nn.functional.x</code> 则需要把相应的weights 作为输入参数传递,才能完成运算, 所以用 <code>torch.nn.functional</code> 创建模型时需要创建并初始化相应参数。<br />
例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FunctionalConvolutionalModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FunctionalConvolutionalModel, self).__init__()</span><br><span class="line">        <span class="comment"># 初始化权重和偏置作为模型的参数</span></span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))  <span class="comment"># 假设输出通道为6，输入通道为3，卷积核为3x3</span></span><br><span class="line">        self.bias = nn.Parameter(torch.randn(<span class="number">6</span>))</span><br><span class="line">        <span class="comment"># 初始化激活函数为ReLU</span></span><br><span class="line">        self.activation_function = nn.ReLU()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_tensor</span>):</span><br><span class="line">        <span class="comment"># 使用functional接口应用卷积，传入权重和偏置</span></span><br><span class="line">        conv_output = F.conv2d(input_tensor, self.weights, self.bias)</span><br><span class="line">        <span class="comment"># 应用ReLU激活函数</span></span><br><span class="line">        activated_output = self.activation_function(conv_output)</span><br><span class="line">        <span class="keyword">return</span> activated_output</span><br></pre></td></tr></table></figure>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<h2 id="4请实现两个数的加法即计算-ab并输出其中-a-是常量b是占位符数据类型自定"><a class="markdownIt-Anchor" href="#4请实现两个数的加法即计算-ab并输出其中-a-是常量b是占位符数据类型自定"></a> 4.请实现两个数的加法，即计算 A+B并输出，其中 A 是常量，B是占位符，数据类型自定。</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#由于我的tf版本是2.13.0，所以需要启用 TensorFlow 1.x 兼容模式，才能使用tf.placeholder 和 tf.Session</span></span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 TensorFlow 常量 A，值为 2，数据类型为 float32</span></span><br><span class="line">A = tf.constant(<span class="number">2</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 TensorFlow 占位符 B，用于稍后输入数据，数据类型为 float32</span></span><br><span class="line">B = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算和</span></span><br><span class="line">C = tf.add(A, B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 TensorFlow 会话来执行图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 运行会话，计算 C 的值，同时通过 feed_dict 提供 B 的值</span></span><br><span class="line">    result = sess.run(C, feed_dict=&#123;B: <span class="number">2</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<br>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4.0</span></span><br></pre></td></tr></table></figure>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<h2 id="5请实现一个矩阵乘法数据类型和规模自定并分别使用-cpu和gpu-执行"><a class="markdownIt-Anchor" href="#5请实现一个矩阵乘法数据类型和规模自定并分别使用-cpu和gpu-执行"></a> 5.请实现一个矩阵乘法，数据类型和规模自定，并分别使用 CPU和GPU 执行。</h2>
<p>由于我使用的是MacOS设备，所以在 PyTorch 中使用 Metal Performance Shaders (MPS) 作为后端进行计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_matrix_multiplication_on_device</span>(<span class="params">device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在指定的设备（MPS）上执行矩阵乘法。</span></span><br><span class="line"><span class="string">    测量并打印操作所需的时间。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    device: str，指定使用的设备，即&#x27;mps&#x27;。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置 PyTorch 张量使用的设备</span></span><br><span class="line">    device = torch.device(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化两个 4000x4000 的随机矩阵，并将它们转移到指定设备</span></span><br><span class="line">    matrix_A = torch.randn(<span class="number">4000</span>, <span class="number">4000</span>, device=device)</span><br><span class="line">    matrix_B = torch.randn(<span class="number">4000</span>, <span class="number">4000</span>, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行矩阵乘法</span></span><br><span class="line">    product_matrix = torch.matmul(matrix_A, matrix_B)</span><br><span class="line"></span><br><span class="line">    elapsed_time = time.time() - start_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;device&#125;</span> 上的矩阵乘法：耗时 = <span class="subst">&#123;elapsed_time&#125;</span> 秒&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 在 CPU 上运行矩阵乘法</span></span><br><span class="line">    run_matrix_multiplication_on_device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果可用，在 MPS 上运行矩阵乘法，否则打印消息</span></span><br><span class="line">    <span class="keyword">if</span> torch.backends.mps.is_available():</span><br><span class="line">        run_matrix_multiplication_on_device(<span class="string">&#x27;mps&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;无可用 MPS&quot;</span>)</span><br></pre></td></tr></table></figure>
<br>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cpu 上的矩阵乘法：耗时 = <span class="number">0.43274903297424316</span> 秒</span><br><span class="line">mps 上的矩阵乘法：耗时 = <span class="number">0.03922915458679199</span> 秒</span><br></pre></td></tr></table></figure>
<p>GPU上的矩阵乘法明显比 CPU 上的矩阵乘法更快，执行时间仅为 CPU 执行时间的约十分之一，显示了其加速的强大性能</p>
<br>
<h2 id="6-请调研了解常用的图像数据预处理和数据增强方法-实现一个函数从imagenet2012_val数据集中选择一张图片文件并读入数据调整为2562563大小的图片然后居中裁剪为2242243大小的图片再实现一个函数读入数据后居中裁剪为0875-width0875-height3大小的图片再调整为2242243-大小的图片"><a class="markdownIt-Anchor" href="#6-请调研了解常用的图像数据预处理和数据增强方法-实现一个函数从imagenet2012_val数据集中选择一张图片文件并读入数据调整为2562563大小的图片然后居中裁剪为2242243大小的图片再实现一个函数读入数据后居中裁剪为0875-width0875-height3大小的图片再调整为2242243-大小的图片"></a> 6. 请调研了解常用的图像数据预处理和数据增强方法。实现一个函数，从ImageNet2012_val数据集中选择一张图片文件并读入数据，调整为(256，256，3)大小的图片，然后居中裁剪为(224，224，3)大小的图片;再实现一个函数，读入数据后居中裁剪为(0.875* width，0.875 * height，3)大小的图片，再调整为(224，224，3) 大小的图片。</h2>
<!-- - [图片数据的基本预处理与数据增强](https://blog.csdn.net/qq_44289607/article/details/122853933)
- [数据增强及预处理](https://blog.csdn.net/mzpmzk/article/details/80039481)
- [imagenet下载地址](https://pan.baidu.com/s/1MEjNh6evha2hcdrQXjNv8w?pwd=yzza) -->
<h2 id="61-数据增强"><a class="markdownIt-Anchor" href="#61-数据增强"></a> 6.1 <code>数据增强</code></h2>
<p>(1)翻转（Flip）：将图像沿水平或垂直方法随机翻转一定角度；<br />
<img src="https://pbs.twimg.com/media/GCB2JB-WgAAOhBV?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>(2)旋转（Rotation）：将图像按顺时针或逆时针方向随机旋转一定角度；<br />
<img src="https://pbs.twimg.com/media/GCB2XctWoAAjJN-?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>(3)平移（Shift）：将图像沿水平或垂直方法平移一定步长；</p>
<p><img src="https://pbs.twimg.com/media/GCB2lhbWIAA7rJy?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>(4)缩放（Resize）：将图像放大或缩小；</p>
<br>
<p>(5)随机裁剪或补零（Random Crop or Pad）：将图像随机裁剪或补零到指定大小;<br />
<img src="https://pbs.twimg.com/media/GCB2yIvXgAA9Zab?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>(6)色彩抖动（Color jittering）：HSV 颜色空间随机改变图像原有的饱和度和明度（即，改变 S 和 V 通道的值）或对色调(Hue)进行小范围微调。</p>
<p><img src="https://pbs.twimg.com/media/GCB3BjTWkAAR_3Q?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>(7)加噪声（Noise）：加入随机噪声。</p>
<br>
<p>(8)特殊的数据增强方法：</p>
<ul>
<li>Fancy PCA（Alexnet）&amp; 监督式数据扩充（海康）</li>
<li>使用生成对抗网络（GAN） 生成模拟图像</li>
</ul>
<br>
<h2 id="62-数据预处理"><a class="markdownIt-Anchor" href="#62-数据预处理"></a> 6.2 <code>数据预处理</code></h2>
<p>(1)去均值与归一化<br />
<img src="https://pbs.twimg.com/media/GCB4MsdWMAAJmjU?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>(2)尺寸调整:需要输入最终希望得到的图像尺寸。</p>
<br>
<p>(3)颜色变换：调整亮度、对比度、饱和度等。</p>
<br>
<p>(4)灰度转换：将RGB图像转换为灰度图像。</p>
<br>
<p>(5)ToTensor(PIL Only): 将任意图像转变为Tensor</p>
<br>
<h3 id="63-程序实现"><a class="markdownIt-Anchor" href="#63-程序实现"></a> 6.3 <code>程序实现</code></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">image_path = <span class="string">&quot;dog.jpg&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_image</span>(<span class="params">image, output_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将处理后的图像保存到指定路径。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    image: 处理后的图像，为numpy数组格式。</span></span><br><span class="line"><span class="string">    output_path: 输出图像的路径。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cv.imwrite(output_path, image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">crop_image</span>(<span class="params">image, target_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将图像中心裁剪到指定的目标尺寸。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">        image: 输入的图像，为numpy数组格式。</span></span><br><span class="line"><span class="string">        target_size: 一个元组（高度, 宽度），指定裁剪图像的大小。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">        裁剪后的图像。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    original_height, original_width = image.shape[:<span class="number">2</span>]</span><br><span class="line">    target_height, target_width = target_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确保原始图像大于目标尺寸</span></span><br><span class="line">    <span class="keyword">assert</span> original_height &gt; target_height <span class="keyword">and</span> original_width &gt; target_width, \</span><br><span class="line">        <span class="string">&quot;输入图像的尺寸必须大于目标尺寸。&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算裁剪坐标</span></span><br><span class="line">    start_y = <span class="built_in">int</span>((original_height - target_height) / <span class="number">2</span>)</span><br><span class="line">    end_y = start_y + target_height</span><br><span class="line">    start_x = <span class="built_in">int</span>((original_width - target_width) / <span class="number">2</span>)</span><br><span class="line">    end_x = start_x + target_width</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 裁剪并返回图像</span></span><br><span class="line">    cropped_image = image[start_y:end_y, start_x:end_x]</span><br><span class="line">    <span class="keyword">return</span> cropped_image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_A</span>(<span class="params">image_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    处理图像，先调整大小然后裁剪。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    image_path: 输入图像的路径。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    image = cv.imread(image_path)</span><br><span class="line">    <span class="keyword">if</span> image <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;无法加载图像:&quot;</span>, image_path)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;原始图像尺寸: <span class="subst">&#123;image.shape[:<span class="number">2</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    resized_image = cv.resize(image, (<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;过程A调整大小后的图像尺寸: <span class="subst">&#123;resized_image.shape[:<span class="number">2</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    save_image(resized_image, <span class="string">&quot;process_A_output1.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">    cropped_image = crop_image(resized_image, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;过程A裁剪后的图像尺寸: <span class="subst">&#123;cropped_image.shape[:<span class="number">2</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    save_image(cropped_image, <span class="string">&quot;process_A_output2.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_B</span>(<span class="params">image_path</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    处理图像，先裁剪然后调整大小。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    image_path: 输入图像的路径。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    image = cv.imread(image_path)</span><br><span class="line">    <span class="keyword">if</span> image <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;无法加载图像:&quot;</span>, image_path)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;原始图像尺寸: <span class="subst">&#123;image.shape[:<span class="number">2</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    resized_dimensions = (<span class="built_in">int</span>(image.shape[<span class="number">0</span>] * <span class="number">0.875</span>), <span class="built_in">int</span>(image.shape[<span class="number">1</span>] * <span class="number">0.875</span>))</span><br><span class="line">    cropped_image = crop_image(image, resized_dimensions)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;过程B裁剪后的图像尺寸: <span class="subst">&#123;cropped_image.shape[:<span class="number">2</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    save_image(cropped_image, <span class="string">&quot;process_B_output1.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">    resized_image = cv.resize(cropped_image, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;过程B调整大小后的图像尺寸: <span class="subst">&#123;resized_image.shape[:<span class="number">2</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    save_image(resized_image, <span class="string">&quot;process_B_output2.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    process_A(image_path)</span><br><span class="line">    process_B(image_path)</span><br></pre></td></tr></table></figure>
<br>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">原始图像尺寸: (<span class="number">375</span>, <span class="number">500</span>)</span><br><span class="line">过程A调整大小后的图像尺寸: (<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">过程A裁剪后的图像尺寸: (<span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">原始图像尺寸: (<span class="number">375</span>, <span class="number">500</span>)</span><br><span class="line">过程B裁剪后的图像尺寸: (<span class="number">328</span>, <span class="number">437</span>)</span><br><span class="line">过程B调整大小后的图像尺寸: (<span class="number">224</span>, <span class="number">224</span>)</span><br></pre></td></tr></table></figure>
<br>
<p>原始图像：<br />
<img src="https://pbs.twimg.com/media/GCCJ6zfXIAAqgc_?format=jpg&amp;name=medium" alt="" /></p>
<br>
<p>过程A:<br />
<img src="https://pbs.twimg.com/media/GCCJb33XkAA07LE?format=jpg&amp;name=900x900" alt="" /><br />
<img src="https://pbs.twimg.com/media/GCCJb37XkAAXG5w?format=jpg&amp;name=900x900" alt="" /></p>
<br>
<p>过程B:<br />
<img src="https://pbs.twimg.com/media/GCCJdZNWgAA15ue?format=jpg&amp;name=900x900" alt="" /><br />
<img src="https://pbs.twimg.com/media/GCCJdZTXsAAkolR?format=jpg&amp;name=900x900" alt="" /></p>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<h2 id="7-计算t1-t2的结果-t1torchtensor1234dtypetorchfloat32t2torchtensor9876dtypetorchfloat32"><a class="markdownIt-Anchor" href="#7-计算t1-t2的结果-t1torchtensor1234dtypetorchfloat32t2torchtensor9876dtypetorchfloat32"></a> 7. 计算t1 + t2的结果。 t1=torch.tensor([[1,2],[3,4]],dtype=torch.float32),t2=torch.tensor([[9,8],[7,6]],dtype=torch.float32)</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">t1=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype=torch.float32)</span><br><span class="line">t2=torch.tensor([[<span class="number">9</span>,<span class="number">8</span>],[<span class="number">7</span>,<span class="number">6</span>]],dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">res=t1+t2</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<br>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">10.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">10.</span>, <span class="number">10.</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<h1 id="第五章"><a class="markdownIt-Anchor" href="#第五章"></a> 第五章</h1>
<h2 id="1什么是pytorchpytorch的基本要素是什么"><a class="markdownIt-Anchor" href="#1什么是pytorchpytorch的基本要素是什么"></a> 1.什么是PyTorch?PyTorch的基本要素是什么?</h2>
<!-- - [pytorch是什么？解释pytorch的基本概念](https://blog.csdn.net/gu1857035894/article/details/125235535)
- [什么是 PyTorch以及PyTorch 的特点](https://blog.csdn.net/qq_43539854/article/details/108626803) -->
<p>PyTorch是一个由Facebook的人工智能研究团队开发的开源深度学习框架。在2016年发布后，PyTorch很快就因其易用性、灵活性和强大的功能而在科研社区中广受欢迎。</p>
<p>在2016年，Facebook的AI研究团队（FAIR）公开了PyTorch，其旨在提供一个快速，灵活且动态的深度学习框架。PyTorch的设计哲学与Python的设计哲学非常相似：易读性和简洁性优于隐式的复杂性。PyTorch用Python语言编写，是Python的一种扩展，这使得其更易于学习和使用。</p>
<p>PyTorch在设计上取了一些大胆的决定，其中最重要的一项就是选择动态计算图（Dynamic Computation Graph）作为其核心。动态计算图与其他框架（例如TensorFlow和Theano）中的静态计算图有着本质的区别，它允许我们在运行时改变计算图。这使得PyTorch在处理复杂模型时更具灵活性，并且对于研究人员来说，更易于理解和调试。</p>
<p>在发布后的几年里，PyTorch迅速在科研社区中取得了广泛的认可。在2019年，PyTorch发布了1.0版本，引入了一些重要的新功能，包括支持ONNX、一个新的分布式包以及对C++的前端支持等。这些功能使得PyTorch在工业界的应用更加广泛，同时也保持了其在科研领域的强劲势头。</p>
<p>到了近两年，PyTorch已经成为全球最流行的深度学习框架之一。其在GitHub上的星标数量超过了50k，被用在了各种各样的项目中，从最新的研究论文到大规模的工业应用。</p>
<br>
<p>PyTorch基本要素：</p>
<ul>
<li>
<p>张量（Tensors）：张量是PyTorch中的基本数据结构，与NumPy的数组类似，但它们可以在GPU上运行以加速计算。</p>
</li>
<li>
<p>动态计算图（Dynamic Computation Graphs）：PyTorch使用动态计算图，这意味着图在运行时构建，允许更多灵活性。在PyTorch中，这通常通过自动微分（Autograd）系统实现。</p>
</li>
<li>
<p>自动微分（Autograd）：PyTorch的Autograd模块提供了自动微分机制，这是训练神经网络的关键部分。它允许用户轻松计算导数。</p>
</li>
<li>
<p>神经网络（nn）模块：这个模块提供了构建神经网络的工具。它包括预定义的层、损失函数和优化器。</p>
</li>
<li>
<p>优化器（Optimizers）：PyTorch提供了各种优化算法，如Adam、SGD等，用于在训练过程中更新网络的权重和参数。</p>
</li>
<li>
<p>数据加载和处理（Data loading and Processing）：PyTorch提供了数据加载器和转换器，方便用户高效地加载和处理数据集。</p>
</li>
</ul>
<br>
<br>
<br>
<br>
<h2 id="2-pytorch如何使用多gpu列举出两种方法并将两种方法进行对比"><a class="markdownIt-Anchor" href="#2-pytorch如何使用多gpu列举出两种方法并将两种方法进行对比"></a> 2. Pytorch如何使用多GPU?列举出两种方法，并将两种方法进行对比。</h2>
<!-- - [pytorch的多GPU训练的两种方式](https://blog.csdn.net/Mr_health/article/details/122822483)
- [pytorch多卡训练nn.DataParallel和nn.DistributedDataParallel比较](https://blog.csdn.net/weixin_38076506/article/details/124227219) -->
<h3 id="torchnndataparallel"><a class="markdownIt-Anchor" href="#torchnndataparallel"></a> <code>torch.nn.DataParallel</code></h3>
<p><strong>工作原理</strong>:</p>
<p>这种方式只有一个进程，假设我们有一个8卡的机器，然后设置的batchsize是128，那么单卡的batchsize就是32，在训练的时候，他把我们的模型分发到每个GPU上，然后在每个GPU上面对着32张图进行前向和反向传播得到loss，之后将所有的loss汇总到0卡上，对0卡的参数进行更新，然后，非常重要的一点：他再把更新后的模型传给其他7张卡，这个操作是在每个batch跑完都会执行一次的。<br />
可以想象一下，每个batch，GPU之间都要互相传模型，GPU的通信显然成为了一个极大的瓶颈，直接导致GPU在一段时间是闲着的，也就导致了GPU的利用率低的问题。</p>
<p><strong>缺点</strong>：</p>
<ul>
<li>GPU利用率低</li>
<li>不支持多机多卡</li>
<li>不支持混合精度</li>
</ul>
<br>
<h3 id="torchdistributed"><a class="markdownIt-Anchor" href="#torchdistributed"></a> <code>torch.distributed</code></h3>
<p><strong>工作原理</strong>:<br />
每个进程控制一个GPU，然后每个GPU计算自己分得的数据的梯度后，通过进程之间的通信进行综合所有梯度的操作(all-reduce)，每个GPU独立更新参数(实际上就是同样的东西算了好多遍)，但是这里更新完每个GPU的模型参数是一样的。</p>
<br>
<br>
<p>以下是对<code>nn.DataParallel</code>和<code>nn.DistributedDataParallel</code>的对比表格：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>torch.nn.DataParallel</th>
<th>torch.distributed</th>
</tr>
</thead>
<tbody>
<tr>
<td>设计用途</td>
<td>适用于单机多GPU的场景。</td>
<td>适用于单机多GPU，甚至多机多GPU的场景。</td>
</tr>
<tr>
<td>工作原理</td>
<td>将模型复制到每个GPU上，然后将输入数据的一个batch分割成多个小batch，分别送到不同的GPU上进行计算。每个GPU完成自己的前向和反向计算后，梯度被收集到主GPU上进行汇总，然后进行梯度平均，接着在主GPU上更新模型参数。</td>
<td>采用多进程方式，为每个GPU创建一个进程。模型在所有GPU上复制，每个进程处理输入数据的一部分。所有进程独立完成前向和反向计算，然后通过all-reduce操作汇总梯度信息，确保每个进程的模型参数同步更新。</td>
</tr>
<tr>
<td>优点</td>
<td>实现简单，只需几行代码即可设置。</td>
<td>相比DataParallel，DistributedDataParallel在扩展性和效率上更优，更适合大规模并行训练。它解决了负载不均衡问题，并且通过有效的通信策略减少了通信开销。</td>
</tr>
<tr>
<td>缺点</td>
<td>可能存在负载不均衡问题（通常主GPU负担更重），并且随着GPU数量的增加，由于数据传输和同步的开销，效率可能不是线性提升。</td>
<td>实现相对复杂，需要更多的设置和配置。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="3如何在pytorch中找到函数的导数"><a class="markdownIt-Anchor" href="#3如何在pytorch中找到函数的导数"></a> 3.如何在PyTorch中找到函数的导数?</h2>
<p>生成一个从 -10 到 10，步长为 0.01 的输入值序列。对每个输入值，使用 PyTorch 计算 sin(x) 及其导数,将计算结果存储在列表中。使用 Matplotlib 绘制 sin(x)、sin’(x)（即导数），以及 cos(x) 的图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义范围和步长</span></span><br><span class="line">start, end, step = -<span class="number">10</span>, <span class="number">10</span>, <span class="number">0.01</span></span><br><span class="line">aList = np.arange(start, end, step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化结果列表</span></span><br><span class="line">results = []</span><br><span class="line">gradients = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">function = torch.sin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算函数值和梯度</span></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> aList:</span><br><span class="line">    a = nn.Parameter(torch.tensor(<span class="built_in">float</span>(value)))</span><br><span class="line">    b = function(a)</span><br><span class="line">    results.append(b.item())</span><br><span class="line">    b.backward()</span><br><span class="line">    gradients.append(a.grad.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制函数和梯度图</span></span><br><span class="line">plt.plot(aList, results, label=<span class="string">&#x27;sin(x)&#x27;</span>)</span><br><span class="line">plt.plot(aList, gradients, label=<span class="string">&quot;grad&quot;</span>)</span><br><span class="line">plt.plot(aList, [np.cos(i) <span class="keyword">for</span> i <span class="keyword">in</span> aList], <span class="string">&#x27;-.&#x27;</span>, label=<span class="string">&#x27;cos(x)&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加图例和保存图像</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&#x27;derivative_plot.jpg&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pbs.twimg.com/media/GCF_bUraEAAUS-9?format=jpg&amp;name=small" alt="" /></p>
<p>生成了三条曲线：sin(x)，sin’(x)，以及 cos(x)。sin(x) 的导数曲线（sin’(x)）与 cos(x) 的曲线完美重合，验证了微分的正确性。</p>
<br>
<h2 id="4对比四种求导方法的优劣并分析为什么选择自动求导机制"><a class="markdownIt-Anchor" href="#4对比四种求导方法的优劣并分析为什么选择自动求导机制"></a> 4.对比四种求导方法的优劣并分析为什么选择自动求导机制。</h2>
<table>
<thead>
<tr>
<th>方法</th>
<th>对图像的调用方法</th>
<th>顺序</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>手动分类法</td>
<td>NA</td>
<td>高</td>
<td>实现复杂</td>
</tr>
<tr>
<td>数值求导法</td>
<td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>l</mi></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n_l + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></td>
<td>低</td>
<td>计算量大，速度慢</td>
</tr>
<tr>
<td>符号求导法</td>
<td>NA</td>
<td>高</td>
<td>表达式膨胀</td>
</tr>
<tr>
<td>自动求导法</td>
<td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>o</mi></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n_o + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></td>
<td>高</td>
<td>对输入人维度较大的情况优势明显</td>
</tr>
</tbody>
</table>
<h3 id="手动求解法"><a class="markdownIt-Anchor" href="#手动求解法"></a> <code>手动求解法</code></h3>
<p><strong>即传统的反向传播算法:</strong> 手动用链式法则求解出梯度公式，代入数值，得到最终梯度值</p>
<p>优点：</p>
<ul>
<li>通过手动计算，可以更深入地理解梯度和链式法则的工作原理。</li>
<li>对特定问题可以定制化梯度求解，优化计算效率</li>
</ul>
<p>缺点:</p>
<ul>
<li>对于大规模的深度学习算法，手动用链式法则进行梯度计算并转换成计算机程序非常困难</li>
<li>需要手动编写梯度求解代码</li>
<li>每次修改算法模型，都要修改对应的梯度求解算法</li>
</ul>
<br>
<h3 id="数值求导法"><a class="markdownIt-Anchor" href="#数值求导法"></a> <code>数值求导法</code></h3>
<p><strong>利用导数的原始定义求解</strong>: 𝑓′(𝑥) = lim(h→0) [𝑓(𝑥+h) − 𝑓(𝑥)] / h</p>
<p>优点:</p>
<ul>
<li>易操作</li>
<li>可对用户隐藏求解过程</li>
</ul>
<p>缺点:</p>
<ul>
<li>计算量大，求解速度慢</li>
<li>可能引起舍入误差和截断误差</li>
</ul>
<br>
<h3 id="符号求导法"><a class="markdownIt-Anchor" href="#符号求导法"></a> <code>符号求导法</code></h3>
<p><strong>利用求导规则来对表达式进行自动操作，从而获得导数</strong></p>
<p><strong>常见求导规则:</strong></p>
<ul>
<li>𝑑(𝑓(𝑥)+𝑔(𝑥))/𝑑𝑥 = 𝑑𝑓(𝑥)/𝑑𝑥 + 𝑑𝑔(𝑥)/𝑑𝑥</li>
<li>𝑑(𝑓(𝑥)𝑔(𝑥))/𝑑𝑥 = 𝑑𝑓(𝑥)/𝑑𝑥 𝑔(𝑥) + 𝑓(𝑥) 𝑑𝑔(𝑥)/𝑑𝑥</li>
<li>𝑑𝑓(𝑥)/𝑑𝑥 = 𝑓′(𝑥) 𝑔(𝑥) − 𝑓(𝑥)𝑔′(𝑥) / 𝑔(𝑥)²</li>
</ul>
<p>优点:</p>
<ul>
<li>准确性: 可以得到精确的导数表达式。</li>
<li>自动化: 适合自动化工具，不需要像数值方法那样进行迭代计算。</li>
</ul>
<p>缺点:</p>
<ul>
<li>表达式膨胀问题</li>
</ul>
<br>
<h3 id="自动求导"><a class="markdownIt-Anchor" href="#自动求导"></a> <code>自动求导</code></h3>
<p>计算图结构天然适用于自动求导</p>
<p>计算图将多输入的复杂计算表达成了由多个基本二元计算组成的有向图，并保留了所有中间变量，有助于程序自动利用链式法则进行求导</p>
<p>优点:</p>
<ul>
<li>灵活，可以完全向用户隐藏求导过程</li>
<li>只对基本函数运用符号求导法，因此可以灵活结合编程语言的循环结构、条件结构等</li>
</ul>
<p>缺点：</p>
<ul>
<li>在处理小维度数据时候优势不明显</li>
</ul>
<br>
<h3 id="为什么使用自动求导"><a class="markdownIt-Anchor" href="#为什么使用自动求导"></a> <code>为什么使用自动求导</code></h3>
<p>在深度学习领域，自动求导（Automatic Differentiation，简称Auto-Diff）的应用已成为研究和实践中的核心技术。本综述旨在深入探讨自动求导在训练深度学习模型中的重要性，特别是在处理现代复杂神经网络时的优势和影响。</p>
<p>首先，现代深度神经网络的架构通常非常复杂，涉及大量层次、非线性激活函数和多维度数据处理。这种复杂性使得手动计算梯度（即传统的链式法则）不仅效率低下，而且在实际操作中几乎不可行。自动求导通过计算图的形式，有效地管理和优化这些复杂的梯度计算过程，确保了计算的准确性和效率。</p>
<p>其次，自动求导在计算效率和可扩展性方面表现卓越。随着模型规模的增长，自动求导能够适应性地扩展，有效地处理大型模型中的高维梯度计算。此外，自动求导与现代硬件加速器（如GPU）紧密集成，进一步提升了梯度计算的速度。</p>
<p>在数值稳定性和准确性方面，自动求导避免了传统数值求导方法中常见的截断误差和舍入误差，提供了更加稳定和精确的梯度值。这对于深度学习模型的训练尤为重要，因为准确的梯度直接影响到模型的收敛速度和最终性能。</p>
<p>从用户的角度来看，自动求导极大地降低了深度学习模型开发的门槛。通过抽象化复杂的微分操作，深度学习框架（如TensorFlow和PyTorch）使研究人员和开发人员能够专注于模型架构的设计和优化，而不必深入了解底层的数学运算。这促进了快速原型设计和迭代实验，加速了新模型和算法的开发。</p>
<p>最后，自动求导的灵活性和泛化能力也是其在深度学习领域广泛应用的关键因素。自动求导不仅适用于标准的神经网络结构，还可以灵活地适应自定义的复杂模型和新颖的算法设计。</p>
<p>综上所述，自动求导技术在深度学习模型的训练中发挥着至关重要的作用。它不仅优化了计算过程，提高了效率和精度，还降低了模型开发的复杂性，为深度学习的研究和应用提供了强大的支持。随着深度学习技术的不断进步和发展，自动求导仍将是一个不断演进和优化的关键领域。</p>
<br>
<h2 id="5使用gpu计算时试分析在单机单卡-单机多卡-多机多卡的设备上训练卷积神经网络流程上的区别-其中哪些步骤是可以并行的哪些步骤是必须串行的"><a class="markdownIt-Anchor" href="#5使用gpu计算时试分析在单机单卡-单机多卡-多机多卡的设备上训练卷积神经网络流程上的区别-其中哪些步骤是可以并行的哪些步骤是必须串行的"></a> 5.使用GPU计算时，试分析在单机单卡、单机多卡、多机多卡的设备上训练卷积神经网络流程上的区别。其中哪些步骤是可以并行的，哪些步骤是必须串行的?</h2>
<!-- - [深度学习分布式训练相关介绍 - Part 1 多GPU训练](https://zhuanlan.zhihu.com/p/70312627) -->
<br>
<table>
<thead>
<tr>
<th>分类</th>
<th>单机单卡</th>
<th>单机多卡</th>
<th>多机多卡</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据处理</td>
<td>所有数据在单个GPU上加载和处理。</td>
<td>数据在CPU上加载，然后分布到多个GPU上。</td>
<td>数据在多个机器上分布加载，然后每台机器分布到其GPU上。</td>
</tr>
<tr>
<td>计算流程</td>
<td>前向传播：在单个GPU上计算。反向传播：梯度计算也在同一GPU上完成。权重更新：权重更新同样在单个GPU上进行。</td>
<td>前向传播：每个GPU处理数据的一个子集。反向传播：每个GPU独立计算梯度。权重更新：需要跨GPU合并梯度，然后更新权重。</td>
<td>前向传播：类似单机多卡，但在更多的GPU上进行。反向传播：每个GPU独立计算梯度。权重更新：需要在多个机器间同步合并梯度，然后更新权重。</td>
</tr>
<tr>
<td>资源限制和扩展</td>
<td>由于只有一块GPU，模型的大小和批次大小受到限制。</td>
<td>可以处理更大的模型和更大批次的数据。</td>
<td>进一步扩展了处理能力，适合非常大规模的模型训练。</td>
</tr>
<tr>
<td>并行性</td>
<td>没有并行处理，所有步骤顺序执行。</td>
<td>有一定的并行处理，但梯度合并和权重更新可能需要同步操作。</td>
<td>高度并行，但跨机器的通信和同步可能成为瓶颈。</td>
</tr>
</tbody>
</table>
<br>
<p>单机单卡：基本都是串行的步骤</p>
<p>单机多卡:</p>
<ul>
<li><strong>并行的步骤</strong>：
<ul>
<li><strong>前向传播</strong>：每个GPU处理数据的一个子集，因此可以并行进行。</li>
<li><strong>反向传播</strong>：每个GPU独立计算自己部分数据的梯度，这个过程也是并行的。</li>
</ul>
</li>
<li><strong>串行的步骤</strong>：
<ul>
<li><strong>梯度聚合和权重更新</strong>：虽然各个GPU可以同时计算梯度，但最终这些梯度需要合并（通常在CPU上或一个指定的GPU上进行），然后进行权重更新。这个合并和更新过程是串行的。</li>
</ul>
</li>
</ul>
<p>多机多卡:</p>
<ul>
<li><strong>并行的步骤</strong>：
<ul>
<li><strong>前向传播</strong>：跨多个机器和GPU进行，每个机器的GPU处理数据的一个子集。</li>
<li><strong>反向传播</strong>：每个GPU独立计算梯度，整个过程在多个机器和GPU上并行进行。</li>
</ul>
</li>
<li><strong>串行的步骤</strong>：
<ul>
<li><strong>梯度聚合和权重更新</strong>：需要跨多个机器进行梯度合并和权重更新，这通常涉及复杂的通信和同步机制。尽管各个机器可以同时计算梯度，但最终的聚合和更新步骤是串行的，并且通常是分布式训练中的性能瓶颈。</li>
</ul>
</li>
</ul>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/12/23/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E5%9B%9B%E4%BA%94%E7%AB%A0%E4%BD%9C%E4%B8%9A/" data-id="cls1ihebh00na9869b9aog3qg" data-title="智能计算系统:第四五章作业"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/chapter04/" rel="tag">chapter04</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/chapter05/" rel="tag">chapter05</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" rel="tag">专业知识</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" rel="tag">智能计算系统</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://pbs.twimg.com/media/GBTN7RnW0AA6G9K?format=jpg&amp;name=medium" data-sizes="auto" alt="Pytorch:torch.utils.data.DataLoader"
                         class="lazyload">
                
                <a href="/2023/12/24/Pytorch-torch-utils-data-DataLoader/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        Pytorch:torch.utils.data.DataLoader
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://pbs.twimg.com/media/GBTN7RnW0AA6G9K?format=jpg&amp;name=medium" data-sizes="auto" alt="Mac M2使用GPU进行训练"
                         class="lazyload">
                
                <a href="/2023/12/23/Mac-M2%E4%BD%BF%E7%94%A8GPU%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        Mac M2使用GPU进行训练
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="Jerome" class="lazyload">
            <div class="sidebar-author-name">Jerome</div>
            <div class="sidebar-description">Indeed, I am quite the oddity.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">342</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">35</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">382</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/74X2u8JMVooG2QbjRxXwR8?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>


    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Competition/">Competition</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Future/">Future</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/IELTS/">IELTS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/paper/ItWorks-SNN/">ItWorks-SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/boring-SNN/">boring-SNN</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS224N/">CS224N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/CS231N/">CS231N</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/Missing-Semester-of-CS/">Missing Semester of CS</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Computer-Vision/">Computer Vision</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Natural-Language-Process/">Natural Language Process</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%96%87%E5%8C%96%E8%AE%A1%E7%AE%97/">文化计算</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/">智能信息网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/">语音信息处理</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BF%A1%E6%81%AFGap/">信息Gap</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 12.35px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/17/" style="font-size: 10px;">17</a> <a href="/tags/2/" style="font-size: 12.94px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 11.76px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/4/" style="font-size: 11.18px;">4</a> <a href="/tags/5/" style="font-size: 10.59px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 11.18px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 17.65px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.18px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 14.12px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/BatchNorm/" style="font-size: 10px;">BatchNorm</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 11.76px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10.59px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 11.76px;">CNN</a> <a href="/tags/CS224N/" style="font-size: 10px;">CS224N</a> <a href="/tags/CS231N/" style="font-size: 10px;">CS231N</a> <a href="/tags/CV/" style="font-size: 11.76px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 12.94px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/ComPetition/" style="font-size: 10px;">ComPetition</a> <a href="/tags/Competition/" style="font-size: 10.59px;">Competition</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 15.88px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.59px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Filter/" style="font-size: 10px;">Filter</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/Future/" style="font-size: 12.94px;">Future</a> <a href="/tags/GB/" style="font-size: 10px;">GB</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GiB/" style="font-size: 10px;">GiB</a> <a href="/tags/Git/" style="font-size: 10.59px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 17.06px;">GoAbroad</a> <a href="/tags/Graduate/" style="font-size: 10px;">Graduate</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 12.35px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/ItWorks/" style="font-size: 10px;">ItWorks</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/LayerNorm/" style="font-size: 10px;">LayerNorm</a> <a href="/tags/Lec01/" style="font-size: 11.18px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.59px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.76px;">Linux</a> <a href="/tags/Listening/" style="font-size: 10px;">Listening</a> <a href="/tags/M2/" style="font-size: 10.59px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.35px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 14.12px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.59px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 11.18px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NLP/" style="font-size: 11.18px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 16.47px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OCR/" style="font-size: 10px;">OCR</a> <a href="/tags/OS/" style="font-size: 14.12px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.59px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Reading/" style="font-size: 10px;">Reading</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10.59px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.18px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.35px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SNNNLP/" style="font-size: 10px;">SNNNLP</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Script/" style="font-size: 10px;">Script</a> <a href="/tags/Shell/" style="font-size: 10.59px;">Shell</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-Neural-Network-for-Ultra-low-latency-and-High-accurate-Object-Detection/" style="font-size: 10px;">Spiking Neural Network for Ultra-low-latency and High-accurate Object Detection</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.59px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.59px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/Terminal/" style="font-size: 10px;">Terminal</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/Transformer/" style="font-size: 10px;">Transformer</a> <a href="/tags/Undergraduate/" style="font-size: 10px;">Undergraduate</a> <a href="/tags/University/" style="font-size: 12.94px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 11.18px;">ViT</a> <a href="/tags/Vim/" style="font-size: 10px;">Vim</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.59px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/alexnet/" style="font-size: 10px;">alexnet</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 11.76px;">bert</a> <a href="/tags/blip2/" style="font-size: 10px;">blip2</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/boring/" style="font-size: 11.18px;">boring</a> <a href="/tags/bug/" style="font-size: 16.47px;">bug</a> <a href="/tags/cat/" style="font-size: 10px;">cat</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.18px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.59px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.59px;">chapter05</a> <a href="/tags/chapter6/" style="font-size: 10px;">chapter6</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/chmod/" style="font-size: 10px;">chmod</a> <a href="/tags/chrome/" style="font-size: 10px;">chrome</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/code/" style="font-size: 11.18px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/commit/" style="font-size: 10px;">commit</a> <a href="/tags/competition/" style="font-size: 10px;">competition</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/copilot/" style="font-size: 10.59px;">copilot</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10.59px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.53px;">d2l</a> <a href="/tags/database/" style="font-size: 14.12px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.59px;">deep neural network</a> <a href="/tags/delete/" style="font-size: 10px;">delete</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/dowhy/" style="font-size: 10.59px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10.59px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/embedding/" style="font-size: 10px;">embedding</a> <a href="/tags/explainer/" style="font-size: 10.59px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/github/" style="font-size: 12.35px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 11.18px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10.59px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/import/" style="font-size: 10px;">import</a> <a href="/tags/instructor/" style="font-size: 11.76px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 11.76px;">intern00</a> <a href="/tags/internship/" style="font-size: 18.82px;">internship</a> <a href="/tags/interview/" style="font-size: 10px;">interview</a> <a href="/tags/introduction/" style="font-size: 11.18px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/jmbook/" style="font-size: 10px;">jmbook</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.59px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 11.18px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lr/" style="font-size: 10px;">lr</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 12.35px;">mac</a> <a href="/tags/memory/" style="font-size: 11.76px;">memory</a> <a href="/tags/mentor/" style="font-size: 10.59px;">mentor</a> <a href="/tags/mid/" style="font-size: 10.59px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.59px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.59px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 14.71px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.18px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.41px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 11.76px;">prml</a> <a href="/tags/profile/" style="font-size: 10px;">profile</a> <a href="/tags/project/" style="font-size: 12.35px;">project</a> <a href="/tags/pycharm/" style="font-size: 10px;">pycharm</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/pytorch/" style="font-size: 14.12px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10.59px;">reading</a> <a href="/tags/register/" style="font-size: 10px;">register</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 14.71px;">review</a> <a href="/tags/rf/" style="font-size: 10px;">rf</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.29px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/server/" style="font-size: 10px;">server</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/snn/" style="font-size: 11.18px;">snn</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/sora/" style="font-size: 10px;">sora</a> <a href="/tags/spike/" style="font-size: 10.59px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.59px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikebert/" style="font-size: 10px;">spikebert</a> <a href="/tags/spikingjelly/" style="font-size: 12.35px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.59px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.59px;">ssh</a> <a href="/tags/sta/" style="font-size: 10px;">sta</a> <a href="/tags/terminal/" style="font-size: 10px;">terminal</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.59px;">tips</a> <a href="/tags/tool/" style="font-size: 18.24px;">tool</a> <a href="/tags/transformer/" style="font-size: 12.94px;">transformer</a> <a href="/tags/transformers/" style="font-size: 10px;">transformers</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10.59px;">vscode</a> <a href="/tags/wakatime/" style="font-size: 10px;">wakatime</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">专业知识</a> <a href="/tags/%E4%B8%93%E7%A1%95/" style="font-size: 10px;">专硕</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E4%BB%A3%E7%90%86/" style="font-size: 10px;">代理</a> <a href="/tags/%E4%BF%A1%E6%81%AFGap/" style="font-size: 10px;">信息Gap</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">公选课</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.59px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" style="font-size: 10px;">博弈论</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">图像描述生成</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">大作业</a> <a href="/tags/%E5%A4%A7%E5%88%9B/" style="font-size: 10px;">大创</a> <a href="/tags/%E5%A4%A7%E8%8B%B1%E8%B5%9B/" style="font-size: 10px;">大英赛</a> <a href="/tags/%E5%AD%A6%E7%A1%95/" style="font-size: 10px;">学硕</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.59px;">审稿意见</a> <a href="/tags/%E5%B0%8F%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">小作业</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%82%84%E6%82%84%E8%AF%9D/" style="font-size: 10px;">悄悄话</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%99%E8%82%B2%E8%AE%B8%E5%8F%AF/" style="font-size: 10px;">教育许可</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%96%87%E5%8C%96%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">文化计算</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">显卡</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.59px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">智能信息网络</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 14.12px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.59px;">服务器</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期中复习</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">朴素贝叶斯</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 11.76px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.59px;">李宏毅</a> <a href="/tags/%E6%9D%8E%E6%B2%90/" style="font-size: 10px;">李沐</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">模型训练流程</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 12.94px;">毛概</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.59px;">特征学习</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">第三章</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3/" style="font-size: 10px;">脑机接口</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%8A%82%E8%83%BD%E5%87%8F%E6%8E%92/" style="font-size: 11.18px;">节能减排</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">评测指标</a> <a href="/tags/%E8%AF%AD%E4%B9%89%E7%A9%BA%E9%97%B4/" style="font-size: 10px;">语义空间</a> <a href="/tags/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86/" style="font-size: 10.59px;">语音信息处理</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B/" style="font-size: 10px;">课程</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.18px;">调研</a> <a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/" style="font-size: 10px;">贝叶斯</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">软件概要设计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%87%8F%E5%8C%96/" style="font-size: 10px;">量化</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                Jerome
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    681.8k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    43:13
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="Jerome" class="lazyload">
            <div class="sidebar-author-name">Jerome</div>
            <div class="sidebar-description">Indeed, I am quite the oddity.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">342</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">35</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">382</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

