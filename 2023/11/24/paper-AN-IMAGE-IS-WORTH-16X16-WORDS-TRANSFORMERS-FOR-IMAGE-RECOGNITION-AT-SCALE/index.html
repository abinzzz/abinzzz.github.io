
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjaxÔºöÈò≤Ê≠¢Ë∑≥ËΩ¨È°µÈù¢Èü≥‰πêÊöÇÂÅú-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE |
        
        Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   ËÆ∫ÊñáÈìæÊé• üîóÔºöAN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT üîóÔºöÊùéÊ≤êËÆ∫ÊñáÁ≤æËØªÁ≥ªÂàó‰∫åÔºöVision Transformer„ÄÅM">
<meta property="og:type" content="article">
<meta property="og:title" content="paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE">
<meta property="og:url" content="https://abinzzz.github.io/2023/11/24/paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   ËÆ∫ÊñáÈìæÊé• üîóÔºöAN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT üîóÔºöÊùéÊ≤êËÆ∫ÊñáÁ≤æËØªÁ≥ªÂàó‰∫åÔºöVision Transformer„ÄÅM">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/F_r7X7PW8AAUeKt?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F_r75OFWQAABJqp?format=png&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_sQE6PWcAA-37V?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F_slGHXXAAAQ8zD?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F_tFQ7AW4AAnFh8?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_tFQ7EW0AAzhax?format=jpg&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F_txvGWXAAAetnz?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_txvGTW4AEru5m?format=jpg&amp;name=large">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUkWsAAoYn0?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUnWkAEqdX1?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUqXcAA2aaT?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUmXAAE6U8-?format=jpg&amp;name=small">
<meta property="article:published_time" content="2023-11-24T03:46:20.000Z">
<meta property="article:modified_time" content="2023-11-24T18:35:08.077Z">
<meta property="article:author" content="„ÅÇ„Åæ„ÅÆ„Å≤„Å™">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="vit">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/F_r7X7PW8AAUeKt?format=jpg&amp;name=medium">
    
        <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">È¶ñÈ°µ</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">ÂΩíÊ°£</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">ÂÖ≥‰∫é</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">ÂèãÈìæ</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS ËÆ¢ÈòÖ"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="ÊêúÁ¥¢"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/11/24/paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/" class="article-date-link">
        <time datetime="2023-11-24T03:46:20.000Z"
              itemprop="datePublished">2023-11-24</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/paper/">paper</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>

<h2 id="ËÆ∫ÊñáÈìæÊé•"><a href="#ËÆ∫ÊñáÈìæÊé•" class="headerlink" title="ËÆ∫ÊñáÈìæÊé•"></a>ËÆ∫ÊñáÈìæÊé•</h2><ul>
<li>üîóÔºö<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT</a></li>
<li>üîóÔºö<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/127358168">ÊùéÊ≤êËÆ∫ÊñáÁ≤æËØªÁ≥ªÂàó‰∫åÔºöVision Transformer„ÄÅMAE„ÄÅSwin-Transformer</a></li>
</ul>
<p><br></p>
<h2 id="ËØ¥Âú®ÂâçÈù¢"><a href="#ËØ¥Âú®ÂâçÈù¢" class="headerlink" title="ËØ¥Âú®ÂâçÈù¢"></a>ËØ¥Âú®ÂâçÈù¢</h2><p>ËøôÁØáÊñáÁ´†ÊåëÊàò‰∫ÜËá™‰ªé2012Âπ¥AlexNetÊèêÂá∫‰ª•Êù•Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂú®ËÆ°ÁÆóÊú∫ËßÜËßâÈáåÁªùÂØπÁªüÊ≤ªÁöÑÂú∞‰Ωç„ÄÇÁªìËÆ∫ÊòØÂ¶ÇÊûúÂú®Ë∂≥Â§üÂ§öÁöÑÊï∞ÊçÆ‰∏äÂÅöÈ¢ÑËÆ≠ÁªÉÔºå‰πüÂèØ‰ª•‰∏çÈúÄË¶ÅÂç∑ÁßØÁ•ûÁªèÁΩëË∑ØÔºåÁõ¥Êé•‰ΩøÁî®Ê†áÂáÜÁöÑtransformer‰πüËÉΩÂ§üÊääËßÜËßâÈóÆÈ¢òËß£ÂÜ≥ÁöÑÂæàÂ•Ω„ÄÇÂÆÉÊâìÁ†¥‰∫ÜCVÂíåNLPÂú®Ê®°Âûã‰∏äÁöÑÂ£ÅÂûíÔºåÂºÄÂêØ‰∫ÜCVÁöÑ‰∏Ä‰∏™Êñ∞Êó∂‰ª£ÔºåÊé®Ëøõ‰∫ÜÂ§öÊ®°ÊÄÅÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ</p>
<p><strong>paperswithcode</strong>ÂèØ‰ª•Êü•ËØ¢Áé∞Âú®Êüê‰∏™È¢ÜÂüüÊàñËÄÖËØ¥Êüê‰∏™Êï∞ÊçÆÈõÜË°®Áé∞ÊúÄÂ•ΩÁöÑ‰∏Ä‰∫õÊñπÊ≥ïÊúâÂì™‰∫õ„ÄÇÂõæÂÉèÂàÜÁ±ªÂú®ImageNetÊï∞ÊçÆÈõÜ‰∏äÊéíÂêçÈù†ÂâçÁöÑÂÖ®ÊòØÂü∫‰∫éVision Transformer„ÄÇ</p>
<p>ÂØπ‰∫éÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°Âú®COCOÊï∞ÊçÆÈõÜ‰∏äÔºåÊéíÂêçÈù†ÂâçÈÉΩÈÉΩÊòØÂü∫‰∫éSwin Transformer„ÄÇSwin TransformerÊòØICCV 21ÁöÑÊúÄ‰Ω≥ËÆ∫ÊñáÔºåÂèØ‰ª•ÊääÂÆÉÊÉ≥Ë±°Êàê‰∏Ä‰∏™Â§öÂ∞∫Â∫¶ÁöÑVitÔºàVision TransformerÔºâ„ÄÇ</p>
<p>Âú®ÂÖ∂‰ªñÈ¢ÜÂüüÔºàËØ≠‰πâÂàÜÂâ≤„ÄÅÂÆû‰æãÂàÜÂâ≤„ÄÅËßÜÈ¢ë„ÄÅÂåªÁñó„ÄÅÈÅ•ÊÑüÔºâÔºåÂü∫Êú¨‰∏äÂèØ‰ª•ËØ¥Vision TransformerÂ∞ÜÊï¥‰∏™ËßÜËßâÈ¢ÜÂüü‰∏≠ÊâÄÊúâÁöÑ‰ªªÂä°ÈÉΩÂà∑‰∫Ü‰∏™ÈÅç„ÄÇ</p>
<p><br></p>
<h2 id="1-ÂºïË®Ä"><a href="#1-ÂºïË®Ä" class="headerlink" title="1. ÂºïË®Ä"></a>1. ÂºïË®Ä</h2><h2 id="1-1-Vision-TransformerÁöÑ‰∏Ä‰∫õÊúâË∂£ÁâπÊÄß"><a href="#1-1-Vision-TransformerÁöÑ‰∏Ä‰∫õÊúâË∂£ÁâπÊÄß" class="headerlink" title="1.1 Vision TransformerÁöÑ‰∏Ä‰∫õÊúâË∂£ÁâπÊÄß"></a>1.1 Vision TransformerÁöÑ‰∏Ä‰∫õÊúâË∂£ÁâπÊÄß</h2><p>‰ΩúËÄÖ‰ªãÁªçÁöÑÂè¶‰∏ÄÁØáËÆ∫ÊñáÔºö<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.10497.pdf">„ÄäIntriguing Properties of Vision Transformer„Äã</a></p>
<p><img src="https://pbs.twimg.com/media/F_r7X7PW8AAUeKt?format=jpg&amp;name=medium" alt=""></p>
<ul>
<li>ÂõæaË°®Á§∫ÁöÑÊòØÈÅÆÊå°ÔºåÂú®Ëøô‰πà‰∏•ÈáçÁöÑÈÅÆÊå°ÊÉÖÂÜµ‰∏ãÔºå‰∏çÁÆ°ÊòØÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºå‰∫∫Áúº‰πüÂæàÈöæËßÇÂØüÂá∫Âõæ‰∏≠ÊâÄÁ§∫ÁöÑÊòØ‰∏ÄÂè™È∏ü</li>
<li>ÂõæbË°®Á§∫Êï∞ÊçÆÂàÜÂ∏É‰∏äÊúâÊâÄÂÅèÁßªÔºåËøôÈáåÂØπÂõæÁâáÂÅö‰∫Ü‰∏ÄÊ¨°Á∫πÁêÜÂéªÈô§ÁöÑÊìç‰ΩúÔºåÊâÄ‰ª•ÂõæÁâáÁúãËµ∑Êù•ÊØîËæÉÈ≠îÂπª</li>
<li>ÂõæcË°®Á§∫Âú®È∏üÂ§¥ÁöÑ‰ΩçÁΩÆÂä†‰∫Ü‰∏Ä‰∏™ÂØπÊäóÊÄßÁöÑpatch</li>
<li>ÂõædË°®Á§∫Â∞ÜÂõæÁâáÊâìÊï£‰∫Ü‰πãÂêéÂÅöÊéíÂàóÁªÑÂêà</li>
</ul>
<p>‰∏äËø∞‰æãÂ≠ê‰∏≠ÔºåÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂæàÈöæÂà§Êñ≠Âà∞Â∫ïÊòØ‰∏Ä‰∏™‰ªÄ‰πàÁâ©‰ΩìÔºå‰ΩÜÊòØÂØπ‰∫éÊâÄÊúâÁöÑËøô‰∫õ‰æãÂ≠êVision TransformerÈÉΩËÉΩÂ§üÂ§ÑÁêÜÁöÑÂæàÂ•Ω„ÄÇ</p>
<p><br></p>
<h2 id="1-2-Ê†áÈ¢ò"><a href="#1-2-Ê†áÈ¢ò" class="headerlink" title="1.2 Ê†áÈ¢ò"></a>1.2 Ê†áÈ¢ò</h2><p>‰∏ÄÂº†ÂõæÁâáÁ≠â‰ª∑‰∫éÂæàÂ§ö16<em>16Â§ßÂ∞èÁöÑÂçïËØç„ÄÇ‰∏∫‰ªÄ‰πàÊòØ16</em>16ÁöÑÂçïËØçÔºüÊääÂõæÁâáÂàÜÂâ≤ÊàêÂæàÂ§öÊñπÊ†ºpatchÁöÑÂΩ¢ÂºèÔºåÊØè‰∏Ä‰∏™ÊñπÊ†ºÁöÑÂ§ßÂ∞èÈÉΩÊòØ16<em>16ÔºåÈÇ£‰πàËøôÂº†ÂõæÁâáÂ∞±Áõ∏ÂΩì‰∫éÊòØÂæàÂ§ö16</em>16ÁöÑpatchÁªÑÊàêÁöÑÊï¥‰Ωì</p>
<p><img src="https://pbs.twimg.com/media/F_r75OFWQAABJqp?format=png&amp;name=small" alt=""></p>
<p><br></p>
<h2 id="1-3-ÊëòË¶Å"><a href="#1-3-ÊëòË¶Å" class="headerlink" title="1.3 ÊëòË¶Å"></a>1.3 ÊëòË¶Å</h2><p>Âú®VIT‰πãÂâçÔºåself-attentionÂú®CVÈ¢ÜÂüüÁöÑÂ∫îÁî®ÂæàÊúâÈôêÔºåË¶Å‰πàÂíåÂç∑ÁßØ‰∏ÄËµ∑‰ΩøÁî®ÔºåË¶Å‰πàÂ∞±ÊòØÊääCNNÈáåÈù¢ÁöÑÊüê‰∫õÊ®°ÂùóÊõøÊç¢Êàêself-attentionÔºå‰ΩÜÊòØÊï¥‰ΩìÊû∂ÊûÑ‰∏çÂèò„ÄÇ</p>
<p>ËøôÁØáÊñáÁ´†ËØÅÊòé‰∫ÜÔºåÂú®ÂõæÁâáÂàÜÁ±ª‰ªªÂä°‰∏≠ÔºåÂè™‰ΩøÁî®Á∫ØÁöÑVision TransformerÁªìÊûÑÁõ¥Êé•‰ΩúÁî®‰∫é‰∏ÄÁ≥ªÂàóÂõæÂÉèÂùóÔºå‰πüÂèØ‰ª•ÂèñÁöÑÂæàÂ•ΩÁöÑÊïàÊûúÔºàÊúÄ‰Ω≥Ê®°ÂûãÂú®ImageNet1K‰∏äËÉΩÂ§üËææÂà∞88.55%ÁöÑÂáÜÁ°ÆÁéáÔºâ„ÄÇÂ∞§ÂÖ∂ÊòØÂΩìÂú®Â§ßËßÑÊ®°ÁöÑÊï∞ÊçÆ‰∏äÈù¢ÂÅöÈ¢ÑËÆ≠ÁªÉÁÑ∂ÂêéËøÅÁßªÂà∞‰∏≠Â∞èÂûãÊï∞ÊçÆÈõÜÔºàImageNet„ÄÅCIFAR-100„ÄÅVATBÔºâ‰∏äÈù¢‰ΩøÁî®ÁöÑÊó∂ÂÄôÔºåVision TransformerËÉΩÂ§üËé∑ÂæóË∑üÊúÄÂ•ΩÁöÑÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÁõ∏Â™≤ÁæéÁöÑÁªìÊûú„ÄÇTransformerÁöÑÂè¶Â§ñ‰∏Ä‰∏™Â•ΩÂ§ÑÔºöÂÆÉÂè™ÈúÄË¶ÅÊõ¥Â∞ëÁöÑËÆ≠ÁªÉËµÑÊ∫êÔºåËÄå‰∏îË°®Áé∞ËøòÁâπÂà´Â•Ω„ÄÇ</p>
<p>‰ΩúËÄÖËøôÈáåÊåáÁöÑÂ∞ëÁöÑËÆ≠ÁªÉËµÑÊ∫êÊòØÊåá2500Â§©TPUv3ÁöÑÂ§©Êï∞„ÄÇËøôÈáåÁöÑÂ∞ëÂè™ÊòØË∑üÊõ¥ËÄóÂç°ÁöÑÊ®°ÂûãÂéªÂÅöÂØπÊØî„ÄÇ</p>
<p><br></p>
<h2 id="1-4-ÂºïË®Ä"><a href="#1-4-ÂºïË®Ä" class="headerlink" title="1.4 ÂºïË®Ä"></a>1.4 ÂºïË®Ä</h2><h3 id="TransformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®"><a href="#TransformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®" class="headerlink" title="TransformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®"></a><code>TransformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®</code></h3><p>‚ÄÉ‚ÄÉÂü∫‰∫éself-attentionÁöÑÊ®°ÂûãÊû∂ÊûÑÔºåÁâπÂà´ÊòØTransformerÔºåÂú®NLPÈ¢ÜÂüüÂá†‰πéÊàê‰∫ÜÂøÖÈÄâÊû∂ÊûÑ„ÄÇÁé∞Âú®ÊØîËæÉ‰∏ªÊµÅÁöÑÊñπÂºèÔºåÂ∞±ÊòØÂÖàÂéª‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÊï∞ÊçÆÈõÜ‰∏äÂéªÂÅöÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂÜçÂú®‰∏Ä‰∫õÁâπÂÆöÈ¢ÜÂüüÁöÑÂ∞èÊï∞ÊçÆÈõÜ‰∏äÈù¢ÂÅöÂæÆË∞É„ÄÇÂ§ö‰∫è‰∫ÜTransformerÁöÑÈ´òÊïàÊÄßÂíåÂèØÊâ©Â±ïÊÄßÔºåÁé∞Âú®Â∑≤ÁªèÂèØ‰ª•ËÆ≠ÁªÉË∂ÖËøá1000‰∫øÂèÇÊï∞ÁöÑÂ§ßÊ®°ÂûãÔºàGPT3Ôºâ„ÄÇÈöèÁùÄÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜÁöÑÂ¢ûÈïøÔºåËøòÊ≤°ÊúâÁúãÂà∞ÊÄßËÉΩÈ•±ÂíåÁöÑÁé∞Ë±°„ÄÇ</p>
<ul>
<li>ÂæàÂ§öÊó∂ÂÄô‰∏çÊòØ‰∏ÄÂë≥Âú∞Êâ©Â§ßÊï∞ÊçÆÈõÜÊàñËÄÖËØ¥Êâ©Â§ßÊ®°ÂûãÂ∞±ËÉΩÂ§üËé∑ÂæóÊõ¥Â•ΩÁöÑÊïàÊûúÁöÑÔºåÂ∞§ÂÖ∂ÊòØÂΩìÊâ©Â§ßÊ®°ÂûãÁöÑÊó∂ÂÄôÂæàÂÆπÊòìÁ¢∞Âà∞ËøáÊãüÂêàÁöÑÈóÆÈ¢òÔºå‰ΩÜÊòØÂØπ‰∫étransformerÊù•ËØ¥ÁõÆÂâçËøòÊ≤°ÊúâËßÇÊµãÂà∞Ëøô‰∏™Áì∂È¢à</li>
<li>ÂæÆËΩØÂíåËã±‰ºüËææËÅîÂêàÊé®Âá∫‰∫Ü‰∏Ä‰∏™Ë∂ÖÁ∫ßÂ§ßÁöÑËØ≠Ë®ÄÁîüÊàêÊ®°ÂûãMegatron-TuringÔºåÂÆÉÂ∑≤ÁªèÊúâ5300‰∫øÂèÇÊï∞‰∫ÜÔºåËøòËÉΩÂú®ÂêÑ‰∏™‰ªªÂä°‰∏äÁªßÁª≠Â§ßÂπÖÂ∫¶ÊèêÂçáÊÄßËÉΩÔºåÊ≤°Êúâ‰ªª‰ΩïÊÄßËÉΩÈ•±ÂíåÁöÑÁé∞Ë±°</li>
</ul>
<p><br></p>
<h3 id="Â∞ÜtransformerËøêÁî®Âà∞ËßÜËßâÈ¢ÜÂüüÁöÑÈöæÂ§Ñ"><a href="#Â∞ÜtransformerËøêÁî®Âà∞ËßÜËßâÈ¢ÜÂüüÁöÑÈöæÂ§Ñ" class="headerlink" title="Â∞ÜtransformerËøêÁî®Âà∞ËßÜËßâÈ¢ÜÂüüÁöÑÈöæÂ§Ñ"></a><code>Â∞ÜtransformerËøêÁî®Âà∞ËßÜËßâÈ¢ÜÂüüÁöÑÈöæÂ§Ñ</code></h3><p>TransformerÂú®ÂÅöËá™Ê≥®ÊÑèÂäõÁöÑÊó∂ÂÄôÊòØ‰∏§‰∏§‰∫íÁõ∏ÁöÑÔºåËøô‰∏™ËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÊòØË∑üÂ∫èÂàóÁöÑÈïøÂ∫¶ÂëàÂπ≥ÊñπÂÄçÁöÑ„ÄÇÁõÆÂâç‰∏ÄËà¨Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏≠ÔºåÁ°¨‰ª∂ËÉΩÊîØÊåÅÁöÑÂ∫èÂàóÈïøÂ∫¶‰∏ÄËà¨‰πüÂ∞±ÊòØÂá†ÁôæÊàñËÄÖÊòØ‰∏äÂçÉÔºàÊØîÂ¶ÇËØ¥BERTÁöÑÂ∫èÂàóÈïøÂ∫¶‰πüÂ∞±ÊòØ512Ôºâ„ÄÇ</p>
<p>È¶ñÂÖàË¶ÅËß£ÂÜ≥ÁöÑÊòØÂ¶Ç‰ΩïÊää‰∏Ä‰∏™2DÁöÑÂõæÁâáÂèòÊàê‰∏Ä‰∏™1DÁöÑÂ∫èÂàóÔºàÊàñËÄÖËØ¥ÂèòÊàê‰∏Ä‰∏™ÈõÜÂêàÔºâ„ÄÇÊúÄÁõ¥ËßÇÁöÑÊñπÂºèÂ∞±ÊòØÊääÊØè‰∏™ÂÉèÁ¥†ÁÇπÂΩìÊàêÂÖÉÁ¥†ÔºåÂ∞ÜÂõæÁâáÊãâÁõ¥ÊîæËøõtransformerÈáåÔºåÁúãËµ∑Êù•ÊØîËæÉÁÆÄÂçïÔºå‰ΩÜÊòØÂÆûÁé∞Ëµ∑Êù•Â§çÊùÇÂ∫¶ËæÉÈ´ò„ÄÇ</p>
<p>‰∏ÄËà¨Êù•ËØ¥Âú®ËßÜËßâ‰∏≠ËÆ≠ÁªÉÂàÜÁ±ª‰ªªÂä°ÁöÑÊó∂ÂÄôÂõæÁâáÁöÑËæìÂÖ•Â§ßÂ∞èÂ§ßÊ¶ÇÊòØ224<em>224ÔºåÂ¶ÇÊûúÂ∞ÜÂõæÁâá‰∏≠ÁöÑÊØè‰∏Ä‰∏™ÂÉèÁ¥†ÁÇπÈÉΩÁõ¥Êé•ÂΩìÊàêÂÖÉÁ¥†Êù•ÁúãÂæÖÁöÑËØùÔºåÂ∫èÂàóÈïøÂ∫¶Â∞±ÊòØ224</em>224=50176‰∏™ÂÉèÁ¥†ÁÇπÔºåËøô‰∏™Â§ßÂ∞èÂ∞±Áõ∏ÂΩì‰∫éÊòØBERTÂ∫èÂàóÈïøÂ∫¶ÁöÑ100ÂÄç„ÄÇËøôËøò‰ªÖ‰ªÖÊòØÂàÜÁ±ª‰ªªÂä°ÔºåÂØπ‰∫éÊ£ÄÊµãÂíåÂàÜÂâ≤ÔºåÁé∞Âú®ÂæàÂ§öÊ®°ÂûãÁöÑËæìÂÖ•ÈÉΩÂ∑≤ÁªèÂèòÊàê600<em>600ÊàñËÄÖ800</em>800ÊàñËÄÖÊõ¥Â§ßÔºåËÆ°ÁÆóÂ§çÊùÇÂ∫¶Êõ¥È´òÔºåÊâÄ‰ª•Âú®ËßÜËßâÈ¢ÜÂüüÔºåÂç∑ÁßØÁ•ûÁªèÁΩëÁªúËøòÊòØÂç†‰∏ªÂØºÂú∞‰ΩçÁöÑÔºåÊØîÂ¶ÇAlexNetÊàñËÄÖÊòØResNet„ÄÇ</p>
<p><br></p>
<h3 id="Â∞ÜËá™Ê≥®ÊÑèÂäõÁî®Âà∞Êú∫Âô®ËßÜËßâÁöÑÁõ∏ÂÖ≥Â∑•‰Ωú"><a href="#Â∞ÜËá™Ê≥®ÊÑèÂäõÁî®Âà∞Êú∫Âô®ËßÜËßâÁöÑÁõ∏ÂÖ≥Â∑•‰Ωú" class="headerlink" title="Â∞ÜËá™Ê≥®ÊÑèÂäõÁî®Âà∞Êú∫Âô®ËßÜËßâÁöÑÁõ∏ÂÖ≥Â∑•‰Ωú"></a><code>Â∞ÜËá™Ê≥®ÊÑèÂäõÁî®Âà∞Êú∫Âô®ËßÜËßâÁöÑÁõ∏ÂÖ≥Â∑•‰Ωú</code></h3><p>ÂèóNLPÂêØÂèëÔºåÂæàÂ§öÂ∑•‰ΩúÁ†îÁ©∂Â¶Ç‰ΩïÂ∞ÜËá™Ê≥®ÊÑèÂäõÁî®Âà∞Êú∫Âô®ËßÜËßâ‰∏≠„ÄÇ‰∏Ä‰∫õÂ∑•‰ΩúÊòØËØ¥ÊääÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂíåËá™Ê≥®ÊÑèÂäõÊ∑∑Âà∞‰∏ÄËµ∑Áî®ÔºõÂè¶Â§ñ‰∏Ä‰∫õÂ∑•‰ΩúÂ∞±ÊòØÊï¥‰∏™Â∞ÜÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊç¢ÊéâÔºåÂÖ®ÈÉ®Áî®Ëá™Ê≥®ÊÑèÂäõ„ÄÇËøô‰∫õÊñπÊ≥ïÂÖ∂ÂÆûÈÉΩÊòØÂú®Âπ≤‰∏Ä‰∏™‰∫ãÊÉÖÔºö<strong>Âõ†‰∏∫Â∫èÂàóÈïøÂ∫¶Â§™ÈïøÔºåÊâÄ‰ª•ÂØºËá¥Ê≤°ÊúâÂäûÊ≥ïÂ∞ÜtransformerÁî®Âà∞ËßÜËßâ‰∏≠ÔºåÊâÄ‰ª•Â∞±ÊÉ≥ÂäûÊ≥ïÈôç‰ΩéÂ∫èÂàóÈïøÂ∫¶</strong></p>
<p><strong>Non-local Neural Networks</strong>ÔºàCVRPÔºå2018ÔºâÔºöÂ∞ÜÁΩëÁªú‰∏≠Èó¥Â±ÇËæìÂá∫ÁöÑÁâπÂæÅÂõæ‰Ωú‰∏∫transformerËæìÂÖ•Â∫èÂàóÔºåÈôç‰ΩéÂ∫èÂàóÁöÑÈïøÂ∫¶„ÄÇÊØîÂ¶ÇResNet50Âú®ÊúÄÂêé‰∏Ä‰∏™StageÁöÑÁâπÂæÅÂõæsize=14√ó14ÔºåÊääÂÆÉÊãâÂπ≥ÔºåÂ∫èÂàóÂÖÉÁ¥†Â∞±Âè™Êúâ196‰∫ÜÔºåËøôÂ∞±Âú®‰∏Ä‰∏™ÂèØ‰ª•Êé•ÂèóÁöÑËåÉÂõ¥ÂÜÖ‰∫Ü„ÄÇ</p>
<p><strong>„ÄäStand-Alone &amp; Self-Attention in Vision Models„Äã</strong>ÔºàNeurIPSÔºå2019ÔºâÔºö‰ΩøÁî®Â≠§Á´ãÊ≥®ÊÑèÂäõStand-AloneÂíå Axial-AttentionÊù•Â§ÑÁêÜ„ÄÇÂÖ∑‰ΩìÁöÑËØ¥Ôºå‰∏çÊòØËæìÂÖ•Êï¥Âº†ÂõæÔºåËÄåÊòØÂú®‰∏Ä‰∏™local windowÔºàÂ±ÄÈÉ®ÁöÑÂ∞èÁ™óÂè£Ôºâ‰∏≠ËÆ°ÁÆóattention„ÄÇÁ™óÂè£ÁöÑÂ§ßÂ∞èÂèØ‰ª•ÊéßÂà∂ÔºåÂ§çÊùÇÂ∫¶‰πüÂ∞±Â§ßÂ§ßÈôç‰Ωé„ÄÇÔºàÁ±ª‰ººÂç∑ÁßØÁöÑÊìç‰ΩúÔºâ</p>
<p><strong>„ÄäAxial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation„Äã</strong>ÔºàECCVÔºå2020aÔºâÔºö</p>
<ul>
<li><strong>Â≠§Á´ãËá™Ê≥®ÊÑèÂäõ</strong>Ôºö‰∏ç‰ΩøÁî®Êï¥Âº†ÂõæÔºåÂ∞±Áî®‰∏Ä‰∏™local windowÔºàÂ±ÄÈÉ®ÁöÑÂ∞èÁ™óÂè£ÔºâÔºåÈÄöËøáÊéßÂà∂Ëøô‰∏™Á™óÂè£ÁöÑÂ§ßÂ∞èÔºåÊù•ËÆ©ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Âú®ÂèØÊé•ÂèóÁöÑËåÉÂõ¥‰πãÂÜÖ„ÄÇËøôÂ∞±Á±ª‰ºº‰∫éÂç∑ÁßØÊìç‰ΩúÔºàÂç∑ÁßØ‰πüÊòØÂú®‰∏Ä‰∏™Â±ÄÈÉ®ÁöÑÁ™óÂè£‰∏≠Êìç‰ΩúÁöÑÔºâ</li>
<li><strong>ËΩ¥Ëá™Ê≥®ÊÑèÂäõ</strong>Ôºö‰πãÊâÄ‰ª•ËßÜËßâËÆ°ÁÆóÁöÑÂ§çÊùÇÂ∫¶È´òÊòØÂõ†‰∏∫Â∫èÂàóÈïøÂ∫¶N=H*WÔºåÊòØ‰∏Ä‰∏™2DÁöÑÁü©ÈòµÔºåÂ∞ÜÂõæÁâáÁöÑËøô‰∏™2DÁöÑÁü©ÈòµÊÉ≥ÂäûÊ≥ïÊãÜÊàê2‰∏™1DÁöÑÂêëÈáèÔºåÊâÄ‰ª•ÂÖàÂú®È´òÂ∫¶ÁöÑÁª¥Â∫¶‰∏äÂÅö‰∏ÄÊ¨°self-attentionÔºàËá™Ê≥®ÊÑèÂäõÔºâÔºåÁÑ∂ÂêéÂÜçÂú®ÂÆΩÂ∫¶ÁöÑÁª¥Â∫¶‰∏äÂÜçÂéªÂÅö‰∏ÄÊ¨°Ëá™Ê≥®ÊÑèÂäõÔºåÁõ∏ÂΩì‰∫éÊää‰∏Ä‰∏™Âú®2DÁü©Èòµ‰∏äËøõË°åÁöÑËá™Ê≥®ÊÑèÂäõÊìç‰ΩúÂèòÊàê‰∫Ü‰∏§‰∏™1DÁöÑÈ°∫Â∫èÁöÑÊìç‰ΩúÔºåËøôÊ†∑Â§ßÂπÖÂ∫¶Èôç‰Ωé‰∫ÜËÆ°ÁÆóÁöÑÂ§çÊùÇÂ∫¶  </li>
</ul>
<p><br></p>
<p>Ëøô‰∫õÊ®°ÂûãËôΩÁÑ∂ÁêÜËÆ∫‰∏äÊòØÈùûÂ∏∏È´òÊïàÁöÑÔºå‰ΩÜ‰∫ãÂÆû‰∏äËøô‰∏™Ëá™Ê≥®ÊÑèÂäõÊìç‰ΩúÈÉΩÊòØ‰∏Ä‰∫õÊØîËæÉ<strong>ÁâπÊÆäÁöÑËá™Ê≥®ÊÑèÂäõÊìç‰ΩúÔºåÊó†Ê≥ïÂú®Áé∞Âú®ÁöÑÁ°¨‰ª∂‰∏äËøõË°åÂä†ÈÄü</strong>ÔºåÊâÄ‰ª•Â∞±ÂØºËá¥ÂæàÈöæËÆ≠ÁªÉÂá∫‰∏Ä‰∏™Â§ßÊ®°Âûã„ÄÇ<strong>Âõ†Ê≠§Âú®Â§ßËßÑÊ®°ÁöÑÂõæÂÉèËØÜÂà´‰∏äÔºå‰º†ÁªüÁöÑÊÆãÂ∑ÆÁΩëÁªúËøòÊòØÊïàÊûúÊúÄÂ•ΩÁöÑ„ÄÇ</strong></p>
<p>ÊâÄ‰ª•ÔºåËá™Ê≥®ÊÑèÂäõÊó©Â∑≤ÁªèÂú®ËÆ°ÁÆóÊú∫ËßÜËßâÈáåÊúâÊâÄÂ∫îÁî®ÔºåËÄå‰∏îÂ∑≤ÁªèÊúâÂÆåÂÖ®Áî®Ëá™Ê≥®ÊÑèÂäõÂéªÂèñ‰ª£Âç∑ÁßØÊìç‰ΩúÁöÑÂ∑•‰Ωú‰∫Ü„ÄÇÊú¨ÊñáÊòØË¢´transformerÂú®NLPÈ¢ÜÂüüÁöÑÂèØÊâ©Â±ïÊÄßÊâÄÂêØÂèëÔºåÁõ¥Êé•Â∫îÁî®‰∏Ä‰∏™Ê†áÂáÜÁöÑtransformer‰ΩúÁî®‰∫éÂõæÁâáÔºåÂ∞ΩÈáèÂÅöÂ∞ëÁöÑ‰øÆÊîπ„ÄÇ</p>
<p><br></p>
<h3 id="vision-transformerÂ¶Ç‰ΩïËß£ÂÜ≥Â∫èÂàóÈïøÂ∫¶ÁöÑÈóÆÈ¢ò"><a href="#vision-transformerÂ¶Ç‰ΩïËß£ÂÜ≥Â∫èÂàóÈïøÂ∫¶ÁöÑÈóÆÈ¢ò" class="headerlink" title="vision transformerÂ¶Ç‰ΩïËß£ÂÜ≥Â∫èÂàóÈïøÂ∫¶ÁöÑÈóÆÈ¢ò"></a><code>vision transformerÂ¶Ç‰ΩïËß£ÂÜ≥Â∫èÂàóÈïøÂ∫¶ÁöÑÈóÆÈ¢ò</code></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>vision transformerÂ∞Ü‰∏ÄÂº†ÂõæÁâáÊâìÊàê‰∫ÜÂæàÂ§öÁöÑpatch</strong>ÔºåÊØè‰∏Ä‰∏™patchÊòØ16*16</li>
<li>ÂÅáÂ¶ÇÂõæÁâáÁöÑÂ§ßÂ∞èÊòØ224<em>224ÔºåÂàôsequence lenthÔºàÂ∫èÂàóÈïøÂ∫¶ÔºâÂ∞±ÊòØN=224</em>224=<strong>50176</strong>ÔºåÂ¶ÇÊûúÊç¢ÊàêpatchÔºå‰∏Ä‰∏™patchÁõ∏ÂΩì‰∫é‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑËØùÔºåÊúâÊïàÁöÑÈïøÂÆΩÂ∞±ÂèòÊàê‰∫Ü224/16=14ÔºåÊâÄ‰ª•ÊúÄÂêéÁöÑÂ∫èÂàóÈïøÂ∫¶Â∞±ÂèòÊàê‰∫ÜN=14<em>14=<em>*196</em></em>ÔºåÂØπ‰∫éÊôÆÈÄöÁöÑtransformerÊù•ËØ¥ÊòØÂèØ‰ª•Êé•ÂèóÁöÑ</li>
<li>Â∞ÜÊØè‰∏Ä‰∏™patchÂΩì‰Ωú‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÈÄöËøá‰∏Ä‰∏™ÂÖ®ËøûÊé•Â±ÇÂ∞±‰ºöÂæóÂà∞‰∏Ä‰∏™linear embeddingÔºåËøô‰∫õÂ∞±‰ºöÂΩì‰ΩúËæìÂÖ•‰º†Áªôtransformer„ÄÇËøôÊó∂ÂÄô‰∏ÄÂº†ÂõæÁâáÂ∞±ÂèòÊàê‰∫Ü‰∏Ä‰∏™‰∏Ä‰∏™ÁöÑÂõæÁâáÂùó‰∫ÜÔºåÂèØ‰ª•Â∞ÜËøô‰∫õÂõæÁâáÂùóÂΩìÊàêÊòØNLP‰∏≠ÁöÑÂçïËØçÔºå‰∏Ä‰∏™Âè•Â≠ê‰∏≠ÊúâÂ§öÂ∞ëÂçïËØçÂ∞±Áõ∏ÂΩì‰∫éÊòØ‰∏ÄÂº†ÂõæÁâá‰∏≠ÊúâÂ§öÂ∞ë‰∏™patchÔºåËøôÂ∞±ÊòØÈ¢òÁõÆ‰∏≠ÊâÄÊèêÂà∞ÁöÑ‰∏ÄÂº†ÂõæÁâáÁ≠â‰ª∑‰∫éÂæàÂ§ö16*16ÁöÑÂçïËØç</li>
</ul>
<p><br></p>
<h3 id="ÊúâÁõëÁù£ÁöÑËÆ≠ÁªÉ"><a href="#ÊúâÁõëÁù£ÁöÑËÆ≠ÁªÉ" class="headerlink" title="ÊúâÁõëÁù£ÁöÑËÆ≠ÁªÉ"></a><code>ÊúâÁõëÁù£ÁöÑËÆ≠ÁªÉ</code></h3><p>Êú¨ÊñáËÆ≠ÁªÉvision transformer‰ΩøÁî®ÁöÑÊòØÊúâÁõëÁù£ÁöÑËÆ≠ÁªÉ„ÄÇ‰∏∫‰ªÄ‰πàË¶ÅÁ™ÅÂá∫ÊúâÁõëÁù£ÔºüÂõ†‰∏∫ÂØπ‰∫éNLPÊù•ËØ¥ÔºåtransformerÂü∫Êú¨‰∏äÈÉΩÊòØÁî®Êó†ÁõëÁù£ÁöÑÊñπÂºèËÆ≠ÁªÉÁöÑÔºåË¶Å‰πàÊòØÁî®language modelingÔºåË¶Å‰πàÊòØÁî®mask language modelingÔºåÈÉΩÊòØÁî®ÁöÑÊó†ÁõëÁù£ÁöÑËÆ≠ÁªÉÊñπÂºè„ÄÇ‰ΩÜÊòØÂØπ‰∫é<strong>ËßÜËßâ</strong>Êù•ËØ¥ÔºåÂ§ßÈÉ®ÂàÜÁöÑ<strong>Âü∫Á∫øÔºàbaselineÔºâÁΩëÁªúËøòÈÉΩÊòØÁî®ÁöÑÊúâÁõëÁù£ÁöÑËÆ≠ÁªÉÊñπÂºèÂéªËÆ≠ÁªÉÁöÑ</strong>.</p>
<p><br></p>
<h3 id="Ââç‰∫∫ÊúÄÁõ∏ÂÖ≥ÁöÑÂ∑•‰Ωú"><a href="#Ââç‰∫∫ÊúÄÁõ∏ÂÖ≥ÁöÑÂ∑•‰Ωú" class="headerlink" title="Ââç‰∫∫ÊúÄÁõ∏ÂÖ≥ÁöÑÂ∑•‰Ωú"></a><code>Ââç‰∫∫ÊúÄÁõ∏ÂÖ≥ÁöÑÂ∑•‰Ωú</code></h3><p>Êú¨ÊñáÊääËßÜËßâÂΩìÊàêËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑ‰ªªÂä°ÂéªÂÅöÁöÑÔºåÂ∞§ÂÖ∂ÊòØ‰∏≠Èó¥ÁöÑÊ®°ÂûãÂ∞±ÊòØ‰ΩøÁî®ÁöÑtransformer encoderÔºåË∑üBERTÂÆåÂÖ®‰∏ÄÊ†∑„ÄÇËøô‰πàÁÆÄÂçïÁöÑÊÉ≥Ê≥ïÔºå‰πãÂâçÂÖ∂ÂÆû‰πüÊúâ‰∫∫ÊÉ≥Âà∞ËøáÂéªÂÅöÔºåË∑üÊú¨ÊñáÁöÑÂ∑•‰ΩúÊúÄÂÉèÁöÑÊòØ‰∏ÄÁØáICLR 2020ÁöÑpaper</p>
<ul>
<li>ËøôÁØáËÆ∫ÊñáÊòØ‰ªéËæìÂÖ•ÂõæÁâá‰∏≠ÊäΩÂèñ2*2ÁöÑÂõæÁâápatch</li>
<li>‰∏∫‰ªÄ‰πàÊòØ2<em>2ÔºüÂõ†‰∏∫ËøôÁØáËÆ∫ÊñáÁöÑ‰ΩúËÄÖÂè™Âú®CIFAR-10Êï∞ÊçÆÈõÜ‰∏äÂÅö‰∫ÜÂÆûÈ™åÔºåËÄåCIFAR-10Ëøô‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂõæÁâáÈÉΩÊòØ32</em>32ÁöÑÔºåÊâÄ‰ª•Âè™ÈúÄË¶ÅÊäΩÂèñ2<em>2ÁöÑpatchÂ∞±Ë∂≥Â§ü‰∫ÜÔºå16</em>16ÁöÑpatchÂ§™Â§ß‰∫Ü</li>
<li>Âú®ÊäΩÂèñÂ•Ωpatch‰πãÂêéÔºåÂ∞±Âú®‰∏äÈù¢ÂÅöself-attention</li>
</ul>
<p>‰ªéÊäÄÊúØ‰∏äËÄåË®ÄËøôÂ∞±ÊòØVision TransformerÔºå‰ΩÜÊòØÊú¨ÊñáÁöÑ‰ΩúËÄÖËÆ§‰∏∫‰∫åËÄÖÁöÑÂå∫Âà´Âú®‰∫éÔºåÊú¨ÊñáÁöÑÂ∑•‰ΩúËØÅÊòé‰∫ÜÂ¶ÇÊûúÂú®Â§ßËßÑÊ®°ÁöÑÊï∞ÊçÆÈõÜ‰∏äÂÅöÈ¢ÑËÆ≠ÁªÉÁöÑËØùÔºåÈÇ£‰πàÂ∞±ËÉΩËÆ©‰∏Ä‰∏™Ê†áÂáÜÁöÑTransformerÔºå‰∏çÁî®Âú®ËßÜËßâ‰∏äÂÅö‰ªª‰ΩïÁöÑÊõ¥ÊîπÊàñËÄÖÁâπÊÆäÁöÑÊîπÂä®ÔºåÂèñÂæóÊØîÁé∞Âú®ÊúÄÂ•ΩÁöÑÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂ∑Æ‰∏çÂ§öÊàñËÄÖËøòÂ•ΩÁöÑÁªìÊûú„ÄÇ<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs.</span><br></pre></td></tr></table></figure><br>ËøôÁØáÊñáÁ´†ÁöÑ‰∏ªË¶ÅÁõÆÁöÑÂ∞±ÊòØËØ¥ÔºåTransformerÂú®VisionÈ¢ÜÂüüËÉΩÂ§üÊâ©Â±ïÁöÑÊúâÂ§öÂ•ΩÔºåÂ∞±ÊòØÂú®Ë∂ÖÁ∫ßÂ§ßÊï∞ÊçÆÈõÜÂíåË∂ÖÁ∫ßÂ§ßÊ®°Âûã‰∏§ÊñπÁöÑÂä†ÊåÅ‰∏ãÔºåtransformer‰πüËÉΩÂú®ËßÜËßâ‰∏≠Ëµ∑Âà∞ÂæàÂ•ΩÁöÑÊïàÊûú</p>
<p><br></p>
<h3 id="ViTÂíåCNNÁΩëÁªú‰ΩøÁî®ÊïàÊûúÁöÑÊØîËæÉ"><a href="#ViTÂíåCNNÁΩëÁªú‰ΩøÁî®ÊïàÊûúÁöÑÊØîËæÉ" class="headerlink" title="ViTÂíåCNNÁΩëÁªú‰ΩøÁî®ÊïàÊûúÁöÑÊØîËæÉ"></a><code>ViTÂíåCNNÁΩëÁªú‰ΩøÁî®ÊïàÊûúÁöÑÊØîËæÉ</code></h3><p>Âú®‰∏≠ÂûãÂ§ßÂ∞èÁöÑÊï∞ÊçÆÈõÜ‰∏äÔºàÊØîÂ¶ÇËØ¥ImageNetÔºâ‰∏äËÆ≠ÁªÉÁöÑÊó∂ÂÄôÔºåÂ¶ÇÊûú‰∏çÂä†ÊØîËæÉÂº∫ÁöÑÁ∫¶ÊùüÔºåViTÁöÑÊ®°ÂûãÂÖ∂ÂÆûË∑üÂêåÁ≠âÂ§ßÂ∞èÁöÑÊÆãÂ∑ÆÁΩëÁªúÁõ∏ÊØîË¶ÅÂº±‰∏ÄÁÇπ„ÄÇ</p>
<p>‰ΩúËÄÖÂØπÊ≠§ÁöÑËß£ÈáäÊòØÔºö<strong>transformerË∑üCNNÁõ∏ÊØîÔºåÁº∫Â∞ë‰∫Ü‰∏Ä‰∫õCNNÊâÄÂ∏¶ÊúâÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÔºàinductive biasÔºåÊòØÊåá‰∏ÄÁßçÂÖàÈ™åÁü•ËØÜÊàñËÄÖËØ¥ÊòØ‰∏ÄÁßçÊèêÂâçÂÅöÂ•ΩÁöÑÂÅáËÆæÔºâ</strong>„ÄÇ</p>
<p>CNNÁöÑÂΩíÁ∫≥ÂÅèÁΩÆ‰∏ÄËà¨Êù•ËØ¥Êúâ‰∏§ÁßçÔºö</p>
<ul>
<li><strong>locality</strong>ÔºöCNNÊòØ‰ª•ÊªëÂä®Á™óÂè£ÁöÑÂΩ¢Âºè‰∏ÄÁÇπ‰∏ÄÁÇπÂú∞Âú®ÂõæÁâá‰∏äËøõË°åÂç∑ÁßØÁöÑÔºåÊâÄ‰ª•ÂÅáËÆæÂõæÁâá‰∏äÁõ∏ÈÇªÁöÑÂå∫Âüü‰ºöÊúâÁõ∏ÈÇªÁöÑÁâπÂæÅÔºåÈù†ÂæóË∂äËøëÁöÑ‰∏úË•øÁõ∏ÂÖ≥ÊÄßË∂äÂº∫Ôºõ</li>
<li><strong>translation equivarianceÔºàÂπ≥ÁßªÁ≠âÂèòÊÄßÔºâ</strong>ÔºöÂÜôÊàêÂÖ¨ÂºèÂ∞±ÊòØf(g(x))=g(f(x))Ôºå‰∏çËÆ∫ÊòØÂÖàÂÅö g Ëøô‰∏™ÂáΩÊï∞ÔºåËøòÊòØÂÖàÂÅö f Ëøô‰∏™ÂáΩÊï∞ÔºåÊúÄÂêéÁöÑÁªìÊûúÊòØ‰∏çÂèòÁöÑÔºõÂÖ∂‰∏≠f‰ª£Ë°®Âç∑ÁßØÊìç‰ΩúÔºåg‰ª£Ë°®Âπ≥ÁßªÊìç‰Ωú„ÄÇÂõ†‰∏∫Âú®Âç∑ÁßØÁ•ûÁªèÁΩëÁªú‰∏≠ÔºåÂç∑ÁßØÊ†∏Â∞±Áõ∏ÂΩì‰∫éÊòØ‰∏Ä‰∏™Ê®°ÊùøÔºå‰∏çËÆ∫ÂõæÁâá‰∏≠ÂêåÊ†∑ÁöÑÁâ©‰ΩìÁßªÂä®Âà∞Âì™ÈáåÔºåÂè™Ë¶ÅÊòØÂêåÊ†∑ÁöÑËæìÂÖ•ËøõÊù•ÔºåÁÑ∂ÂêéÈÅáÂà∞ÂêåÊ†∑ÁöÑÂç∑ÁßØÊ†∏ÔºåÈÇ£‰πàËæìÂá∫Ê∞∏ËøúÊòØ‰∏ÄÊ†∑ÁöÑ</li>
</ul>
<p><br></p>
<p><strong>‰∏ÄÊó¶Á•ûÁªèÁΩëÁªúÊúâ‰∫ÜËøô‰∏§‰∏™ÂΩíÁ∫≥ÂÅèÁΩÆ‰πãÂêéÔºå‰ªñÂ∞±Êã•Êúâ‰∫ÜÂæàÂ§öÁöÑÂÖàÈ™å‰ø°ÊÅØÔºåÊâÄ‰ª•Âè™ÈúÄË¶ÅÁõ∏ÂØπËæÉÂ∞ëÁöÑÊï∞ÊçÆÂ∞±ÂèØ‰ª•Â≠¶‰π†‰∏Ä‰∏™Áõ∏ÂØπÊØîËæÉÂ•ΩÁöÑÊ®°Âûã„ÄÇ‰ΩÜÊòØÂØπ‰∫étransformerÊù•ËØ¥ÔºåÂÆÉÊ≤°ÊúâËøô‰∫õÂÖàÈ™å‰ø°ÊÅØÔºåÊâÄ‰ª•ÂÆÉÂØπËßÜËßâÁöÑÊÑüÁü•ÂÖ®ÈÉ®ÈúÄË¶Å‰ªéËøô‰∫õÊï∞ÊçÆ‰∏≠Ëá™Â∑±Â≠¶‰π†</strong>„ÄÇ</p>
<p><br></p>
<p>‰∏∫‰∫ÜÈ™åËØÅËøô‰∏™ÂÅáËÆæÔºå ‰ΩúËÄÖÂú®Êõ¥Â§ßÁöÑÊï∞ÊçÆÈõÜÔºàImageNet 22kÊï∞ÊçÆÈõÜÔºå 14M‰∏™Ê†∑Êú¨&amp;JFT 300MÊï∞ÊçÆÈõÜÔºå 300M‰∏™Ê†∑Êú¨Ôºâ‰∏äÂÅö‰∫ÜÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂Âêé<strong>ÂèëÁé∞Âú®ÊúâË∂≥Â§üÁöÑÊï∞ÊçÆÂÅöÈ¢ÑËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåVitËÉΩÂ§üËé∑ÂæóË∑üÁé∞Âú®ÊúÄÂ•ΩÁöÑÊÆãÂ∑ÆÁ•ûÁªèÁΩëÁªúÁõ∏ËøëÊàñËÄÖËØ¥Êõ¥Â•ΩÁöÑÁªìÊûú</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks</span><br></pre></td></tr></table></figure><br>‰∏äÈù¢VTAB‰πüÊòØ‰ΩúËÄÖÂõ¢ÈòüÊâÄÊèêÂá∫Êù•ÁöÑ‰∏Ä‰∏™Êï∞ÊçÆÈõÜÔºåËûçÂêà‰∫Ü19‰∏™Êï∞ÊçÆÈõÜÔºå‰∏ªË¶ÅÊòØÁî®Êù•Ê£ÄÊµãÊ®°ÂûãÁöÑÁ®≥ÂÅ•ÊÄßÔºå‰ªé‰æßÈù¢‰πüÂèçÊò†Âá∫‰∫ÜVisionTransformerÁöÑÁ®≥ÂÅ•ÊÄß‰πüÊòØÁõ∏ÂΩì‰∏çÈîôÁöÑ„ÄÇ</p>
<p><br></p>
<h2 id="2-ÁªìËÆ∫"><a href="#2-ÁªìËÆ∫" class="headerlink" title="2.ÁªìËÆ∫"></a>2.ÁªìËÆ∫</h2><p>ËøôÁØáËÆ∫ÊñáÁöÑÂ∑•‰ΩúÊòØÁõ¥Êé•ÊãøNLPÈ¢ÜÂüü‰∏≠Ê†áÂáÜÁöÑTransformerÊù•ÂÅöËÆ°ÁÆóÊú∫ËßÜËßâÁöÑÈóÆÈ¢òÔºåË∑ü‰πãÂâçÁî®Ëá™Ê≥®ÊÑèÂäõÁöÑÈÇ£‰∫õÂ∑•‰ΩúÁöÑÂå∫Âà´Âú®‰∫éÔºåÈô§‰∫ÜÂú®ÂàöÂºÄÂßãÊäΩÂõæÂÉèÂùóÁöÑÊó∂ÂÄôÔºåËøòÊúâ‰ΩçÁΩÆÁºñÁ†ÅÁî®‰∫Ü‰∏Ä‰∫õÂõæÂÉèÁâπÊúâÁöÑÂΩíÁ∫≥ÂÅèÁΩÆÔºåÈô§Ê≠§‰πãÂ§ñÂ∞±ÂÜç‰πüÊ≤°ÊúâÂºïÂÖ•‰ªª‰ΩïÂõæÂÉèÁâπÊúâÁöÑÂΩíÁ∫≥ÂÅèÁΩÆ‰∫Ü„ÄÇËøôÊ†∑ÁöÑÂ•ΩÂ§ÑÂ∞±ÊòØÂèØ‰ª•Áõ¥Êé•ÊääÂõæÁâáÂΩìÂÅöNLP‰∏≠ÁöÑtokenÔºåÊãøNLP‰∏≠‰∏Ä‰∏™Ê†áÂáÜÁöÑTransformerÂ∞±ÂèØ‰ª•ÂÅöÂõæÂÉèÂàÜÁ±ª‰∫Ü„ÄÇ</p>
<p>ÂΩìËøô‰∏™ÁÆÄÂçïËÄå‰∏îÊâ©Â±ïÊÄßÂæàÂ•ΩÁöÑÁ≠ñÁï•ÂíåÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÁªìÂêàËµ∑Êù•ÁöÑÊó∂ÂÄôÊïàÊûúÂá∫Â•áÁöÑÂ•ΩÔºöVision TransformerÂú®ÂæàÂ§öÂõæÂÉèÂàÜÁ±ªÁöÑbenchmark‰∏äË∂ÖËøá‰∫Ü‰πãÂâçÊúÄÂ•ΩÁöÑÊñπÊ≥ïÔºåËÄå‰∏îËÆ≠ÁªÉËµ∑Êù•ËøòÁõ∏ÂØπ‰æøÂÆú</p>
<p><br></p>
<p>‰ΩúËÄÖÂØπÊú™Êù•ÁöÑÂ±ïÊúõÔºö</p>
<ul>
<li><strong>Vit‰∏çÂè™ÂÅöÂàÜÁ±ªÔºåËøòÊúâÊ£ÄÊµãÂíåÂàÜÂâ≤</strong><ul>
<li><strong>DETR</strong>Ôºö<strong>ÁõÆÊ†áÊ£ÄÊµã</strong>ÁöÑ‰∏Ä‰∏™Âäõ‰ΩúÔºåÁõ∏ÂΩì‰∫éÊòØÊîπÂèò‰∫ÜÊï¥‰∏™ÁõÆÊ†áÊ£ÄÊµã‰πãÂâçÁöÑÊ°ÜÊû∂</li>
<li>Âú®VitÂá∫Áé∞Áü≠Áü≠ÁöÑ‰∏Ä‰∏™ÂçäÊúà‰πãÂêéÔºå2020Âπ¥12ÊúàÂá∫Êù•‰∫Ü‰∏Ä‰∏™Âè´<strong>Vit-FRCNN</strong>ÁöÑÂ∑•‰ΩúÔºåÂ∞ÜVitÁî®Âà∞<strong>Ê£ÄÊµã</strong>‰∏äÈù¢‰∫Ü</li>
<li>2020Âπ¥12ÊúàÊúâ‰∏ÄÁØáSETRÁöÑpaperÂ∞ÜVitÁî®Âà∞<strong>ÂàÜÂâ≤</strong>Èáå‰∫Ü</li>
<li>3‰∏™Êúà‰πãÂêé<strong>Swin Transformer</strong>Ê®™Á©∫Âá∫‰∏ñÔºåÂÆÉÂ∞Ü<strong>Â§öÂ∞∫Â∫¶ÁöÑËÆæËÆ°</strong>ËûçÂêàÂà∞‰∫ÜTransformer‰∏≠ÔºåÊõ¥Âä†ÈÄÇÂêàÂÅöËßÜËßâÁöÑÈóÆÈ¢òÔºåÁúüÊ≠£ËØÅÊòé‰∫ÜTransformerÊòØËÉΩÂ§üÂΩìÊàê‰∏Ä‰∏™ËßÜËßâÈ¢ÜÂüüÁöÑÈÄöÁî®È™®Âπ≤ÁΩëÁªú</li>
</ul>
</li>
<li><strong>Êé¢Á¥¢‰∏Ä‰∏ãËá™ÁõëÁù£ÁöÑÈ¢ÑËÆ≠ÁªÉÊñπÊ°à</strong>ÔºöÂõ†‰∏∫Âú®NLPÈ¢ÜÂüüÔºåÊâÄÊúâÂ§ßÁöÑtransformerÂÖ®ÈÉΩÊòØÁî®Ëá™ÁõëÁù£ÁöÑÊñπÂºèËÆ≠ÁªÉÁöÑÔºåVitËøôÁØápaper‰πüÂÅö‰∫Ü‰∏Ä‰∫õÂàùÂßãÂÆûÈ™åÔºåËØÅÊòé‰∫ÜÁî®ËøôÁßçËá™ÁõëÁù£ÁöÑËÆ≠ÁªÉÊñπÂºè‰πüÊòØÂèØË°åÁöÑÔºå‰ΩÜÊòØË∑üÊúâÁõëÁù£ÁöÑËÆ≠ÁªÉÊØîËµ∑Êù•ËøòÊòØÊúâ‰∏çÂ∞èÁöÑÂ∑ÆË∑ùÁöÑ</li>
<li><strong>Â∞ÜVision TransformerÂèòÂæóÊõ¥Â§ßÔºåÊúâÂèØËÉΩ‰ºöÂ∏¶Êù•Êõ¥Â•ΩÁöÑÁªìÊûú</strong>ÔºöËøá‰∫ÜÂçäÂπ¥ÔºåÂêåÊ†∑ÁöÑ‰ΩúËÄÖÂõ¢ÈòüÂèàÂá∫‰∫Ü‰∏ÄÁØápaperÂè´ÂÅöScaling Vision TransformerÔºåÂ∞±ÊòØÂ∞ÜTransformerÂèòÂæóÂæàÂ§ßÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™Vit-GÔºåÂ∞ÜImageNetÂõæÂÉèÂàÜÁ±ªÁöÑÂáÜÁ°ÆÁéáÊèêÈ´òÂà∞‰∫Ü90‰ª•‰∏ä‰∫Ü</li>
</ul>
<p><br></p>
<h2 id="3-Áõ∏ÂÖ≥Â∑•‰Ωú"><a href="#3-Áõ∏ÂÖ≥Â∑•‰Ωú" class="headerlink" title="3.Áõ∏ÂÖ≥Â∑•‰Ωú"></a>3.Áõ∏ÂÖ≥Â∑•‰Ωú</h2><h3 id="transformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®"><a href="#transformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®" class="headerlink" title="transformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®"></a><code>transformerÂú®NLPÈ¢ÜÂüüÁöÑÂ∫îÁî®</code></h3><p>Ëá™‰ªé2017Âπ¥transformerÊèêÂá∫ÂÅöÊú∫Âô®ÁøªËØë‰ª•ÂêéÔºåÂü∫Êú¨‰∏ätransformerÂ∞±ÊòØÂæàÂ§öNLP‰ªªÂä°‰∏≠Ë°®Áé∞ÊúÄÂ•ΩÁöÑÊñπÊ≥ï„ÄÇ<strong>Áé∞Âú®Â§ßËßÑÊ®°ÁöÑtransformerÊ®°Âûã‰∏ÄËà¨ÈÉΩÊòØÂÖàÂú®‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑËØ≠ÊñôÂ∫ì‰∏äÂÅöÈ¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéÂÜçÂú®ÁõÆÊ†á‰ªªÂä°‰∏äÂÅö‰∏Ä‰∫õÁªÜÂ∞èÁöÑÂæÆË∞É</strong>ÔºåËøôÂΩì‰∏≠Êúâ‰∏§Á≥ªÂàóÊØîËæÉÂá∫ÂêçÁöÑÂ∑•‰ΩúÔºöBERTÂíåGPT„ÄÇBERTÊòØÁî®‰∏Ä‰∏™denoisingÁöÑËá™ÁõëÁù£ÊñπÂºèÔºàÂÖ∂ÂÆûÂ∞±ÊòØÂÆåÂΩ¢Â°´Á©∫ÔºåÂ∞Ü‰∏Ä‰∏™Âè•Â≠ê‰∏≠Êüê‰∫õËØçÂàíÊéâÔºåÂÜçÂ∞ÜËøô‰∫õËØçÈ¢ÑÊµãÂá∫Êù•ÔºâÔºõGPTÁî®ÁöÑÊòØlanguage modelingÔºàÂ∑≤ÁªèÊúâ‰∏Ä‰∏™Âè•Â≠êÔºåÁÑ∂ÂêéÂéªÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™ËØçÊòØ‰ªÄ‰πàÔºå‰πüÂ∞±ÊòØnext word predictionÔºâÂÅöËá™ÁõëÁù£„ÄÇËøô‰∏§‰∏™‰∫∫Áâ©ÂÖ∂ÂÆûÈÉΩÊòØ‰∫∫‰∏∫ÂÆöÁöÑÔºåËØ≠ÊñôÊòØÂõ∫ÂÆöÁöÑÔºåÂè•Â≠ê‰πüÊòØÂÆåÊï¥ÁöÑÔºåÂè™ÊòØ‰∫∫‰∏∫ÂàíÊéâÂÖ∂‰∏≠ÁöÑÊüê‰∫õÈÉ®ÂàÜÊàñËÄÖÊääÊúÄÂêéÁöÑËØçÊãøÊéâÔºåÁÑ∂ÂêéÂéªÂÅöÂÆåÂΩ¢Â°´Á©∫ÊàñËÄÖÊòØÈ¢ÑÊµã‰∏ã‰∏Ä‰∏™ËØçÔºåÊâÄ‰ª•ËøôÂè´Ëá™ÁõëÁù£ÁöÑËÆ≠ÁªÉÊñπÂºè.</p>
<p><br></p>
<h3 id="Ëá™Ê≥®ÊÑèÂäõÂú®ËßÜËßâ‰∏≠ÁöÑÂ∫îÁî®"><a href="#Ëá™Ê≥®ÊÑèÂäõÂú®ËßÜËßâ‰∏≠ÁöÑÂ∫îÁî®" class="headerlink" title="Ëá™Ê≥®ÊÑèÂäõÂú®ËßÜËßâ‰∏≠ÁöÑÂ∫îÁî®"></a><code>Ëá™Ê≥®ÊÑèÂäõÂú®ËßÜËßâ‰∏≠ÁöÑÂ∫îÁî®</code></h3><p>ËßÜËßâ‰∏≠Â¶ÇÊûúÊÉ≥ÁÆÄÂçïÂú∞Âú®ÂõæÁâá‰∏ä‰ΩøÁî®Ëá™Ê≥®ÊÑèÂäõÔºå<strong>ÊúÄÁÆÄÂçïÁöÑÊñπÂºèÂ∞±ÊòØÂ∞ÜÊØè‰∏Ä‰∏™ÂÉèÁ¥†ÁÇπÂΩìÊàêÊòØ‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåËÆ©‰ªñ‰ª¨‰∏§‰∏§ÂÅöËá™Ê≥®ÊÑèÂäõÂ∞±Â•Ω‰∫ÜÔºå‰ΩÜÊòØËøô‰∏™ÊòØÂπ≥ÊñπÂ§çÊùÇÂ∫¶ÔºåÊâÄ‰ª•ÂæàÈöæÂ∫îÁî®Âà∞ÁúüÂÆûÁöÑÂõæÁâáËæìÂÖ•Â∞∫ÂØ∏‰∏ä</strong>„ÄÇÂÉèÁé∞Âú®ÂàÜÁ±ª‰ªªÂä°ÁöÑ224*224Ôºå‰∏Ä‰∏™transformerÈÉΩÂæàÈöæÂ§ÑÁêÜÔºåÊõ¥‰∏çÁî®Êèê‰∫∫ÁúºÁúãÁöÑÊØîËæÉÊ∏ÖÊô∞ÁöÑÂõæÁâá‰∫ÜÔºå‰∏ÄËà¨ÊòØ1kÊàñËÄÖ4kÁöÑÁîªË¥®ÔºåÂ∫èÂàóÈïøÂ∫¶ÈÉΩÊòØ‰∏äÁôæ‰∏áÔºåÁõ¥Êé•Âú®ÂÉèÁ¥†Â±ÇÈù¢‰ΩøÁî®transformerÁöÑËØù‰∏çÂ§™Áé∞ÂÆûÔºåÊâÄ‰ª•Â¶ÇÊûúÊÉ≥Áî®transformerÂ∞±‰∏ÄÂÆöÂæóÂÅö‰∏Ä‰∫õËøë‰ºº</p>
<ul>
<li><strong>Â§çÊùÇÂ∫¶È´òÊòØÂõ†‰∏∫Áî®‰∫ÜÊï¥Âº†Âõæ</strong>ÔºåÊâÄ‰ª•Â∫èÂàóÈïøÂ∫¶ÈïøÔºåÈÇ£‰πàÂèØ‰ª•‰∏çÁî®Êï¥Âº†ÂõæÔºåÂ∞±<strong>Áî®local neighborhoodÔºà‰∏Ä‰∏™Â∞èÁ™óÂè£ÔºâÊù•ÂÅöËá™Ê≥®ÊÑèÂäõ</strong>ÔºåÈÇ£‰πàÂ∫èÂàóÈïøÂ∫¶Â∞±Â§ßÂ§ßÈôç‰Ωé‰∫ÜÔºåÊúÄÂêéÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶‰πüÂ∞±Èôç‰Ωé‰∫Ü</li>
<li>‰ΩøÁî®<strong>Sparse Transformer</strong>ÔºåÂ∞±ÊòØÂè™ÂØπ‰∏Ä‰∫õ<strong>Á®ÄÁñèÁöÑÁÇπÂéªÂÅöËá™Ê≥®ÊÑèÂäõ</strong>ÔºåÊâÄ‰ª•Âè™ÊòØ‰∏Ä‰∏™ÂÖ®Â±ÄÊ≥®ÊÑèÂäõÁöÑËøë‰ºº</li>
<li>Â∞ÜËá™Ê≥®ÊÑèÂäõÁî®Âà∞Â§ßÂ∞è‰∏çÂêåÁöÑblock‰∏äÔºåÊàñËÄÖËØ¥Âú®ÊûÅÁ´ØÁöÑÊÉÖÂÜµ‰∏ã‰ΩøÁî®<strong>ËΩ¥Ê≥®ÊÑèÂäõ</strong>ÔºàÂÖàÂú®Ê®™ËΩ¥‰∏äÂÅöËá™Ê≥®ÊÑèÂäõÔºåÁÑ∂ÂêéÂÜçÂú®Á∫µËΩ¥‰∏äÂÅöËá™Ê≥®ÊÑèÂäõÔºâÔºåÂ∫èÂàóÈïøÂ∫¶‰πüÊòØÂ§ßÂ§ßÂáèÂ∞èÁöÑ</li>
</ul>
<p>Ëøô‰∫õÁâπÂà∂ÁöÑËá™Ê≥®ÊÑèÂäõÁªìÊûÑÂÖ∂ÂÆûÂú®ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏äÁöÑÁªìÊûúÈÉΩ‰∏çÈîôÔºåË°®Áé∞ÈÉΩÊòØÊ≤°ÈóÆÈ¢òÁöÑÔºå‰ΩÜÊòØÂÆÉ‰ª¨ÈúÄË¶ÅÂæàÂ§çÊùÇÁöÑÂ∑•Á®ãÂéªÂä†ÈÄüÁÆóÂ≠êÔºåËôΩÁÑ∂Âú®CPUÊàñËÄÖGPU‰∏äË∑ëÂæóÂæàÂø´ÊàñËÄÖËØ¥ËÆ©ËÆ≠ÁªÉ‰∏Ä‰∏™Â§ßÊ®°ÂûãÊàê‰∏∫ÂèØËÉΩ</p>
<p><br></p>
<p>Ë∑üÊú¨ÊñáÂ∑•‰ΩúÊúÄÁõ∏‰ººÁöÑÊòØ‰∏ÄÁØáICLR2020ÁöÑËÆ∫ÊñáÔºåÂå∫Âà´Âú®‰∫éVision Transformer‰ΩøÁî®‰∫ÜÊõ¥Â§ßÁöÑpatchÔºåÊõ¥Â§ßÁöÑÊï∞ÊçÆÈõÜ</p>
<p>Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüËøòÊúâÂæàÂ§öÂ∑•‰ΩúÊòØÊääÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂíåËá™Ê≥®ÊÑèÂäõÁªìÂêàËµ∑Êù•ÁöÑÔºåËøôÁ±ªÂ∑•‰ΩúÁõ∏ÂΩìÂ§öÔºåËÄå‰∏îÂü∫Êú¨Ê∂µÁõñ‰∫ÜËßÜËßâÈáåÁöÑÂæàÂ§ö‰ªªÂä°ÔºàÊ£ÄÊµã„ÄÅÂàÜÁ±ª„ÄÅËßÜÈ¢ë„ÄÅÂ§öÊ®°ÊÄÅÁ≠âÔºâ</p>
<p>ËøòÊúâ‰∏Ä‰∏™Â∑•‰ΩúÂíåÊú¨ÊñáÁöÑÂ∑•‰ΩúÂæàÁõ∏ËøëÔºåÂè´<strong>imageGPT:</strong></p>
<ul>
<li>GPTÊòØÁî®Âú®NLP‰∏≠ÁöÑÔºåÊòØ‰∏Ä‰∏™ÁîüÊàêÊÄßÁöÑÊ®°Âûã„ÄÇimageGPT‰πüÊòØ‰∏Ä‰∏™ÁîüÊàêÊÄßÊ®°ÂûãÔºå‰πüÊòØÁî®Êó†ÁõëÁù£ÁöÑÊñπÂºèÂéªËÆ≠ÁªÉÁöÑÔºåÂíåVitÁõ∏ËøëÁöÑÂú∞ÊñπÂú®‰∫éÂÆÉ‰πüÁî®‰∫Ütransformer</li>
<li><strong>image GPTÊúÄÁªàÊâÄËÉΩËææÂà∞ÁöÑÊïàÊûú</strong>ÔºöÂ¶ÇÊûúÂ∞ÜËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂÅöÂæÆË∞ÉÊàñËÄÖÂ∞±ÊääÂÆÉÂΩìÊàê‰∏Ä‰∏™ÁâπÂæÅÊèêÂèñÂô®ÔºåÂÆÉÂú®ImageNet‰∏äÁöÑÊúÄÈ´òÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéá‰πüÂè™ËÉΩÂà∞72ÔºåVitÊúÄÁªàÁöÑÁªìÊûúÂ∑≤ÁªèÊúâ88.5‰∫ÜÔºåËøúÈ´ò‰∫é72</li>
<li><strong>‰ΩÜÊòØËøô‰∏™ÁªìÊûú‰πüÊòØÊúÄËøë‰∏ÄÁØápaperÂè´ÂÅöMAEÁàÜÁÅ´ÁöÑÂéüÂõ†</strong>„ÄÇÂõ†‰∏∫Âú®BEiTÂíåMAEËøôÁ±ªÂ∑•‰Ωú‰πãÂâçÔºåÁîüÊàêÂºèÁΩëÁªúÂú®ËßÜËßâÈ¢ÜÂüüÂæàÂ§ö‰ªªÂä°‰∏äÊòØÊ≤°ÊúâÂäûÊ≥ïË∑üÂà§Âà´ÂºèÁΩëÁªúÁõ∏ÊØîÁöÑÔºåÂà§Âà´ÂºèÁΩëÁªúÂæÄÂæÄË¶ÅÊØîÁîüÊàêÂºèÁΩëÁªúÁöÑÁªìÊûúÈ´òÂæàÂ§öÔºå‰ΩÜÊòØMAEÂÅöÂà∞‰∫ÜÔºåÂÆÉÂú®ImageNet-1kÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåÁî®‰∏Ä‰∏™ÁîüÊàêÂºèÁöÑÊ®°ÂûãÔºåÊØî‰πãÂâçÂà§Âà´ÂºèÁöÑÊ®°ÂûãÊïàÊûúÂ•ΩÂæàÂ§öÔºåËÄå‰∏î‰∏çÂÖâÊòØÂú®ÂàÜÁ±ª‰ªªÂä°‰∏äÔºåÊúÄËøëÂèëÁé∞Âú®ÁõÆÊ†áÊ£ÄÊµã‰∏äÁöÑËøÅÁßªÂ≠¶‰π†ÁöÑÊïàÊûú‰πüÈùûÂ∏∏Â•Ω</li>
</ul>
<p><br></p>
<p><strong>VitÂÖ∂ÂÆûËøòË∑üÂè¶Â§ñ‰∏ÄÁ≥ªÂàóÂ∑•‰ΩúÊòØÊúâÂÖ≥Á≥ªÁöÑÔºåÁî®ÊØîImageNetÊõ¥Â§ßÁöÑÊï∞ÊçÆÈõÜÂéªÂÅöÈ¢ÑËÆ≠ÁªÉÔºåËøôÁßç‰ΩøÁî®È¢ùÂ§ñÊï∞ÊçÆÁöÑÊñπÂºèÔºå‰∏ÄËà¨ÊúâÂä©‰∫éËææÂà∞ÁâπÂà´Â•ΩÁöÑÊïàÊûú</strong></p>
<ul>
<li>2017Âπ¥‰ªãÁªçJFT 300Êï∞ÊçÆÈõÜÁöÑpaperÁ†îÁ©∂‰∫ÜÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÁöÑÊïàÊûúÊòØÊÄé‰πàÈöèÁùÄÊï∞ÊçÆÈõÜÁöÑÂ¢ûÂ§ßËÄåÊèêÈ´òÁöÑ</li>
<li>‰∏Ä‰∫õËÆ∫ÊñáÊòØÁ†îÁ©∂‰∫ÜÂú®Êõ¥Â§ßÁöÑÊï∞ÊçÆÈõÜÔºàÊØîÂ¶ÇËØ¥ImageNet-21kÂíåJFT 300MÔºâ‰∏äÂÅöÈ¢ÑËÆ≠ÁªÉÁöÑÊó∂ÂÄôÔºåËøÅÁßªÂà∞ImageNetÊàñËÄÖCIFAR-100‰∏äÁöÑÊïàÊûúÂ¶Ç‰Ωï</li>
</ul>
<p>ËøôÁØáËÆ∫Êñá‰πüÊòØËÅöÁÑ¶‰∫éImageNet-21kÂíåJFT 300MÔºå‰ΩÜÊòØËÆ≠ÁªÉÁöÑÂπ∂‰∏çÊòØ‰∏Ä‰∏™ÊÆãÂ∑ÆÁΩëÁªúÔºåËÄåÊòØËÆ≠ÁªÉtransformer.</p>
<p><br></p>
<h2 id="4-ViTÊ®°Âûã"><a href="#4-ViTÊ®°Âûã" class="headerlink" title="4.ViTÊ®°Âûã"></a>4.ViTÊ®°Âûã</h2><h2 id="4-1-Êï¥‰ΩìÁªìÊûÑÂíåÂâçÂêë‰º†Êí≠"><a href="#4-1-Êï¥‰ΩìÁªìÊûÑÂíåÂâçÂêë‰º†Êí≠" class="headerlink" title="4.1 Êï¥‰ΩìÁªìÊûÑÂíåÂâçÂêë‰º†Êí≠"></a>4.1 Êï¥‰ΩìÁªìÊûÑÂíåÂâçÂêë‰º†Êí≠</h2><p>Âú®Ê®°ÂûãÁöÑËÆæËÆ°‰∏äÂ∞ΩÂèØËÉΩÊåâÁÖßÊúÄÂéüÂßãÁöÑtransformerÊù•ÂÅöÁöÑÔºåËøôÊ†∑ÂÅöÁöÑÂ•ΩÂ§ÑÊòØtransformerÂú®NLPÈ¢ÜÂüüÂ∑≤ÁªèÁÅ´‰∫ÜÂæà‰πÖ‰∫ÜÔºåÂÆÉÊúâ‰∏Ä‰∫õÈùûÂ∏∏È´òÊïàÁöÑÂÆûÁé∞ÔºåÂèØ‰ª•Áõ¥Êé•ÊãøÊù•‰ΩøÁî®</p>
<p><img src="https://pbs.twimg.com/media/F_sQE6PWcAA-37V?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<p>ÁÆÄÂçïËÄåË®ÄÔºåÊ®°ÂûãÁî±<strong>‰∏â‰∏™Ê®°Âùó</strong>ÁªÑÊàêÔºö</p>
<ul>
<li><strong>EmbeddingÂ±Ç</strong>ÔºàÁ∫øÊÄßÊäïÂ∞ÑÂ±ÇLinear Projection of Flattened PatchesÔºâ</li>
<li><strong>Transformer Encoder</strong>(ÂõæÂè≥‰æßÊúâÁªôÂá∫Êõ¥Âä†ËØ¶ÁªÜÁöÑÁªìÊûÑ)</li>
<li><strong>MLP Head</strong>ÔºàÊúÄÁªàÁî®‰∫éÂàÜÁ±ªÁöÑÂ±ÇÁªìÊûÑÔºâ</li>
</ul>
<p><br></p>
<p><strong>ÂâçÂêë‰º†Êí≠ËøáÁ®ã</strong>Ôºö</p>
<ul>
<li><strong>Pacth embedding</strong>: ‰∏ÄÂº†ÂõæÁâáÂÖàÂàÜÂâ≤Êàên‰∏™patchsÔºåÁÑ∂ÂêéËøô‰∫õpatchsÂèòÊàêÂ∫èÂàóÔºåÊØè‰∏™patchËæìÂÖ•Á∫øÊÄßÊäïÂ∞ÑÂ±ÇÔºåÂæóÂà∞Pacth embedding„ÄÇÊØîÂ¶ÇViT-L/16Ë°®Á§∫ÊØè‰∏™patchsÂ§ßÂ∞èÊòØ16√ó16„ÄÇ       </li>
<li><strong>position embedding</strong>Ôºöself-attentionÊú¨Ë∫´Ê≤°ÊúâËÄÉËôëËæìÂÖ•ÁöÑ‰ΩçÁΩÆ‰ø°ÊÅØÔºåÊó†Ê≥ïÂØπÂ∫èÂàóÂª∫Ê®°„ÄÇËÄåÂõæÁâáÂàáÊàêÁöÑpatches‰πüÊòØÊúâÈ°∫Â∫èÁöÑÔºåÊâì‰π±‰πãÂêéÂ∞±‰∏çÊòØÂéüÊù•ÁöÑÂõæÁâá‰∫Ü„ÄÇ‰∫éÊòØÂíåtransformer‰∏ÄÊ†∑ÔºåÂºïÂÖ•position embedding„ÄÇ       </li>
<li><strong>class token</strong>ÔºöÂú®ÊâÄÊúâtokensÂâçÈù¢Âä†‰∏Ä‰∏™Êñ∞ÁöÑclass token‰Ωú‰∏∫Ëøô‰∫õpatchsÂÖ®Â±ÄËæìÂá∫ÔºåÁõ∏ÂΩì‰∫étransformer‰∏≠ÁöÑCLSÔºàËøôÈáåÁöÑÂä†ÊòØconcatÊãºÊé•Ôºâ„ÄÇËÄå‰∏îÂÆÉ‰πüÊòØÊúâposition embeddingÔºå‰ΩçÁΩÆ‰ø°ÊÅØÊ∞∏ËøúÊòØ0</li>
<li><strong>Pacth embedding+position embedding+class token</strong>‰∏ÄËµ∑ËæìÂÖ•Transformer EncoderÔºåÂæóÂà∞ËæìÂá∫„ÄÇ</li>
<li>Âõ†‰∏∫ÊâÄÊúâÁöÑtokenÈÉΩÂú®Ë∑üÂÖ∂ÂÆÉtokenÂÅö‰∫§‰∫í‰ø°ÊÅØÔºåÊâÄ‰ª•class embeddingËÉΩÂ§ü‰ªéÂà´ÁöÑembedding‰∏≠Â≠¶Âà∞ÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåclass tokenÁöÑËæìÂá∫ÂΩìÂÅöÊï¥‰∏™ÂõæÁâáÁöÑÁâπÂæÅÔºåÁªèËøáMLP HeadÂæóÂà∞ÂàÜÁ±ªÁªìÊûúÔºàVITÂè™ÂÅöÂàÜÁ±ª‰ªªÂä°Ôºâ„ÄÇÊúÄÂêéÁî®‰∫§ÂèâÁÜµÂáΩÊï∞ËøõË°åÊ®°ÂûãÁöÑËÆ≠ÁªÉ</li>
</ul>
<p>Ê®°Âûã‰∏≠ÁöÑTransformer encoderÊòØ‰∏Ä‰∏™Ê†áÂáÜÁöÑTransformer„ÄÇÊï¥‰Ωì‰∏äÊù•ÁúãVision TransformerÁöÑÊû∂ÊûÑËøòÊòØÁõ∏ÂΩìÁÆÄÊ¥ÅÁöÑÔºå<strong>ÂÆÉÁöÑÁâπÊÆä‰πãÂ§ÑÂ∞±Âú®‰∫éÂ¶Ç‰ΩïÊää‰∏Ä‰∏™ÂõæÁâáÂèòÊàê‰∏ÄÁ≥ªÂàóÁöÑtoken</strong></p>
<p><br></p>
<h2 id="4-2-ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ"><a href="#4-2-ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ" class="headerlink" title="4.2 ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ"></a>4.2 ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ</h2><p>Ê†áÂáÜÁöÑTransformerÊ®°ÂùóË¶ÅÊ±ÇËæìÂÖ•ÁöÑÊòØtokenÔºàÂêëÈáèÔºâÂ∫èÂàóÔºåÂç≥<strong>‰∫åÁª¥Áü©Èòµ[num_token, token_dim]</strong>„ÄÇÂØπ‰∫éÂõæÂÉèÊï∞ÊçÆËÄåË®ÄÔºåÂÖ∂Êï∞ÊçÆ‰∏∫ <strong>[H, W, C]Ê†ºÂºèÁöÑ‰∏âÁª¥Áü©Èòµ</strong>ÔºåÊâÄ‰ª•ÈúÄË¶ÅÂÖàÈÄöËøá‰∏Ä‰∏™EmbeddingÂ±ÇÊù•ÂØπÊï∞ÊçÆÂÅöÂèòÊç¢.<br><img src="https://pbs.twimg.com/media/F_slGHXXAAAQ8zD?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<p>È¶ñÂÖàÂ∞Ü‰∏ÄÂº†ÂõæÁâáÊåâÁªôÂÆöÂ§ßÂ∞èÂàÜÊàê‰∏ÄÂ†ÜPatches„ÄÇ‰ª•ViT-B/16‰∏∫‰æãÔºåÂ∞ÜËæìÂÖ•ÂõæÁâá(224x224)ÊåâÁÖß16x16Â§ßÂ∞èÁöÑPatchÂ∞∫ÂØ∏ËøõË°åÂàíÂàÜÔºåÂàíÂàÜÂêé‰ºöÂæóÂà∞196‰∏™PatchesÔºåÊØè‰∏Ä‰∏™ÂõæÂÉèÂùóÁöÑÁª¥Â∫¶Â∞±ÊòØ16<em> 16 </em>3=768</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Âú®‰ª£Á†ÅÂÆûÁé∞‰∏≠ÔºåÁõ¥Êé•ÈÄöËøá‰∏Ä‰∏™Âç∑ÁßØÂ±ÇÊù•ÂÆûÁé∞„ÄÇÂç∑ÁßØÊ†∏Â§ßÂ∞è‰∏∫16x16ÔºåÊ≠•Ë∑ù‰∏∫16ÔºåÂç∑ÁßØÊ†∏‰∏™Êï∞‰∏∫768„ÄÇÈÄöËøáÂç∑ÁßØ[224, 224, 3] -&gt; [14, 14, 768]ÔºåÁÑ∂ÂêéÊääH‰ª•ÂèäW‰∏§‰∏™Áª¥Â∫¶Â±ïÂπ≥Âç≥ÂèØ[14, 14, 768] -&gt; [196, 768]ÔºåÊ≠§Êó∂Ê≠£Â•ΩÂèòÊàê‰∫Ü‰∏Ä‰∏™‰∫åÁª¥Áü©ÈòµÔºåÊ≠£ÊòØTransformerÊÉ≥Ë¶ÅÁöÑ„ÄÇ</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span></span><br><span class="line">nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br></pre></td></tr></table></figure>
<p><br></p>
<p>Êé•ÁùÄÈÄöËøáÁ∫øÊÄßÊò†Â∞ÑEÂ∞ÜÊØè‰∏™PatchÊò†Â∞ÑÂà∞‰∏ÄÁª¥ÂêëÈáè‰∏≠„ÄÇËøô‰∏™ÂÖ®ËøûÊé•Â±ÇÁöÑÁª¥Â∫¶ÊòØ768*768ÔºåÁ¨¨‰∫å‰∏™768Â∞±ÊòØÊñáÁ´†‰∏≠ÁöÑD„ÄÇ</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ÂÖ®ËøûÊé•Â±ÇÁª¥Â∫¶ÊòØ768*768ÁöÑÂéüÂõ†ÔºöÊØè‰∏™PatchÊòØ‰∏Ä‰∏™1616*3ÁöÑÂå∫Âüü(16‰πò‰ª•16‰πò‰ª•3Á≠â‰∫é768)„ÄÇ</span><br></pre></td></tr></table></figure>
<p>Áé∞Âú®ÂæóÂà∞‰∫Ü<strong>patch embeddingÔºåÂÆÉÊòØ‰∏Ä‰∏™196*768ÁöÑÁü©Èòµ</strong>ÔºåÂç≥Áé∞Âú®Êúâ196‰∏™tokenÔºåÊØè‰∏™tokenÂêëÈáèÁöÑÁª¥Â∫¶ÊòØ768ÔºåÂà∞ÁõÆÂâç‰∏∫Ê≠¢Â∞±Â∑≤ÁªèÊàêÂäüÂú∞Â∞Ü‰∏Ä‰∏™visionÁöÑÈóÆÈ¢òÂèòÊàê‰∫Ü‰∏Ä‰∏™NLPÁöÑÈóÆÈ¢ò‰∫ÜÔºåËæìÂÖ•Â∞±ÊòØ‰∏ÄÁ≥ªÂàó1dÁöÑtokenÔºåËÄå‰∏çÂÜçÊòØ‰∏ÄÂº†2dÁöÑÂõæÁâá‰∫Ü</p>
<p>È¢ùÂ§ñÁöÑ<strong>cls tokenÁª¥Â∫¶‰πüÊòØ768</strong>ÔºåËøôÊ†∑ÂèØ‰ª•Êñπ‰æøÂíåÂêéÈù¢ÂõæÂÉèÁöÑ‰ø°ÊÅØÁõ¥Êé•ËøõË°åÊãºÊé•„ÄÇÊâÄ‰ª•ÊúÄÂêéÊï¥‰ΩìËøõÂÖ•TransformerÁöÑÂ∫èÂàóÁöÑÈïøÂ∫¶ÊòØ197*768</p>
<p><strong>Position EmbeddingÊòØÂèØ‰ª•Â≠¶‰π†ÁöÑÔºåÊØè‰∏Ä‰∏™ÂêëÈáè‰ª£Ë°®‰∏Ä‰∏™‰ΩçÁΩÆ‰ø°ÊÅØ</strong>ÔºàÂêëÈáèÁöÑÁª¥Â∫¶ÊòØ768ÔºâÔºåÂ∞ÜËøô‰∫õ‰ΩçÁΩÆ‰ø°ÊÅØÂä†Âà∞ÊâÄÊúâÁöÑtoken‰∏≠ÔºåÂ∫èÂàóËøòÊòØ197*768„ÄÇ‚ÄÉ</p>
<p><strong>Ê≥®</strong>ÔºöÂØπ‰∫é‰ΩçÁΩÆÁºñÁ†Å‰ø°ÊÅØÔºåÊú¨ÊñáÁî®ÁöÑÊòØÊ†áÂáÜÁöÑÂèØ‰ª•Â≠¶‰π†ÁöÑ1d position embeddingÔºåÂÆÉ‰πüÊòØBERT‰ΩøÁî®ÁöÑ‰ΩçÁΩÆÁºñÁ†Å„ÄÇ‰ΩúËÄÖ‰πüÂ∞ùËØï‰∫Ü‰∫ÜÂà´ÁöÑÁºñÁ†ÅÂΩ¢ÂºèÔºåÊØîÂ¶ÇËØ¥2d-awareÔºàÂÆÉÊòØ‰∏Ä‰∏™ËÉΩÂ§ÑÁêÜ2d‰ø°ÊÅØÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÔºâÔºå‰ΩÜÊòØÊúÄÂêéÂèëÁé∞ÁªìÊûúÂÖ∂ÂÆûÈÉΩÂ∑Æ‰∏çÂ§öÔºåÊ≤°Êúâ‰ªÄ‰πàÂå∫Âà´   ‚ÄÉ</p>
<p><br></p>
<Br>

<p>‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî<strong>ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ-example-BEGIN</strong>‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî<br>ÂõæÁâáÈ¢ÑÂ§ÑÁêÜÁöÑÊ®°ÂùóÔºö<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *, image_size, patch_size, dim, pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span></span><br><span class="line"><span class="params">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> image_size % patch_size == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span>  <span class="comment"># ‰øùËØÅ‰∏ÄÂÆöËÉΩÂ§üÂÆåÊï¥ÂàáÂùó</span></span><br><span class="line">        num_patches = (image_size // patch_size) ** <span class="number">2</span>  <span class="comment"># ÂõæÂÉèpatchÁöÑ‰∏™Êï∞</span></span><br><span class="line">        patch_dim = channels * patch_size ** <span class="number">2</span>  <span class="comment"># Á∫øÊÄßÂèòÊç¢Êó∂ÁöÑËæìÂÖ•Â§ßÂ∞èÔºåÂç≥‰∏ãÈù¢ÁöÑp1*p2*c</span></span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span>  <span class="comment"># Ê±†ÂåñÊñπÊ≥ïÂøÖÈ°ª‰∏∫clsÊàñËÄÖmean</span></span><br><span class="line"> </span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_size, p2 = patch_size), <span class="comment"># ÊääbÂº†cÈÄöÈÅìÁöÑÂõæÂÉèÂàÜÂâ≤Êàêb*Ôºàh*wÔºâÂº†Â§ßÂ∞è‰∏∫p1*p2*cÁöÑÂõæÂÉèÂùó</span></span><br><span class="line">            nn.Linear(patch_dim, dim),  <span class="comment"># ÂØπÂàÜÂâ≤Â•ΩÁöÑÂõæÂÉèÂùóËøõË°åÁ∫øÊÄßÂ§ÑÁêÜÔºåËæìÂÖ•Áª¥Â∫¶‰∏∫ÊØè‰∏Ä‰∏™patchÁöÑÊâÄÊúâÂÉèÁ¥†‰∏™Êï∞ÔºåËæìÂá∫‰∏∫dimÔºàÂáΩÊï∞‰º†ÂÖ•ÁöÑÂèÇÊï∞Ôºâ</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))  <span class="comment"># ‰ΩçÁΩÆÁºñÁ†ÅÔºåËé∑Âèñ‰∏ÄÁªÑÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÁî®‰∫éËÆ≠ÁªÉ</span></span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))  <span class="comment"># ÂàÜÁ±ª‰ª§ÁâåÔºåÂèØËÆ≠ÁªÉ</span></span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)  <span class="comment"># ÂàáÂùóÊìç‰ΩúÔºåshape (b, n, dim)Ôºåb‰∏∫ÊâπÈáèÔºån‰∏∫ÂàáÂùóÊï∞ÁõÆÔºådim‰∏∫Á∫øÊÄßÊìç‰ΩúÊó∂ËæìÂÖ•ÁöÑÁ•ûÁªèÂÖÉ‰∏™Êï∞</span></span><br><span class="line">        b, n, _ = x.shape  <span class="comment"># shape (b, n, 1024)</span></span><br><span class="line"> </span><br><span class="line">        cls_tokens = self.cls_token.repeat([b, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># Â∞Üself.cls_tokenÁî±Ôºà1, 1, dimÔºâÂèò‰∏∫shape (b, 1, dim)</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)  <span class="comment"># Â∞ÜÂàÜÁ±ª‰ª§ÁâåÊãºÊé•Âà∞ËæìÂÖ•‰∏≠ÔºåxÁöÑshape (b, n+1, 1024)</span></span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]  <span class="comment"># Âä†‰∏ä‰ΩçÁΩÆÁºñÁ†ÅÔºåshape (b, n+1, 1024) ‰∏çÁü•ÈÅì[:, :(n + 1)]ÊòØÂπ≤ÂòõÁî®ÁöÑÔºåË≤å‰ººÂéªÊéâ‰πüË°å</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x  </span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p>Ê≥®ÊÑèÔºåpatch embeding‰ΩøÁî®ÁöÑ‰∏çÊòØÂç∑ÁßØÂ±ÇÔºåËÄåÊòØÁî®‰∫ÜÁ∫øÊÄßÂèòÊç¢ÔºåËøô‰πàÂÅöÊòØÊúâ‰ºòÂäøÁöÑ„ÄÇ‰∏ãÈù¢ÂàÜÂà´Âª∫Á´ãËøôÊ†∑ÁöÑ‰∏§‰∏™ÁΩëÁªú„ÄÇËæìÂá∫ÈÉΩÊòØtorch.Size([1, 64, 3072])<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">img = torch.randn([<span class="number">1</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>])</span><br><span class="line">net =  nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = <span class="number">32</span>, p2 = <span class="number">32</span>),</span><br><span class="line">            nn.Linear(<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, <span class="number">3072</span>),</span><br><span class="line">        )</span><br><span class="line">out_linear = net(img)</span><br><span class="line"> </span><br><span class="line">patch_embed = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span>, kernel_size=<span class="number">32</span>, stride=<span class="number">32</span>) </span><br><span class="line">)</span><br><span class="line">out_conv = patch_embed(img).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><br><img src="https://pbs.twimg.com/media/F_tFQ7AW4AAnFh8?format=jpg&amp;name=small" alt=""><br>Ëøô‰∏§‰∏™ÁΩëÁªúÁöÑÂèÇÊï∞‰∏™Êï∞ÈÉΩÊòØ9440256Ôºà3072*3073Ôºâ„ÄÇÂç∑ÁßØÂ±ÇÁöÑ‰∏çÊñπ‰æø‰πãÂ§ÑÊòØÔºåembedingÁöÑÊúÄÂêé‰∏Ä‰∏™Áª¥Â∫¶Â§ßÂ∞èÂÆûÈôÖ‰∏äÊòØÈôêÂÆöÊ≠ªÁöÑÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÊîπÂèòÁª¥Â∫¶Â§ßÂ∞èÔºåËøòË¶ÅÂÜçÊ∑ªÂä†‰∏Ä‰∏™Á∫øÊÄßÂ±Ç„ÄÇÁõ¥Êé•‰ΩøÁî®Á∫øÊÄßÂ±ÇÁöÑÂ•ΩÂ§ÑÊòØÔºåËæìÂá∫ÁöÑÊúÄÂêé‰∏Ä‰∏™ÁöÑËÆæÂÆöÂèØ‰ª•‰∏ÄÊ≠•Âà∞‰Ωç„ÄÇ</p>
<p>ÈúÄË¶ÅÈ¢ùÂ§ñÊ≥®ÊÑèÁöÑÊòØÔºåÁ∫øÊÄßÂ±ÇÁöÑËæìÂÖ•Ë¶Å‰ªîÁªÜÂÅöreshapeÔºå‰ΩøÂæóÊØè‰∏Ä‰∏™patch‰∏≠ÁöÑÂÉèÁ¥†Âú®ÂéüÂõæÂÉè‰∏≠ÈÇªËøëÔºàÂç∑ÁßØÂÅöÊ≥ïÊ≤°ÊúâËøôÊ†∑ÁöÑÊãÖÂøÉÔºâ„ÄÇËøôÈáå‰ΩøÁî®‰∫ÜRearrangeÔºåÂéüÁêÜÊòØ‰ªÄ‰πàÔºåÁî®reshapeÂáΩÊï∞Â∫îËØ•ÊÄé‰πàÂÅöÔºåËøòÊ≤°ÊúâÊêûÊáÇ</p>
<p><br></p>
<p>ÊµãËØïÔºö</p>
<p>ËæìÂÖ•ÂΩ¢Áä∂‰∏∫[32,3,256,256]ÁöÑÁü©ÈòµÔºåÊØè‰∏™patchÁöÑÂ§ßÂ∞èÊòØ32Ôºåembeding‰πãÂêéÊúÄÂêé‰∏ÄÁª¥ÁöÑÂ§ßÂ∞èÊòØ1024<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = torch.randn([<span class="number">32</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>])</span><br><span class="line">v = Embed(</span><br><span class="line">    image_size = <span class="number">256</span>,</span><br><span class="line">    patch_size = <span class="number">32</span>, </span><br><span class="line">    dim = <span class="number">1024</span>,</span><br><span class="line">    emb_dropout = <span class="number">0.1</span></span><br><span class="line">)</span><br><span class="line">out = v(img)</span><br></pre></td></tr></table></figure><br><img src="https://pbs.twimg.com/media/F_tFQ7EW0AAzhax?format=jpg&amp;name=900x900" alt=""></p>
<p><br></p>
<p>outÁöÑÂΩ¢Áä∂ÊòØ:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">32</span>, <span class="number">65</span>, <span class="number">1024</span>])</span><br></pre></td></tr></table></figure></p>
<p>‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî<strong>ÂõæÁâáÈ¢ÑÂ§ÑÁêÜ-example-END</strong>‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî</p>
<p><br><br><br></p>
<h2 id="4-3-Transformer-Encoder"><a href="#4-3-Transformer-Encoder" class="headerlink" title="4.3 Transformer Encoder"></a>4.3 Transformer Encoder</h2><p><strong>Transformer Encoder</strong>ÂÖ∂ÂÆûÂ∞±ÊòØÈáçÂ§çÂ†ÜÂè†<strong>Encoder Block</strong> LÊ¨°„ÄÇÁªèËøáÈ¢ÑÂ§ÑÁêÜÔºåÂåÖÊã¨ÁâπÊÆäÁöÑÂ≠óÁ¨¶clsÂíå‰ΩçÁΩÆÁºñÁ†Å‰ø°ÊÅØÔºå<strong>transformer</strong>ËæìÂÖ•ÁöÑ<strong>embedded patches</strong>Â∞±ÊòØ‰∏Ä‰∏™197*768ÁöÑtensor„ÄÇ<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([]) <span class="comment"># TransformerÂåÖÂê´Â§ö‰∏™ÁºñÁ†ÅÂô®ÁöÑÂè†Âä†</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            <span class="comment"># ÁºñÁ†ÅÂô®ÂåÖÂê´‰∏§Â§ßÂùóÔºöËá™Ê≥®ÊÑèÂäõÊ®°ÂùóÂíåÂâçÂêë‰º†Êí≠Ê®°Âùó</span></span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),</span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))</span><br><span class="line">            ]))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = attn(x) + x  <span class="comment"># Ëá™Ê≥®ÊÑèÂäõÊ®°ÂùóÂíåÂâçÂêë‰º†Êí≠Ê®°ÂùóÈÉΩ‰ΩøÁî®‰∫ÜÊÆãÂ∑ÆÁöÑÊ®°Âºè</span></span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<ul>
<li><strong>Layer NormÂ±ÇÊ†áÂáÜÂåñ</strong>ÔºötensorÂÖàËøá‰∏Ä‰∏™layer normÔºåÂá∫Êù•‰πãÂêéËøòÊòØ197*768„ÄÇ<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)  <span class="comment"># Ê≠£ÂàôÂåñ</span></span><br><span class="line">        self.fn = fn  <span class="comment"># ÂÖ∑‰ΩìÁöÑÊìç‰Ωú</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br></p>
<ul>
<li><strong>Multi-Head Attention</strong>ÔºöÂÅáËÆæ‰ΩøÁî®ÁöÑÊòØViTÁöÑbaseÁâàÊú¨ÔºåÂç≥‰ΩøÁî®‰∫Ü12‰∏™Â§¥ÔºåÈÇ£‰πàk„ÄÅq„ÄÅvÁöÑÁª¥Â∫¶ÂèòÊàê‰∫Ü197<em>64Ôºà768/12=64ÔºâÔºåËøõË°å12ÁªÑk„ÄÅq„ÄÅvËá™Ê≥®ÊÑèÂäõÊìç‰ΩúÔºåÊúÄÂêéÂÜçÂ∞Ü12‰∏™Â§¥ÁöÑËæìÂá∫ÊãºÊé•Ëµ∑Êù•ÔºåËæìÂá∫ËøòÊòØ197</em>768<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim) <span class="comment"># Â§öÂ§¥Ê≥®ÊÑèÂäõÊàñËæìÂÖ•ÂíåËæìÂá∫Áª¥Â∫¶‰∏çÁõ∏ÂêåÊó∂‰∏∫True</span></span><br><span class="line"> </span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span>  <span class="comment"># Áº©ÊîæÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        self.attend = nn.Softmax(dim = -<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"> </span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>) <span class="comment"># ÂØπQ„ÄÅK„ÄÅV‰∏âÁªÑÂêëÈáèÁ∫øÊÄßÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Á∫øÊÄßÂÖ®ËøûÊé•ÔºåÂ¶ÇÊûúÂ§öÂ§¥Ê≥®ÊÑèÂäõÊàñËæìÂÖ•ÂíåËæìÂá∫Áª¥Â∫¶‰∏çÁõ∏ÂêåÔºåËøõË°åÊò†Â∞ÑÔºåÂèòÊç¢Áª¥Â∫¶</span></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)  <span class="comment"># ÂÖàÂØπQ„ÄÅK„ÄÅVËøõË°åÁ∫øÊÄßÊìç‰ΩúÔºåÁÑ∂ÂêéchunkÊàê‰∏â‰ªΩ</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = self.heads), qkv) <span class="comment"># Êï¥ÁêÜÁª¥Â∫¶ÔºåËé∑ÂæóQ„ÄÅK„ÄÅV</span></span><br><span class="line"> </span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># ËÆ°ÁÆóÁõ∏ÂÖ≥ÊÄß</span></span><br><span class="line"> </span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"> </span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># # SoftmaxËøêÁÆóÁªìÊûú‰∏éValueÂêëÈáèÁõ∏‰πòÔºåÂæóÂà∞ÊúÄÁªàÁªìÊûú</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)  <span class="comment"># ÈáçÊñ∞Êï¥ÁêÜÁª¥Â∫¶</span></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)  <span class="comment"># ÂÅöÁ∫øÊÄßÁöÑÂÖ®ËøûÊé•Êìç‰ΩúÊàñËÄÖÁ©∫Êìç‰ΩúÔºàÁ©∫Êìç‰ΩúÁõ¥Êé•ËæìÂá∫outÔºâ</span></span><br></pre></td></tr></table></figure>
torch.chunk(tensor, chunk_num, dim)ÂáΩÊï∞ÁöÑÂäüËÉΩÔºö‰∏étorch.cat()ÂàöÂ•ΩÁõ∏ÂèçÔºåÂÆÉÊòØÂ∞ÜtensorÊåâdimÔºàË°åÊàñÂàóÔºâÂàÜÂâ≤Êàêchunk_num‰∏™tensorÂùóÔºåËøîÂõûÁöÑÊòØ‰∏Ä‰∏™ÂÖÉÁªÑ„ÄÇ</li>
</ul>
<p><br><br><br></p>
<p><strong>rearrangeÊìç‰ΩúÂ∞±ÊòØË∞ÉÊï¥Áª¥Â∫¶</strong>„ÄÇÊØîÂ¶Ç:</p>
<ul>
<li>rearrange(out, ‚Äòb h n d -&gt; b n (h d)‚Äô)Á≠â‰∫éout.transpose(1, 2).reshape(B, N, C)</li>
<li>q, k, v = map(lambda t: rearrange(t, ‚Äòb n (h d) -&gt; b h n d‚Äô, h = self.heads), qkv)ÂèØ‰ª•ÂÜôÊàê‰∏ãÈù¢<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># qkv(): -&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]</span></span><br><span class="line"><span class="comment"># reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]</span></span><br><span class="line"><span class="comment"># permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, total_embed_dim // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br><br><br></p>
<p>attentionÊìç‰ΩúÁöÑÊï¥‰ΩìÊµÅÁ®ãÔºö</p>
<ul>
<li>È¶ñÂÖàÂØπËæìÂÖ•ÁîüÊàêquery, keyÂíåvalueÔºåËøôÈáåÁöÑ‚ÄúËæìÂÖ•‚ÄùÊúâÂèØËÉΩÊòØÊï¥‰∏™ÁΩëÁªúÁöÑËæìÂÖ•Ôºå‰πüÂèØËÉΩÊòØÊüê‰∏™hidden layerÁöÑoutput„ÄÇÂú®ËøôÈáåÔºåÁîüÊàêÁöÑqkvÊòØ‰∏™ÈïøÂ∫¶‰∏∫3ÁöÑÂÖÉÁªÑÔºåÊØè‰∏™ÂÖÉÁªÑÁöÑÂ§ßÂ∞è‰∏∫(1, 65, 1024)</li>
<li>ÂØπqkvËøõË°åÂ§ÑÁêÜÔºåÈáçÊñ∞ÊåáÂÆöÁª¥Â∫¶ÔºåÂæóÂà∞ÁöÑq, k, vÁª¥Â∫¶Âùá‰∏∫(1, 16, 65, 64)</li>
<li>qÂíåkÂÅöÁÇπ‰πòÔºåÂæóÂà∞ÁöÑdotsÁª¥Â∫¶‰∏∫(1, 16, 65, 65)</li>
<li>ÂØπdotsÁöÑÊúÄÂêé‰∏ÄÁª¥ÂÅösoftmaxÔºåÂæóÂà∞ÂêÑ‰∏™patchÂØπÂÖ∂‰ªñpatchÁöÑÊ≥®ÊÑèÂäõÂæóÂàÜ</li>
<li>Â∞ÜattentionÂíåvalueÂÅöÁÇπ‰πò</li>
<li>ÂØπÂêÑ‰∏™Áª¥Â∫¶ÈáçÊñ∞ÊéíÂàóÔºåÂæóÂà∞‰∏éËæìÂÖ•Áõ∏ÂêåÁª¥Â∫¶ÁöÑËæìÂá∫ (1, 65, 1024)</li>
<li>Ê†πÊçÆÈúÄË¶ÅÔºåÂÅöÊäïÂ∞Ñ<ul>
<li>Dropout/DropPathÔºöÂú®ÂéüËÆ∫ÊñáÁöÑ‰ª£Á†Å‰∏≠ÊòØÁõ¥Êé•‰ΩøÁî®ÁöÑDropoutÂ±ÇÔºåÂú®‰ΩÜrwightmanÂÆûÁé∞ÁöÑ‰ª£Á†Å‰∏≠‰ΩøÁî®ÁöÑÊòØDropPathÔºàstochastic depthÔºâÔºåÂèØËÉΩÂêéËÄÖ‰ºöÊõ¥Â•Ω‰∏ÄÁÇπ„ÄÇ</li>
<li>ÂÜçËøá‰∏ÄÂ±Çlayer normÔºåËøòÊòØ197*768</li>
<li>MLP BlockÔºåÂÖ®ËøûÊé•+GELUÊøÄÊ¥ªÂáΩÊï∞+DropoutÁªÑÊàê„ÄÇÊääÁª¥Â∫¶ÊîæÂ§ßÂà∞4ÂÄç[197, 768] -&gt; [197, 3072]ÔºåÂÜçËøòÂéüÂõûÂéüËäÇÁÇπ‰∏™Êï∞[197, 3072] -&gt; [197, 768]„ÄÇ</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># ÂâçÂêë‰º†Êí≠</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>
<p>ËøõÂéªTransformer block‰πãÂâçÊòØ197<em>768ÔºåÂá∫Êù•ËøòÊòØ197</em>768ÔºåËøô‰∏™Â∫èÂàóÁöÑÈïøÂ∫¶ÂíåÊØè‰∏™tokenÂØπÂ∫îÁöÑÁª¥Â∫¶Â§ßÂ∞èÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºåÊâÄ‰ª•Â∞±ÂèØ‰ª•Âú®‰∏Ä‰∏™Transformer block‰∏ä‰∏çÂÅúÂú∞ÂæÄ‰∏äÂè†Âä†Transformer blockÔºåÊúÄÂêéÊúâLÂ±ÇTransformer blockÁöÑÊ®°ÂûãÂ∞±ÊûÑÊàê‰∫ÜTransformer encoder</p>
<p>Transformer‰ªéÂ§¥Âà∞Â∞æÈÉΩÊòØ‰ΩøÁî®DÂΩì‰ΩúÂêëÈáèÁöÑÈïøÂ∫¶ÁöÑÔºåÈÉΩÊòØ768ÔºåÂêå‰∏Ä‰∏™Ê®°ÂûãÈáåËøô‰∏™Áª¥Â∫¶ÊòØ‰∏çÂèòÁöÑ„ÄÇÂ¶ÇÊûútransformerÂèòÂæóÊõ¥Â§ß‰∫ÜÔºåD‰πüÂèØ‰ª•Áõ∏Â∫îÁöÑÂèòÂæóÊõ¥Â§ß„ÄÇ</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to D dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="4-4-MLP-HeadÂíåViT-B-16Ê®°ÂûãÁªìÊûÑÂõæ"><a href="#4-4-MLP-HeadÂíåViT-B-16Ê®°ÂûãÁªìÊûÑÂõæ" class="headerlink" title="4.4 MLP HeadÂíåViT-B/16Ê®°ÂûãÁªìÊûÑÂõæ"></a>4.4 MLP HeadÂíåViT-B/16Ê®°ÂûãÁªìÊûÑÂõæ</h2><p>ÂØπ‰∫éÂàÜÁ±ªÔºåÂè™ÈúÄË¶ÅÊèêÂèñÂá∫[class]tokenÁîüÊàêÁöÑÂØπÂ∫îÁªìÊûúÂ∞±Ë°åÔºåÂç≥[197, 768]‰∏≠ÊäΩÂèñÂá∫[class]tokenÂØπÂ∫îÁöÑ[1, 768]ÔºåÈÄöËøáMLP HeadÂæóÂà∞ÊúÄÁªàÁöÑÂàÜÁ±ªÁªìÊûú„ÄÇMLP HeadÂéüËÆ∫Êñá‰∏≠ËØ¥Âú®ËÆ≠ÁªÉImageNet21KÊó∂ÊòØÁî±Linear+tanhÊøÄÊ¥ªÂáΩÊï∞+LinearÁªÑÊàêÔºå‰ΩÜÊòØËøÅÁßªÂà∞ImageNet1K‰∏äÊàñËÄÖ‰Ω†Ëá™Â∑±ÁöÑÊï∞ÊçÆ‰∏äÊó∂ÔºåÂè™ÂÆö‰πâ‰∏Ä‰∏™LinearÂç≥ÂèØ„ÄÇÊ≥®ÊÑèÔºåÂú®Transformer EncoderÂêéÂÖ∂ÂÆûËøòÊúâ‰∏Ä‰∏™Layer NormÔºå</p>
<p><img src="https://pbs.twimg.com/media/F_txvGWXAAAetnz?format=jpg&amp;name=small" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size, patch_size, num_classes, </span></span><br><span class="line"><span class="params">                 dim, depth, heads, mlp_dim, pool = <span class="string">&#x27;cls&#x27;</span>, </span></span><br><span class="line"><span class="params">                 channels = <span class="number">3</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span>, </span></span><br><span class="line"><span class="params">                 emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed = Embed(image_size, patch_size, dim, </span><br><span class="line">                 pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span><br><span class="line">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span>)</span><br><span class="line"> </span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)  <span class="comment"># TransformerÊ®°Âùó</span></span><br><span class="line"> </span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()  <span class="comment"># Âç†‰ΩçÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        self.mlp_head = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),  <span class="comment"># Ê≠£ÂàôÂåñ</span></span><br><span class="line">            nn.Linear(dim, num_classes)  <span class="comment"># Á∫øÊÄßËæìÂá∫</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.embed(img)  <span class="comment"># embedingÊìç‰Ωú</span></span><br><span class="line">        x = self.transformer(x)  <span class="comment"># transformerÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)  <span class="comment"># Á∫øÊÄßËæìÂá∫</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="4-5-Êï∞Â≠¶ÂÖ¨ÂºèÊèèËø∞"><a href="#4-5-Êï∞Â≠¶ÂÖ¨ÂºèÊèèËø∞" class="headerlink" title="4.5 Êï∞Â≠¶ÂÖ¨ÂºèÊèèËø∞"></a>4.5 Êï∞Â≠¶ÂÖ¨ÂºèÊèèËø∞</h2><p><img src="https://pbs.twimg.com/media/F_txvGTW4AEru5m?format=jpg&amp;name=large" alt=""></p>
<p>Ôºà1ÔºâXp‚ÄãË°®Á§∫ÂõæÂÉèÂùóÁöÑpatchÔºå‰∏ÄÂÖ±ÊúâN‰∏™patchÔºåEË°®Á§∫Á∫øÊÄßÊäïÂΩ±ÁöÑÂÖ®ËøûÊé•Â±ÇÔºåÂæóÂà∞‰∏Ä‰∫õpatch embedding„ÄÇÂú®ÂÆÉÂâçÈù¢ÊãºÊé•‰∏Ä‰∏™class embeddingÔºàXclassÔºâ„ÄÇÂæóÂà∞ÊâÄÊúâÁöÑtokensÂêéÔºåÂ∞Ü‰ΩçÁΩÆÁºñÁ†Å‰ø°ÊÅØEpos‰πüÂä†ËøõÂéª„ÄÇ</p>
<ul>
<li>Z0Â∞±ÊòØÊï¥‰∏™transformerÁöÑËæìÂÖ•„ÄÇ</li>
</ul>
<p><br></p>
<p>Ôºà2Ôºâ-Ôºà3ÔºâÂæ™ÁéØ</p>
<p>ÂØπ‰∫éÊØè‰∏™transformer blockÊù•ËØ¥ÔºåÈáåÈù¢ÈÉΩÊúâ‰∏§‰∏™Êìç‰ΩúÔºö‰∏Ä‰∏™ÊòØÂ§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÔºå‰∏Ä‰∏™ÊòØMLP„ÄÇÂú®ÂÅöËøô‰∏§‰∏™Êìç‰Ωú‰πãÂâçÔºåÈÉΩË¶ÅÂÖàÁªèËøálayer normÔºåÊØè‰∏ÄÂ±ÇÂá∫Êù•ÁöÑÁªìÊûúÈÉΩË¶ÅÂÜçÂéªÁî®‰∏Ä‰∏™ÊÆãÂ∑ÆËøûÊé•</p>
<ul>
<li>ZL‚ÄôÂ∞±ÊòØÊØè‰∏Ä‰∏™Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÂá∫Êù•ÁöÑÁªìÊûú</li>
<li>ZLÂ∞±ÊòØÊØè‰∏Ä‰∏™transformer blockÊï¥‰ΩìÂÅöÂÆå‰πãÂêéÂá∫Êù•ÁöÑÁªìÊûú</li>
</ul>
<Br>

<p>4ÔºâLÂ±ÇÂæ™ÁéØÁªìÊùü‰πãÂêéÂ∞ÜZLÔºàÊúÄÂêé‰∏ÄÂ±ÇÁöÑËæìÂá∫ÔºâÁöÑÁ¨¨‰∏Ä‰∏™‰ΩçÁΩÆ‰∏äÁöÑZL0Ôºå‰πüÂ∞±ÊòØclass tokenÊâÄÂØπÂ∫îÁöÑËæìÂá∫ÂΩì‰ΩúÊï¥‰ΩìÂõæÂÉèÁöÑÁâπÂæÅÔºåÂéªÂÅöÊúÄÂêéÁöÑÂàÜÁ±ª‰ªªÂä°</p>
<p><br></p>
<h2 id="ÂÆåÊï¥‰ª£Á†Å"><a href="#ÂÆåÊï¥‰ª£Á†Å" class="headerlink" title="ÂÆåÊï¥‰ª£Á†Å"></a>ÂÆåÊï¥‰ª£Á†Å</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, einsum</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size, patch_size, dim, </span></span><br><span class="line"><span class="params">                 pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span></span><br><span class="line"><span class="params">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> image_size % patch_size == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span>  <span class="comment"># ‰øùËØÅ‰∏ÄÂÆöËÉΩÂ§üÂÆåÊï¥ÂàáÂùó</span></span><br><span class="line">        num_patches = (image_size // patch_size) ** <span class="number">2</span>  <span class="comment"># Ëé∑ÂèñÂõæÂÉèÂàáÂùóÁöÑ‰∏™Êï∞</span></span><br><span class="line">        patch_dim = channels * patch_size ** <span class="number">2</span>  <span class="comment"># Á∫øÊÄßÂèòÊç¢Êó∂ÁöÑËæìÂÖ•Â§ßÂ∞èÔºåÂç≥ÊØè‰∏Ä‰∏™ÂõæÂÉèÂÆΩ„ÄÅÈ´ò„ÄÅÈÄöÈÅìÁöÑ‰πòÁßØ</span></span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span>  <span class="comment"># Ê±†ÂåñÊñπÊ≥ïÂøÖÈ°ª‰∏∫clsÊàñËÄÖmean</span></span><br><span class="line"> </span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_size, p2 = patch_size),  <span class="comment"># ÊääbÂº†cÈÄöÈÅìÁöÑÂõæÂÉèÂàÜÂâ≤Êàêb*Ôºàh*wÔºâÂº†Â§ßÂ∞è‰∏∫P1*p2*cÁöÑÂõæÂÉèÂùó</span></span><br><span class="line">            nn.Linear(patch_dim, dim),  <span class="comment"># ÂØπÂàÜÂâ≤Â•ΩÁöÑÂõæÂÉèÂùóËøõË°åÁ∫øÊÄßÂ§ÑÁêÜÔºàÂÖ®ËøûÊé•ÔºâÔºåËæìÂÖ•Áª¥Â∫¶‰∏∫ÊØè‰∏Ä‰∏™Â∞èÂùóÁöÑÊâÄÊúâÂÉèÁ¥†‰∏™Êï∞ÔºåËæìÂá∫‰∏∫dimÔºàÂáΩÊï∞‰º†ÂÖ•ÁöÑÂèÇÊï∞Ôºâ</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))  <span class="comment"># ‰ΩçÁΩÆÁºñÁ†ÅÔºåËé∑Âèñ‰∏ÄÁªÑÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÁî®‰∫éËÆ≠ÁªÉ</span></span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))  <span class="comment"># ÂàÜÁ±ª‰ª§ÁâåÔºåÂèØËÆ≠ÁªÉ</span></span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)  <span class="comment"># ÂàáÂùóÊìç‰ΩúÔºåshape (b, n, dim)Ôºåb‰∏∫ÊâπÈáèÔºån‰∏∫ÂàáÂùóÊï∞ÁõÆÔºådim‰∏∫ÊúÄÁªàÁ∫øÊÄßÊìç‰ΩúÊó∂ËæìÂÖ•ÁöÑÁ•ûÁªèÂÖÉ‰∏™Êï∞</span></span><br><span class="line">        b, n, _ = x.shape  <span class="comment"># shape (b, n, 1024)</span></span><br><span class="line"> </span><br><span class="line">        cls_tokens = self.cls_token.repeat([b, <span class="number">1</span>, <span class="number">1</span>])  </span><br><span class="line">        <span class="comment"># ÂàÜÁ±ª‰ª§ÁâåÔºåÂ∞Üself.cls_tokenÔºàÂΩ¢Áä∂‰∏∫1, 1, dimÔºâËµãÂÄº‰∏∫shape (b, 1, dim)</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)  <span class="comment"># Â∞ÜÂàÜÁ±ª‰ª§ÁâåÊãºÊé•Âà∞ËæìÂÖ•‰∏≠ÔºåxÁöÑshape (b, n+1, 1024)</span></span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]  <span class="comment"># ËøõË°å‰ΩçÁΩÆÁºñÁ†ÅÔºåshape (b, n+1, 1024)</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)  <span class="comment"># Ê≠£ÂàôÂåñ</span></span><br><span class="line">        self.fn = fn  <span class="comment"># ÂÖ∑‰ΩìÁöÑÊìç‰Ωú</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># ÂâçÂêë‰º†Êí≠</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim) <span class="comment"># Â§öÂ§¥Ê≥®ÊÑèÂäõÊàñËæìÂÖ•ÂíåËæìÂá∫Áª¥Â∫¶‰∏çÁõ∏ÂêåÊó∂‰∏∫True</span></span><br><span class="line"> </span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span>  <span class="comment"># Áº©ÊîæÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        self.attend = nn.Softmax(dim = -<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"> </span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>) <span class="comment"># ÂØπQ„ÄÅK„ÄÅV‰∏âÁªÑÂêëÈáèÁ∫øÊÄßÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Á∫øÊÄßÂÖ®ËøûÊé•ÔºåÂ¶ÇÊûúÂ§öÂ§¥Ê≥®ÊÑèÂäõÊàñËæìÂÖ•ÂíåËæìÂá∫Áª¥Â∫¶‰∏çÁõ∏ÂêåÔºåËøõË°åÊò†Â∞ÑÔºåÂèòÊç¢Áª¥Â∫¶</span></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)  <span class="comment"># ÂÖàÂØπQ„ÄÅK„ÄÅVËøõË°åÁ∫øÊÄßÊìç‰ΩúÔºåÁÑ∂ÂêéchunkÊàê‰∏â‰ªΩ</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = self.heads), qkv) <span class="comment"># Êï¥ÁêÜÁª¥Â∫¶ÔºåËé∑ÂæóQ„ÄÅK„ÄÅV</span></span><br><span class="line"> </span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># ËÆ°ÁÆóÁõ∏ÂÖ≥ÊÄß</span></span><br><span class="line"> </span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"> </span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># # SoftmaxËøêÁÆóÁªìÊûú‰∏éValueÂêëÈáèÁõ∏‰πòÔºåÂæóÂà∞ÊúÄÁªàÁªìÊûú</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)  <span class="comment"># ÈáçÊñ∞Êï¥ÁêÜÁª¥Â∫¶</span></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)  <span class="comment"># ÂÅöÁ∫øÊÄßÁöÑÂÖ®ËøûÊé•Êìç‰ΩúÊàñËÄÖÁ©∫Êìç‰ΩúÔºàÁ©∫Êìç‰ΩúÁõ¥Êé•ËæìÂá∫outÔºâ</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])  <span class="comment"># TransformerÂåÖÂê´Â§ö‰∏™ÁºñÁ†ÅÂô®ÁöÑÂè†Âä†</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            <span class="comment"># ÁºñÁ†ÅÂô®ÂåÖÂê´‰∏§Â§ßÂùóÔºöËá™Ê≥®ÊÑèÂäõÊ®°ÂùóÂíåÂâçÂêë‰º†Êí≠Ê®°Âùó</span></span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),  <span class="comment"># Â§öÂ§¥Ëá™Ê≥®ÊÑèÂäõÊ®°Âùó</span></span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))  <span class="comment"># ÂâçÂêë‰º†Êí≠Ê®°Âùó</span></span><br><span class="line">            ]))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># Ëá™Ê≥®ÊÑèÂäõÊ®°ÂùóÂíåÂâçÂêë‰º†Êí≠Ê®°ÂùóÈÉΩ‰ΩøÁî®‰∫ÜÊÆãÂ∑ÆÁöÑÊ®°Âºè</span></span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size, patch_size, num_classes, </span></span><br><span class="line"><span class="params">                 dim, depth, heads, mlp_dim, pool = <span class="string">&#x27;cls&#x27;</span>, </span></span><br><span class="line"><span class="params">                 channels = <span class="number">3</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span>, </span></span><br><span class="line"><span class="params">                 emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed = Embed(image_size, patch_size, dim, </span><br><span class="line">                 pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span><br><span class="line">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span>)</span><br><span class="line"> </span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)  <span class="comment"># TransformerÊ®°Âùó</span></span><br><span class="line"> </span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()  <span class="comment"># Âç†‰ΩçÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        self.mlp_head = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),  <span class="comment"># Ê≠£ÂàôÂåñ</span></span><br><span class="line">            nn.Linear(dim, num_classes)  <span class="comment"># Á∫øÊÄßËæìÂá∫</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.embed(img)  </span><br><span class="line">        x = self.transformer(x)  <span class="comment"># transformerÊìç‰Ωú</span></span><br><span class="line"> </span><br><span class="line">        x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)  <span class="comment"># Á∫øÊÄßËæìÂá∫</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>image_size</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåÂõæÁâáÂ§ßÂ∞è„ÄÇ Â¶ÇÊûúÁü©ÂΩ¢ÂõæÂÉèÔºå‰∏∫ÂÆΩÂ∫¶ÂíåÈ´òÂ∫¶ÁöÑÊúÄÂ§ßÂÄº</li>
<li><strong>patch_size</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåpatchÂ§ßÂ∞è„ÄÇimage_size ÂøÖÈ°ªËÉΩÂ§üË¢´ patch_sizeÊï¥Èô§„ÄÇn must be greater than 16</li>
<li><strong>num_classes</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåÂàÜÁ±ªÊï∞ÁõÆ„ÄÇ</li>
<li><strong>dim</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåembeddingÁöÑÁª¥Â∫¶„ÄÇ</li>
<li><strong>depth</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåTransformerÊ®°ÂùóÁöÑ‰∏™Êï∞„ÄÇ</li>
<li><strong>heads</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåÂ§öÂ§¥Ê≥®ÊÑèÂäõ‰∏≠‚ÄúÂ§¥‚ÄùÁöÑ‰∏™Êï∞„ÄÇ</li>
<li><strong>mlp_dim</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåÂ§öÂ±ÇÊÑüÁü•Êú∫‰∏≠ÈöêËóèÂ±ÇÁöÑÁ•ûÁªèÂÖÉ‰∏™Êï∞„ÄÇ</li>
<li><strong>channels</strong>Ôºöint Á±ªÂûãÂèÇÊï∞ÔºåËæìÂÖ•ÂõæÂÉèÁöÑÈÄöÈÅìÊï∞ÔºåÈªòËÆ§‰∏∫3„ÄÇ</li>
<li><strong>dropout</strong>ÔºöfloatÁ±ªÂûãÂèÇÊï∞ÔºåDropoutÂá†ÁéáÔºåÂèñÂÄºËåÉÂõ¥‰∏∫[0, 1]ÔºåÈªòËÆ§‰∏∫ 0.„ÄÇ</li>
<li><strong>emb_dropout</strong>ÔºöfloatÁ±ªÂûãÂèÇÊï∞ÔºåËøõË°åEmbeddingÊìç‰ΩúÊó∂DropoutÂá†ÁéáÔºåÂèñÂÄºËåÉÂõ¥‰∏∫[0, 1]ÔºåÈªòËÆ§‰∏∫0„ÄÇ</li>
<li><strong>pool</strong>ÔºöstringÁ±ªÂûãÂèÇÊï∞ÔºåÂèñÂÄº‰∏∫ clsÊàñËÄÖ mean „ÄÇ</li>
</ul>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUkWsAAoYn0?format=jpg&amp;name=small" alt=""></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUnWkAEqdX1?format=jpg&amp;name=small" alt=""></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUqXcAA2aaT?format=jpg&amp;name=small" alt=""></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUmXAAE6U8-?format=jpg&amp;name=small" alt=""></p>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/11/24/paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/" data-id="clpc3438t00003j69bc9w5hxq" data-title="paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
               class="article-share-link">ÂàÜ‰∫´</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/" rel="tag">paper</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vit/" rel="tag">vit</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="d2l:Á∫øÊÄßÂõûÂΩí"
                         class="lazyload">
                
                <a href="/2023/11/25/d2l-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"></a>
                <div class="article-nav-caption">Ââç‰∏ÄÁØá</div>
                <h3 class="article-nav-title">
                    
                        d2l:Á∫øÊÄßÂõûÂΩí
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="Pytorch:nn.Conv2d"
                         class="lazyload">
                
                <a href="/2023/11/24/Pytorch-nn-Conv2d/"></a>
                <div class="article-nav-caption">Âêé‰∏ÄÁØá</div>
                <h3 class="article-nav-title">
                    
                        Pytorch:nn.Conv2d
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="„ÅÇ„Åæ„ÅÆ„Å≤„Å™" class="lazyload">
            <div class="sidebar-author-name">„ÅÇ„Åæ„ÅÆ„Å≤„Å™</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>ÊñáÁ´†</div>
                <div class="sidebar-state-number">253</div>
            </div>
            <div class="sidebar-state-category">
                <div>ÂàÜÁ±ª</div>
                <div class="sidebar-state-number">22</div>
            </div>
            <div class="sidebar-state-tag">
                <div>Ê†áÁ≠æ</div>
                <div class="sidebar-state-number">306</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">È¶ñÈ°µ</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">ÂΩíÊ°£</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">ÂÖ≥‰∫é</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">ÂèãÈìæ</div>
                </div>
            
        </div>
    </div>
    
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=28854246&auto=1&height=66"></iframe>

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">ÂàÜÁ±ª</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">‰∏ì‰∏öÁü•ËØÜ</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Missing-Semester-of-CS/">Missing Semester of CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">Êô∫ËÉΩËÆ°ÁÆóÁ≥ªÁªü</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">ÊùÇÈ°π</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">Ê†áÁ≠æ‰∫ë</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 10.63px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/17/" style="font-size: 10px;">17</a> <a href="/tags/2/" style="font-size: 11.88px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 10.63px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/4/" style="font-size: 10.63px;">4</a> <a href="/tags/5/" style="font-size: 10px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 11.25px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 12.5px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.25px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 12.5px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 11.88px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CV/" style="font-size: 10.63px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 13.13px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 16.25px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.63px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/Git/" style="font-size: 10.63px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.88px;">GoAbroad</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 10.63px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/Lec01/" style="font-size: 11.25px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.63px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.25px;">Linux</a> <a href="/tags/M2/" style="font-size: 10.63px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.5px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 12.5px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.63px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 10px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 17.5px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OS/" style="font-size: 12.5px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.63px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.25px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.5px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.63px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.63px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/University/" style="font-size: 13.13px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 10.63px;">ViT</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.63px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 11.88px;">bert</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/bug/" style="font-size: 14.38px;">bug</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.25px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10.63px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.63px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.63px;">chapter05</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/code/" style="font-size: 11.25px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/commit/" style="font-size: 10px;">commit</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.13px;">d2l</a> <a href="/tags/database/" style="font-size: 13.75px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.63px;">deep neural network</a> <a href="/tags/delete/" style="font-size: 10px;">delete</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/dowhy/" style="font-size: 10.63px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/explainer/" style="font-size: 10.63px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/github/" style="font-size: 11.25px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 10.63px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/instructor/" style="font-size: 11.88px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 11.88px;">intern00</a> <a href="/tags/internship/" style="font-size: 18.75px;">internship</a> <a href="/tags/introduction/" style="font-size: 11.25px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.63px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 11.25px;">mac</a> <a href="/tags/mentor/" style="font-size: 10px;">mentor</a> <a href="/tags/mid/" style="font-size: 10.63px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.63px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.63px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 15.63px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.25px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.38px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 11.88px;">prml</a> <a href="/tags/profile/" style="font-size: 10px;">profile</a> <a href="/tags/pytorch/" style="font-size: 11.88px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 15px;">review</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.63px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/spike/" style="font-size: 10.63px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.63px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikingjelly/" style="font-size: 12.5px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.63px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.63px;">ssh</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.63px;">tips</a> <a href="/tags/tool/" style="font-size: 18.13px;">tool</a> <a href="/tags/transformer/" style="font-size: 11.88px;">transformer</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10px;">vscode</a> <a href="/tags/wakatime/" style="font-size: 10px;">wakatime</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">‰∏ì‰∏öÁü•ËØÜ</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">‰∏≠‰ªã</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">‰∏≠ÁßëÈô¢</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">ÂÖ¨ÈÄâËØæ</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.63px;">ÂÜÖÂ≠ò</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">ÂÜô‰ΩúÂøÉÂæó</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">Âä†ÂàÜ</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">Âä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">ÂõæÂÉèÊèèËø∞ÁîüÊàê</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">Âü∫Á°Ä‰ºòÂåñÊñπÊ≥ï</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">Â§ç‰π†</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">Â§öÊ®°ÊÄÅ</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">Â§ß‰∏â‰∏ä</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">Â§ß‰Ωú‰∏ö</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.63px;">ÂÆ°Á®øÊÑèËßÅ</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">Âº∫Âº±com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">ÂΩ¢Âäø‰∏éÊîøÁ≠ñ</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">Âø´Êç∑ÈîÆ</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">ÊÄÄÊè£ÁùÄ‰∏ÄÂÆöÂèØ‰ª•ÂÅöÂ•ΩÁöÑÁ°Æ‰ø°</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">ÊÉÖÁª™ÁöÑÁßòÂØÜ</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">ÊèêÈóÆ</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">ÊîπÁª¥Â∫¶</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">Êï∞ÊçÆÊìç‰Ωú+È¢ÑÂ§ÑÁêÜ</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">ÊòæÂç°</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.63px;">ÊòæÂ≠ò</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">Êô∫ÊÖßÊ†ë</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 13.13px;">Êô∫ËÉΩËÆ°ÁÆóÁ≥ªÁªü</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.63px;">ÊúçÂä°Âô®</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">Êúü‰∏≠Â§ç‰π†</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">ÊúüÊú´</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">Êú±ËÄÅÂ∏à</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">Êú∫Âô®Â≠¶‰π†</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 10px;">ÊùÇÈ°π</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.63px;">ÊùéÂÆèÊØÖ</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">Ê¶ÇËÆ∫</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">Ê®°ÂûãËÆ≠ÁªÉÊµÅÁ®ã</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 13.13px;">ÊØõÊ¶Ç</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.63px;">ÁâπÂæÅÂ≠¶‰π†</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">ÁéØÂ¢ÉÊê≠Âª∫</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">Áî®‰æãÊ®°Âûã</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">Áü•Ë°åÂêà‰∏Ä</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">Áü©ÈòµËÆ°ÁÆó</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">Á¨¨‰∏âÁ´†</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">Á≥ªÁªüÂºÄÂèëÂª∫ËÆÆ‰π¶</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">Á∫øÊÄß‰ª£Êï∞</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">Á∫øÊÄßÂõûÂΩí</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">ËÑëÊú∫Êé•Âè£‰ø°Âè∑Â§ÑÁêÜ</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">Ëá™Âä®Ê±ÇÂØº</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">ËôöÊãüÊú∫</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">ËßÑÂàô</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">Ëß£ÂéãÁº©</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">ËÆ°ÁΩë</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">ËØÑÊµãÊåáÊ†á</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">ËØæÂ†ÇËÆ®ËÆ∫</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">ËØæÁ®ãÊ¶ÇËßà</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">ËØæÁ®ãË°®</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">ËØæËÆæ</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.25px;">Ë∞ÉÁ†î</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">Ë¥°ÁåÆËÄÖ</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">ËΩØ‰ª∂Ê¶ÇË¶ÅËÆæËÆ°</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">ËΩØ‰ª∂ÁîüÂëΩÂë®ÊúüÊ®°Âûã</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">ËæìÂÖ•Ê≥ï</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">Èô∂Áì∑</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">ÈúÄÊ±ÇÂàÜÊûê</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">Èù¢ÂêëÂØπË±°ÁöÑÈúÄÊ±ÇÂàÜÊûêÂª∫Ê®°</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">È¢ÜÂüüÊ®°Âûã</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">ÂΩíÊ°£</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">‰∏ÄÊúà 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">ÂçÅ‰∫åÊúà 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">ÂçÅ‰∏ÄÊúà 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">ÂçÅÊúà 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">‰πùÊúà 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">ÂÖ´Êúà 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">‰∏ÉÊúà 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">ÂÖ≠Êúà 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">‰∫îÊúà 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                „ÅÇ„Åæ„ÅÆ„Å≤„Å™
            </div>
            
                <div>
                    Âü∫‰∫é&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    521.4k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    33:16
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">ÊÄªËÆøÈóÆÈáè&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">ÊÄªËÆøÂÆ¢Èáè&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="„ÅÇ„Åæ„ÅÆ„Å≤„Å™" class="lazyload">
            <div class="sidebar-author-name">„ÅÇ„Åæ„ÅÆ„Å≤„Å™</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>ÊñáÁ´†</div>
                <div class="sidebar-state-number">253</div>
            </div>
            <div class="sidebar-state-category">
                <div>ÂàÜÁ±ª</div>
                <div class="sidebar-state-number">22</div>
            </div>
            <div class="sidebar-state-tag">
                <div>Ê†áÁ≠æ</div>
                <div class="sidebar-state-number">306</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">È¶ñÈ°µ</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">ÂΩíÊ°£</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">ÂÖ≥‰∫é</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">ÂèãÈìæ</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

