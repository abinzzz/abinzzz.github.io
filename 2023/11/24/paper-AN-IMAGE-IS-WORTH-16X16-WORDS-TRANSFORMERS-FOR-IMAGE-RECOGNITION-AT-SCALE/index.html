
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <!--pjax：防止跳转页面音乐暂停-->
    <script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script> 
    <meta charset="utf-8">
    

    

    
    <title>
        paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE |
        
        Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   论文链接 🔗：AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT 🔗：李沐论文精读系列二：Vision Transformer、M">
<meta property="og:type" content="article">
<meta property="og:title" content="paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE">
<meta property="og:url" content="https://abinzzz.github.io/2023/11/24/paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   论文链接 🔗：AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT 🔗：李沐论文精读系列二：Vision Transformer、M">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/F_r7X7PW8AAUeKt?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F_r75OFWQAABJqp?format=png&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_sQE6PWcAA-37V?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F_slGHXXAAAQ8zD?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F_tFQ7AW4AAnFh8?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_tFQ7EW0AAzhax?format=jpg&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F_txvGWXAAAetnz?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_txvGTW4AEru5m?format=jpg&amp;name=large">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUkWsAAoYn0?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUnWkAEqdX1?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUqXcAA2aaT?format=jpg&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F_uDbUmXAAE6U8-?format=jpg&amp;name=small">
<meta property="article:published_time" content="2023-11-24T03:46:20.000Z">
<meta property="article:modified_time" content="2023-11-24T18:35:08.077Z">
<meta property="article:author" content="あまのひな">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="vit">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/F_r7X7PW8AAUeKt?format=jpg&amp;name=medium">
    
        <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
    
        <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/11/24/paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/" class="article-date-link">
        <time datetime="2023-11-24T03:46:20.000Z"
              itemprop="datePublished">2023-11-24</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/paper/">paper</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>

<h2 id="论文链接"><a href="#论文链接" class="headerlink" title="论文链接"></a>论文链接</h2><ul>
<li>🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT</a></li>
<li>🔗：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_56591814/article/details/127358168">李沐论文精读系列二：Vision Transformer、MAE、Swin-Transformer</a></li>
</ul>
<p><br></p>
<h2 id="说在前面"><a href="#说在前面" class="headerlink" title="说在前面"></a>说在前面</h2><p>这篇文章挑战了自从2012年AlexNet提出以来卷积神经网络在计算机视觉里绝对统治的地位。结论是如果在足够多的数据上做预训练，也可以不需要卷积神经网路，直接使用标准的transformer也能够把视觉问题解决的很好。它打破了CV和NLP在模型上的壁垒，开启了CV的一个新时代，推进了多模态领域的发展。</p>
<p><strong>paperswithcode</strong>可以查询现在某个领域或者说某个数据集表现最好的一些方法有哪些。图像分类在ImageNet数据集上排名靠前的全是基于Vision Transformer。</p>
<p>对于目标检测任务在COCO数据集上，排名靠前都都是基于Swin Transformer。Swin Transformer是ICCV 21的最佳论文，可以把它想象成一个多尺度的Vit（Vision Transformer）。</p>
<p>在其他领域（语义分割、实例分割、视频、医疗、遥感），基本上可以说Vision Transformer将整个视觉领域中所有的任务都刷了个遍。</p>
<p><br></p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h2 id="1-1-Vision-Transformer的一些有趣特性"><a href="#1-1-Vision-Transformer的一些有趣特性" class="headerlink" title="1.1 Vision Transformer的一些有趣特性"></a>1.1 Vision Transformer的一些有趣特性</h2><p>作者介绍的另一篇论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.10497.pdf">《Intriguing Properties of Vision Transformer》</a></p>
<p><img src="https://pbs.twimg.com/media/F_r7X7PW8AAUeKt?format=jpg&amp;name=medium" alt=""></p>
<ul>
<li>图a表示的是遮挡，在这么严重的遮挡情况下，不管是卷积神经网络，人眼也很难观察出图中所示的是一只鸟</li>
<li>图b表示数据分布上有所偏移，这里对图片做了一次纹理去除的操作，所以图片看起来比较魔幻</li>
<li>图c表示在鸟头的位置加了一个对抗性的patch</li>
<li>图d表示将图片打散了之后做排列组合</li>
</ul>
<p>上述例子中，卷积神经网络很难判断到底是一个什么物体，但是对于所有的这些例子Vision Transformer都能够处理的很好。</p>
<p><br></p>
<h2 id="1-2-标题"><a href="#1-2-标题" class="headerlink" title="1.2 标题"></a>1.2 标题</h2><p>一张图片等价于很多16<em>16大小的单词。为什么是16</em>16的单词？把图片分割成很多方格patch的形式，每一个方格的大小都是16<em>16，那么这张图片就相当于是很多16</em>16的patch组成的整体</p>
<p><img src="https://pbs.twimg.com/media/F_r75OFWQAABJqp?format=png&amp;name=small" alt=""></p>
<p><br></p>
<h2 id="1-3-摘要"><a href="#1-3-摘要" class="headerlink" title="1.3 摘要"></a>1.3 摘要</h2><p>在VIT之前，self-attention在CV领域的应用很有限，要么和卷积一起使用，要么就是把CNN里面的某些模块替换成self-attention，但是整体架构不变。</p>
<p>这篇文章证明了，在图片分类任务中，只使用纯的Vision Transformer结构直接作用于一系列图像块，也可以取的很好的效果（最佳模型在ImageNet1K上能够达到88.55%的准确率）。尤其是当在大规模的数据上面做预训练然后迁移到中小型数据集（ImageNet、CIFAR-100、VATB）上面使用的时候，Vision Transformer能够获得跟最好的卷积神经网络相媲美的结果。Transformer的另外一个好处：它只需要更少的训练资源，而且表现还特别好。</p>
<p>作者这里指的少的训练资源是指2500天TPUv3的天数。这里的少只是跟更耗卡的模型去做对比。</p>
<p><br></p>
<h2 id="1-4-引言"><a href="#1-4-引言" class="headerlink" title="1.4 引言"></a>1.4 引言</h2><h3 id="Transformer在NLP领域的应用"><a href="#Transformer在NLP领域的应用" class="headerlink" title="Transformer在NLP领域的应用"></a><code>Transformer在NLP领域的应用</code></h3><p>  基于self-attention的模型架构，特别是Transformer，在NLP领域几乎成了必选架构。现在比较主流的方式，就是先去一个大规模的数据集上去做预训练，然后再在一些特定领域的小数据集上面做微调。多亏了Transformer的高效性和可扩展性，现在已经可以训练超过1000亿参数的大模型（GPT3）。随着模型和数据集的增长，还没有看到性能饱和的现象。</p>
<ul>
<li>很多时候不是一味地扩大数据集或者说扩大模型就能够获得更好的效果的，尤其是当扩大模型的时候很容易碰到过拟合的问题，但是对于transformer来说目前还没有观测到这个瓶颈</li>
<li>微软和英伟达联合推出了一个超级大的语言生成模型Megatron-Turing，它已经有5300亿参数了，还能在各个任务上继续大幅度提升性能，没有任何性能饱和的现象</li>
</ul>
<p><br></p>
<h3 id="将transformer运用到视觉领域的难处"><a href="#将transformer运用到视觉领域的难处" class="headerlink" title="将transformer运用到视觉领域的难处"></a><code>将transformer运用到视觉领域的难处</code></h3><p>Transformer在做自注意力的时候是两两互相的，这个计算复杂度是跟序列的长度呈平方倍的。目前一般在自然语言处理中，硬件能支持的序列长度一般也就是几百或者是上千（比如说BERT的序列长度也就是512）。</p>
<p>首先要解决的是如何把一个2D的图片变成一个1D的序列（或者说变成一个集合）。最直观的方式就是把每个像素点当成元素，将图片拉直放进transformer里，看起来比较简单，但是实现起来复杂度较高。</p>
<p>一般来说在视觉中训练分类任务的时候图片的输入大小大概是224<em>224，如果将图片中的每一个像素点都直接当成元素来看待的话，序列长度就是224</em>224=50176个像素点，这个大小就相当于是BERT序列长度的100倍。这还仅仅是分类任务，对于检测和分割，现在很多模型的输入都已经变成600<em>600或者800</em>800或者更大，计算复杂度更高，所以在视觉领域，卷积神经网络还是占主导地位的，比如AlexNet或者是ResNet。</p>
<p><br></p>
<h3 id="将自注意力用到机器视觉的相关工作"><a href="#将自注意力用到机器视觉的相关工作" class="headerlink" title="将自注意力用到机器视觉的相关工作"></a><code>将自注意力用到机器视觉的相关工作</code></h3><p>受NLP启发，很多工作研究如何将自注意力用到机器视觉中。一些工作是说把卷积神经网络和自注意力混到一起用；另外一些工作就是整个将卷积神经网络换掉，全部用自注意力。这些方法其实都是在干一个事情：<strong>因为序列长度太长，所以导致没有办法将transformer用到视觉中，所以就想办法降低序列长度</strong></p>
<p><strong>Non-local Neural Networks</strong>（CVRP，2018）：将网络中间层输出的特征图作为transformer输入序列，降低序列的长度。比如ResNet50在最后一个Stage的特征图size=14×14，把它拉平，序列元素就只有196了，这就在一个可以接受的范围内了。</p>
<p><strong>《Stand-Alone &amp; Self-Attention in Vision Models》</strong>（NeurIPS，2019）：使用孤立注意力Stand-Alone和 Axial-Attention来处理。具体的说，不是输入整张图，而是在一个local window（局部的小窗口）中计算attention。窗口的大小可以控制，复杂度也就大大降低。（类似卷积的操作）</p>
<p><strong>《Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation》</strong>（ECCV，2020a）：</p>
<ul>
<li><strong>孤立自注意力</strong>：不使用整张图，就用一个local window（局部的小窗口），通过控制这个窗口的大小，来让计算复杂度在可接受的范围之内。这就类似于卷积操作（卷积也是在一个局部的窗口中操作的）</li>
<li><strong>轴自注意力</strong>：之所以视觉计算的复杂度高是因为序列长度N=H*W，是一个2D的矩阵，将图片的这个2D的矩阵想办法拆成2个1D的向量，所以先在高度的维度上做一次self-attention（自注意力），然后再在宽度的维度上再去做一次自注意力，相当于把一个在2D矩阵上进行的自注意力操作变成了两个1D的顺序的操作，这样大幅度降低了计算的复杂度  </li>
</ul>
<p><br></p>
<p>这些模型虽然理论上是非常高效的，但事实上这个自注意力操作都是一些比较<strong>特殊的自注意力操作，无法在现在的硬件上进行加速</strong>，所以就导致很难训练出一个大模型。<strong>因此在大规模的图像识别上，传统的残差网络还是效果最好的。</strong></p>
<p>所以，自注意力早已经在计算机视觉里有所应用，而且已经有完全用自注意力去取代卷积操作的工作了。本文是被transformer在NLP领域的可扩展性所启发，直接应用一个标准的transformer作用于图片，尽量做少的修改。</p>
<p><br></p>
<h3 id="vision-transformer如何解决序列长度的问题"><a href="#vision-transformer如何解决序列长度的问题" class="headerlink" title="vision transformer如何解决序列长度的问题"></a><code>vision transformer如何解决序列长度的问题</code></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>vision transformer将一张图片打成了很多的patch</strong>，每一个patch是16*16</li>
<li>假如图片的大小是224<em>224，则sequence lenth（序列长度）就是N=224</em>224=<strong>50176</strong>，如果换成patch，一个patch相当于一个元素的话，有效的长宽就变成了224/16=14，所以最后的序列长度就变成了N=14<em>14=<em>*196</em></em>，对于普通的transformer来说是可以接受的</li>
<li>将每一个patch当作一个元素，通过一个全连接层就会得到一个linear embedding，这些就会当作输入传给transformer。这时候一张图片就变成了一个一个的图片块了，可以将这些图片块当成是NLP中的单词，一个句子中有多少单词就相当于是一张图片中有多少个patch，这就是题目中所提到的一张图片等价于很多16*16的单词</li>
</ul>
<p><br></p>
<h3 id="有监督的训练"><a href="#有监督的训练" class="headerlink" title="有监督的训练"></a><code>有监督的训练</code></h3><p>本文训练vision transformer使用的是有监督的训练。为什么要突出有监督？因为对于NLP来说，transformer基本上都是用无监督的方式训练的，要么是用language modeling，要么是用mask language modeling，都是用的无监督的训练方式。但是对于<strong>视觉</strong>来说，大部分的<strong>基线（baseline）网络还都是用的有监督的训练方式去训练的</strong>.</p>
<p><br></p>
<h3 id="前人最相关的工作"><a href="#前人最相关的工作" class="headerlink" title="前人最相关的工作"></a><code>前人最相关的工作</code></h3><p>本文把视觉当成自然语言处理的任务去做的，尤其是中间的模型就是使用的transformer encoder，跟BERT完全一样。这么简单的想法，之前其实也有人想到过去做，跟本文的工作最像的是一篇ICLR 2020的paper</p>
<ul>
<li>这篇论文是从输入图片中抽取2*2的图片patch</li>
<li>为什么是2<em>2？因为这篇论文的作者只在CIFAR-10数据集上做了实验，而CIFAR-10这个数据集上的图片都是32</em>32的，所以只需要抽取2<em>2的patch就足够了，16</em>16的patch太大了</li>
<li>在抽取好patch之后，就在上面做self-attention</li>
</ul>
<p>从技术上而言这就是Vision Transformer，但是本文的作者认为二者的区别在于，本文的工作证明了如果在大规模的数据集上做预训练的话，那么就能让一个标准的Transformer，不用在视觉上做任何的更改或者特殊的改动，取得比现在最好的卷积神经网络差不多或者还好的结果。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs.</span><br></pre></td></tr></table></figure><br>这篇文章的主要目的就是说，Transformer在Vision领域能够扩展的有多好，就是在超级大数据集和超级大模型两方的加持下，transformer也能在视觉中起到很好的效果</p>
<p><br></p>
<h3 id="ViT和CNN网络使用效果的比较"><a href="#ViT和CNN网络使用效果的比较" class="headerlink" title="ViT和CNN网络使用效果的比较"></a><code>ViT和CNN网络使用效果的比较</code></h3><p>在中型大小的数据集上（比如说ImageNet）上训练的时候，如果不加比较强的约束，ViT的模型其实跟同等大小的残差网络相比要弱一点。</p>
<p>作者对此的解释是：<strong>transformer跟CNN相比，缺少了一些CNN所带有的归纳偏置（inductive bias，是指一种先验知识或者说是一种提前做好的假设）</strong>。</p>
<p>CNN的归纳偏置一般来说有两种：</p>
<ul>
<li><strong>locality</strong>：CNN是以滑动窗口的形式一点一点地在图片上进行卷积的，所以假设图片上相邻的区域会有相邻的特征，靠得越近的东西相关性越强；</li>
<li><strong>translation equivariance（平移等变性）</strong>：写成公式就是f(g(x))=g(f(x))，不论是先做 g 这个函数，还是先做 f 这个函数，最后的结果是不变的；其中f代表卷积操作，g代表平移操作。因为在卷积神经网络中，卷积核就相当于是一个模板，不论图片中同样的物体移动到哪里，只要是同样的输入进来，然后遇到同样的卷积核，那么输出永远是一样的</li>
</ul>
<p><br></p>
<p><strong>一旦神经网络有了这两个归纳偏置之后，他就拥有了很多的先验信息，所以只需要相对较少的数据就可以学习一个相对比较好的模型。但是对于transformer来说，它没有这些先验信息，所以它对视觉的感知全部需要从这些数据中自己学习</strong>。</p>
<p><br></p>
<p>为了验证这个假设， 作者在更大的数据集（ImageNet 22k数据集， 14M个样本&amp;JFT 300M数据集， 300M个样本）上做了预训练，然后<strong>发现在有足够的数据做预训练的情况下，Vit能够获得跟现在最好的残差神经网络相近或者说更好的结果</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks</span><br></pre></td></tr></table></figure><br>上面VTAB也是作者团队所提出来的一个数据集，融合了19个数据集，主要是用来检测模型的稳健性，从侧面也反映出了VisionTransformer的稳健性也是相当不错的。</p>
<p><br></p>
<h2 id="2-结论"><a href="#2-结论" class="headerlink" title="2.结论"></a>2.结论</h2><p>这篇论文的工作是直接拿NLP领域中标准的Transformer来做计算机视觉的问题，跟之前用自注意力的那些工作的区别在于，除了在刚开始抽图像块的时候，还有位置编码用了一些图像特有的归纳偏置，除此之外就再也没有引入任何图像特有的归纳偏置了。这样的好处就是可以直接把图片当做NLP中的token，拿NLP中一个标准的Transformer就可以做图像分类了。</p>
<p>当这个简单而且扩展性很好的策略和大规模预训练结合起来的时候效果出奇的好：Vision Transformer在很多图像分类的benchmark上超过了之前最好的方法，而且训练起来还相对便宜</p>
<p><br></p>
<p>作者对未来的展望：</p>
<ul>
<li><strong>Vit不只做分类，还有检测和分割</strong><ul>
<li><strong>DETR</strong>：<strong>目标检测</strong>的一个力作，相当于是改变了整个目标检测之前的框架</li>
<li>在Vit出现短短的一个半月之后，2020年12月出来了一个叫<strong>Vit-FRCNN</strong>的工作，将Vit用到<strong>检测</strong>上面了</li>
<li>2020年12月有一篇SETR的paper将Vit用到<strong>分割</strong>里了</li>
<li>3个月之后<strong>Swin Transformer</strong>横空出世，它将<strong>多尺度的设计</strong>融合到了Transformer中，更加适合做视觉的问题，真正证明了Transformer是能够当成一个视觉领域的通用骨干网络</li>
</ul>
</li>
<li><strong>探索一下自监督的预训练方案</strong>：因为在NLP领域，所有大的transformer全都是用自监督的方式训练的，Vit这篇paper也做了一些初始实验，证明了用这种自监督的训练方式也是可行的，但是跟有监督的训练比起来还是有不小的差距的</li>
<li><strong>将Vision Transformer变得更大，有可能会带来更好的结果</strong>：过了半年，同样的作者团队又出了一篇paper叫做Scaling Vision Transformer，就是将Transformer变得很大，提出了一个Vit-G，将ImageNet图像分类的准确率提高到了90以上了</li>
</ul>
<p><br></p>
<h2 id="3-相关工作"><a href="#3-相关工作" class="headerlink" title="3.相关工作"></a>3.相关工作</h2><h3 id="transformer在NLP领域的应用"><a href="#transformer在NLP领域的应用" class="headerlink" title="transformer在NLP领域的应用"></a><code>transformer在NLP领域的应用</code></h3><p>自从2017年transformer提出做机器翻译以后，基本上transformer就是很多NLP任务中表现最好的方法。<strong>现在大规模的transformer模型一般都是先在一个大规模的语料库上做预训练，然后再在目标任务上做一些细小的微调</strong>，这当中有两系列比较出名的工作：BERT和GPT。BERT是用一个denoising的自监督方式（其实就是完形填空，将一个句子中某些词划掉，再将这些词预测出来）；GPT用的是language modeling（已经有一个句子，然后去预测下一个词是什么，也就是next word prediction）做自监督。这两个人物其实都是人为定的，语料是固定的，句子也是完整的，只是人为划掉其中的某些部分或者把最后的词拿掉，然后去做完形填空或者是预测下一个词，所以这叫自监督的训练方式.</p>
<p><br></p>
<h3 id="自注意力在视觉中的应用"><a href="#自注意力在视觉中的应用" class="headerlink" title="自注意力在视觉中的应用"></a><code>自注意力在视觉中的应用</code></h3><p>视觉中如果想简单地在图片上使用自注意力，<strong>最简单的方式就是将每一个像素点当成是一个元素，让他们两两做自注意力就好了，但是这个是平方复杂度，所以很难应用到真实的图片输入尺寸上</strong>。像现在分类任务的224*224，一个transformer都很难处理，更不用提人眼看的比较清晰的图片了，一般是1k或者4k的画质，序列长度都是上百万，直接在像素层面使用transformer的话不太现实，所以如果想用transformer就一定得做一些近似</p>
<ul>
<li><strong>复杂度高是因为用了整张图</strong>，所以序列长度长，那么可以不用整张图，就<strong>用local neighborhood（一个小窗口）来做自注意力</strong>，那么序列长度就大大降低了，最后的计算复杂度也就降低了</li>
<li>使用<strong>Sparse Transformer</strong>，就是只对一些<strong>稀疏的点去做自注意力</strong>，所以只是一个全局注意力的近似</li>
<li>将自注意力用到大小不同的block上，或者说在极端的情况下使用<strong>轴注意力</strong>（先在横轴上做自注意力，然后再在纵轴上做自注意力），序列长度也是大大减小的</li>
</ul>
<p>这些特制的自注意力结构其实在计算机视觉上的结果都不错，表现都是没问题的，但是它们需要很复杂的工程去加速算子，虽然在CPU或者GPU上跑得很快或者说让训练一个大模型成为可能</p>
<p><br></p>
<p>跟本文工作最相似的是一篇ICLR2020的论文，区别在于Vision Transformer使用了更大的patch，更大的数据集</p>
<p>在计算机视觉领域还有很多工作是把卷积神经网络和自注意力结合起来的，这类工作相当多，而且基本涵盖了视觉里的很多任务（检测、分类、视频、多模态等）</p>
<p>还有一个工作和本文的工作很相近，叫<strong>imageGPT:</strong></p>
<ul>
<li>GPT是用在NLP中的，是一个生成性的模型。imageGPT也是一个生成性模型，也是用无监督的方式去训练的，和Vit相近的地方在于它也用了transformer</li>
<li><strong>image GPT最终所能达到的效果</strong>：如果将训练好的模型做微调或者就把它当成一个特征提取器，它在ImageNet上的最高的分类准确率也只能到72，Vit最终的结果已经有88.5了，远高于72</li>
<li><strong>但是这个结果也是最近一篇paper叫做MAE爆火的原因</strong>。因为在BEiT和MAE这类工作之前，生成式网络在视觉领域很多任务上是没有办法跟判别式网络相比的，判别式网络往往要比生成式网络的结果高很多，但是MAE做到了，它在ImageNet-1k数据集上训练，用一个生成式的模型，比之前判别式的模型效果好很多，而且不光是在分类任务上，最近发现在目标检测上的迁移学习的效果也非常好</li>
</ul>
<p><br></p>
<p><strong>Vit其实还跟另外一系列工作是有关系的，用比ImageNet更大的数据集去做预训练，这种使用额外数据的方式，一般有助于达到特别好的效果</strong></p>
<ul>
<li>2017年介绍JFT 300数据集的paper研究了卷积神经网络的效果是怎么随着数据集的增大而提高的</li>
<li>一些论文是研究了在更大的数据集（比如说ImageNet-21k和JFT 300M）上做预训练的时候，迁移到ImageNet或者CIFAR-100上的效果如何</li>
</ul>
<p>这篇论文也是聚焦于ImageNet-21k和JFT 300M，但是训练的并不是一个残差网络，而是训练transformer.</p>
<p><br></p>
<h2 id="4-ViT模型"><a href="#4-ViT模型" class="headerlink" title="4.ViT模型"></a>4.ViT模型</h2><h2 id="4-1-整体结构和前向传播"><a href="#4-1-整体结构和前向传播" class="headerlink" title="4.1 整体结构和前向传播"></a>4.1 整体结构和前向传播</h2><p>在模型的设计上尽可能按照最原始的transformer来做的，这样做的好处是transformer在NLP领域已经火了很久了，它有一些非常高效的实现，可以直接拿来使用</p>
<p><img src="https://pbs.twimg.com/media/F_sQE6PWcAA-37V?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<p>简单而言，模型由<strong>三个模块</strong>组成：</p>
<ul>
<li><strong>Embedding层</strong>（线性投射层Linear Projection of Flattened Patches）</li>
<li><strong>Transformer Encoder</strong>(图右侧有给出更加详细的结构)</li>
<li><strong>MLP Head</strong>（最终用于分类的层结构）</li>
</ul>
<p><br></p>
<p><strong>前向传播过程</strong>：</p>
<ul>
<li><strong>Pacth embedding</strong>: 一张图片先分割成n个patchs，然后这些patchs变成序列，每个patch输入线性投射层，得到Pacth embedding。比如ViT-L/16表示每个patchs大小是16×16。       </li>
<li><strong>position embedding</strong>：self-attention本身没有考虑输入的位置信息，无法对序列建模。而图片切成的patches也是有顺序的，打乱之后就不是原来的图片了。于是和transformer一样，引入position embedding。       </li>
<li><strong>class token</strong>：在所有tokens前面加一个新的class token作为这些patchs全局输出，相当于transformer中的CLS（这里的加是concat拼接）。而且它也是有position embedding，位置信息永远是0</li>
<li><strong>Pacth embedding+position embedding+class token</strong>一起输入Transformer Encoder，得到输出。</li>
<li>因为所有的token都在跟其它token做交互信息，所以class embedding能够从别的embedding中学到有用的信息，class token的输出当做整个图片的特征，经过MLP Head得到分类结果（VIT只做分类任务）。最后用交叉熵函数进行模型的训练</li>
</ul>
<p>模型中的Transformer encoder是一个标准的Transformer。整体上来看Vision Transformer的架构还是相当简洁的，<strong>它的特殊之处就在于如何把一个图片变成一系列的token</strong></p>
<p><br></p>
<h2 id="4-2-图片预处理"><a href="#4-2-图片预处理" class="headerlink" title="4.2 图片预处理"></a>4.2 图片预处理</h2><p>标准的Transformer模块要求输入的是token（向量）序列，即<strong>二维矩阵[num_token, token_dim]</strong>。对于图像数据而言，其数据为 <strong>[H, W, C]格式的三维矩阵</strong>，所以需要先通过一个Embedding层来对数据做变换.<br><img src="https://pbs.twimg.com/media/F_slGHXXAAAQ8zD?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<p>首先将一张图片按给定大小分成一堆Patches。以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch尺寸进行划分，划分后会得到196个Patches，每一个图像块的维度就是16<em> 16 </em>3=768</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在代码实现中，直接通过一个卷积层来实现。卷积核大小为16x16，步距为16，卷积核个数为768。通过卷积[224, 224, 3] -&gt; [14, 14, 768]，然后把H以及W两个维度展平即可[14, 14, 768] -&gt; [196, 768]，此时正好变成了一个二维矩阵，正是Transformer想要的。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span></span><br><span class="line">nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br></pre></td></tr></table></figure>
<p><br></p>
<p>接着通过线性映射E将每个Patch映射到一维向量中。这个全连接层的维度是768*768，第二个768就是文章中的D。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">全连接层维度是768*768的原因：每个Patch是一个1616*3的区域(16乘以16乘以3等于768)。</span><br></pre></td></tr></table></figure>
<p>现在得到了<strong>patch embedding，它是一个196*768的矩阵</strong>，即现在有196个token，每个token向量的维度是768，到目前为止就已经成功地将一个vision的问题变成了一个NLP的问题了，输入就是一系列1d的token，而不再是一张2d的图片了</p>
<p>额外的<strong>cls token维度也是768</strong>，这样可以方便和后面图像的信息直接进行拼接。所以最后整体进入Transformer的序列的长度是197*768</p>
<p><strong>Position Embedding是可以学习的，每一个向量代表一个位置信息</strong>（向量的维度是768），将这些位置信息加到所有的token中，序列还是197*768。 </p>
<p><strong>注</strong>：对于位置编码信息，本文用的是标准的可以学习的1d position embedding，它也是BERT使用的位置编码。作者也尝试了了别的编码形式，比如说2d-aware（它是一个能处理2d信息的位置编码），但是最后发现结果其实都差不多，没有什么区别    </p>
<p><br></p>
<Br>

<p>—————————————<strong>图片预处理-example-BEGIN</strong>—————————————<br>图片预处理的模块：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *, image_size, patch_size, dim, pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span></span><br><span class="line"><span class="params">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> image_size % patch_size == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span>  <span class="comment"># 保证一定能够完整切块</span></span><br><span class="line">        num_patches = (image_size // patch_size) ** <span class="number">2</span>  <span class="comment"># 图像patch的个数</span></span><br><span class="line">        patch_dim = channels * patch_size ** <span class="number">2</span>  <span class="comment"># 线性变换时的输入大小，即下面的p1*p2*c</span></span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span>  <span class="comment"># 池化方法必须为cls或者mean</span></span><br><span class="line"> </span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_size, p2 = patch_size), <span class="comment"># 把b张c通道的图像分割成b*（h*w）张大小为p1*p2*c的图像块</span></span><br><span class="line">            nn.Linear(patch_dim, dim),  <span class="comment"># 对分割好的图像块进行线性处理，输入维度为每一个patch的所有像素个数，输出为dim（函数传入的参数）</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))  <span class="comment"># 位置编码，获取一组正态分布的数据用于训练</span></span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))  <span class="comment"># 分类令牌，可训练</span></span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)  <span class="comment"># 切块操作，shape (b, n, dim)，b为批量，n为切块数目，dim为线性操作时输入的神经元个数</span></span><br><span class="line">        b, n, _ = x.shape  <span class="comment"># shape (b, n, 1024)</span></span><br><span class="line"> </span><br><span class="line">        cls_tokens = self.cls_token.repeat([b, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># 将self.cls_token由（1, 1, dim）变为shape (b, 1, dim)</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)  <span class="comment"># 将分类令牌拼接到输入中，x的shape (b, n+1, 1024)</span></span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]  <span class="comment"># 加上位置编码，shape (b, n+1, 1024) 不知道[:, :(n + 1)]是干嘛用的，貌似去掉也行</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x  </span><br></pre></td></tr></table></figure></p>
<p><br></p>
<p>注意，patch embeding使用的不是卷积层，而是用了线性变换，这么做是有优势的。下面分别建立这样的两个网络。输出都是torch.Size([1, 64, 3072])<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">img = torch.randn([<span class="number">1</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>])</span><br><span class="line">net =  nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = <span class="number">32</span>, p2 = <span class="number">32</span>),</span><br><span class="line">            nn.Linear(<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, <span class="number">3072</span>),</span><br><span class="line">        )</span><br><span class="line">out_linear = net(img)</span><br><span class="line"> </span><br><span class="line">patch_embed = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span>, kernel_size=<span class="number">32</span>, stride=<span class="number">32</span>) </span><br><span class="line">)</span><br><span class="line">out_conv = patch_embed(img).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><br><img src="https://pbs.twimg.com/media/F_tFQ7AW4AAnFh8?format=jpg&amp;name=small" alt=""><br>这两个网络的参数个数都是9440256（3072*3073）。卷积层的不方便之处是，embeding的最后一个维度大小实际上是限定死的，如果想要改变维度大小，还要再添加一个线性层。直接使用线性层的好处是，输出的最后一个的设定可以一步到位。</p>
<p>需要额外注意的是，线性层的输入要仔细做reshape，使得每一个patch中的像素在原图像中邻近（卷积做法没有这样的担心）。这里使用了Rearrange，原理是什么，用reshape函数应该怎么做，还没有搞懂</p>
<p><br></p>
<p>测试：</p>
<p>输入形状为[32,3,256,256]的矩阵，每个patch的大小是32，embeding之后最后一维的大小是1024<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = torch.randn([<span class="number">32</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>])</span><br><span class="line">v = Embed(</span><br><span class="line">    image_size = <span class="number">256</span>,</span><br><span class="line">    patch_size = <span class="number">32</span>, </span><br><span class="line">    dim = <span class="number">1024</span>,</span><br><span class="line">    emb_dropout = <span class="number">0.1</span></span><br><span class="line">)</span><br><span class="line">out = v(img)</span><br></pre></td></tr></table></figure><br><img src="https://pbs.twimg.com/media/F_tFQ7EW0AAzhax?format=jpg&amp;name=900x900" alt=""></p>
<p><br></p>
<p>out的形状是:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">32</span>, <span class="number">65</span>, <span class="number">1024</span>])</span><br></pre></td></tr></table></figure></p>
<p>—————————————<strong>图片预处理-example-END</strong>—————————————</p>
<p><br><br><br></p>
<h2 id="4-3-Transformer-Encoder"><a href="#4-3-Transformer-Encoder" class="headerlink" title="4.3 Transformer Encoder"></a>4.3 Transformer Encoder</h2><p><strong>Transformer Encoder</strong>其实就是重复堆叠<strong>Encoder Block</strong> L次。经过预处理，包括特殊的字符cls和位置编码信息，<strong>transformer</strong>输入的<strong>embedded patches</strong>就是一个197*768的tensor。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([]) <span class="comment"># Transformer包含多个编码器的叠加</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            <span class="comment"># 编码器包含两大块：自注意力模块和前向传播模块</span></span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),</span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))</span><br><span class="line">            ]))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = attn(x) + x  <span class="comment"># 自注意力模块和前向传播模块都使用了残差的模式</span></span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<p><br></p>
<ul>
<li><strong>Layer Norm层标准化</strong>：tensor先过一个layer norm，出来之后还是197*768。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)  <span class="comment"># 正则化</span></span><br><span class="line">        self.fn = fn  <span class="comment"># 具体的操作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br></p>
<ul>
<li><strong>Multi-Head Attention</strong>：假设使用的是ViT的base版本，即使用了12个头，那么k、q、v的维度变成了197<em>64（768/12=64），进行12组k、q、v自注意力操作，最后再将12个头的输出拼接起来，输出还是197</em>768<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim) <span class="comment"># 多头注意力或输入和输出维度不相同时为True</span></span><br><span class="line"> </span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span>  <span class="comment"># 缩放操作</span></span><br><span class="line"> </span><br><span class="line">        self.attend = nn.Softmax(dim = -<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"> </span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>) <span class="comment"># 对Q、K、V三组向量线性操作</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 线性全连接，如果多头注意力或输入和输出维度不相同，进行映射，变换维度</span></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)  <span class="comment"># 先对Q、K、V进行线性操作，然后chunk成三份</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = self.heads), qkv) <span class="comment"># 整理维度，获得Q、K、V</span></span><br><span class="line"> </span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># 计算相关性</span></span><br><span class="line"> </span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"> </span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># # Softmax运算结果与Value向量相乘，得到最终结果</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)  <span class="comment"># 重新整理维度</span></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)  <span class="comment"># 做线性的全连接操作或者空操作（空操作直接输出out）</span></span><br></pre></td></tr></table></figure>
torch.chunk(tensor, chunk_num, dim)函数的功能：与torch.cat()刚好相反，它是将tensor按dim（行或列）分割成chunk_num个tensor块，返回的是一个元组。</li>
</ul>
<p><br><br><br></p>
<p><strong>rearrange操作就是调整维度</strong>。比如:</p>
<ul>
<li>rearrange(out, ‘b h n d -&gt; b n (h d)’)等于out.transpose(1, 2).reshape(B, N, C)</li>
<li>q, k, v = map(lambda t: rearrange(t, ‘b n (h d) -&gt; b h n d’, h = self.heads), qkv)可以写成下面<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># qkv(): -&gt; [batch_size, num_patches + 1, 3 * total_embed_dim]</span></span><br><span class="line"><span class="comment"># reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]</span></span><br><span class="line"><span class="comment"># permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">qkv = self.qkv(x).reshape(B, N, <span class="number">3</span>, self.num_heads, total_embed_dim // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span></span><br><span class="line">q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><br><br><br></p>
<p>attention操作的整体流程：</p>
<ul>
<li>首先对输入生成query, key和value，这里的“输入”有可能是整个网络的输入，也可能是某个hidden layer的output。在这里，生成的qkv是个长度为3的元组，每个元组的大小为(1, 65, 1024)</li>
<li>对qkv进行处理，重新指定维度，得到的q, k, v维度均为(1, 16, 65, 64)</li>
<li>q和k做点乘，得到的dots维度为(1, 16, 65, 65)</li>
<li>对dots的最后一维做softmax，得到各个patch对其他patch的注意力得分</li>
<li>将attention和value做点乘</li>
<li>对各个维度重新排列，得到与输入相同维度的输出 (1, 65, 1024)</li>
<li>根据需要，做投射<ul>
<li>Dropout/DropPath：在原论文的代码中是直接使用的Dropout层，在但rwightman实现的代码中使用的是DropPath（stochastic depth），可能后者会更好一点。</li>
<li>再过一层layer norm，还是197*768</li>
<li>MLP Block，全连接+GELU激活函数+Dropout组成。把维度放大到4倍[197, 768] -&gt; [197, 3072]，再还原回原节点个数[197, 3072] -&gt; [197, 768]。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>
<p>进去Transformer block之前是197<em>768，出来还是197</em>768，这个序列的长度和每个token对应的维度大小都是一样的，所以就可以在一个Transformer block上不停地往上叠加Transformer block，最后有L层Transformer block的模型就构成了Transformer encoder</p>
<p>Transformer从头到尾都是使用D当作向量的长度的，都是768，同一个模型里这个维度是不变的。如果transformer变得更大了，D也可以相应的变得更大。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to D dimensions with a trainable linear projection. We refer to the output of this projection as the patch embeddings.</span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="4-4-MLP-Head和ViT-B-16模型结构图"><a href="#4-4-MLP-Head和ViT-B-16模型结构图" class="headerlink" title="4.4 MLP Head和ViT-B/16模型结构图"></a>4.4 MLP Head和ViT-B/16模型结构图</h2><p>对于分类，只需要提取出[class]token生成的对应结果就行，即[197, 768]中抽取出[class]token对应的[1, 768]，通过MLP Head得到最终的分类结果。MLP Head原论文中说在训练ImageNet21K时是由Linear+tanh激活函数+Linear组成，但是迁移到ImageNet1K上或者你自己的数据上时，只定义一个Linear即可。注意，在Transformer Encoder后其实还有一个Layer Norm，</p>
<p><img src="https://pbs.twimg.com/media/F_txvGWXAAAetnz?format=jpg&amp;name=small" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size, patch_size, num_classes, </span></span><br><span class="line"><span class="params">                 dim, depth, heads, mlp_dim, pool = <span class="string">&#x27;cls&#x27;</span>, </span></span><br><span class="line"><span class="params">                 channels = <span class="number">3</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span>, </span></span><br><span class="line"><span class="params">                 emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed = Embed(image_size, patch_size, dim, </span><br><span class="line">                 pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span><br><span class="line">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span>)</span><br><span class="line"> </span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)  <span class="comment"># Transformer模块</span></span><br><span class="line"> </span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()  <span class="comment"># 占位操作</span></span><br><span class="line"> </span><br><span class="line">        self.mlp_head = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),  <span class="comment"># 正则化</span></span><br><span class="line">            nn.Linear(dim, num_classes)  <span class="comment"># 线性输出</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.embed(img)  <span class="comment"># embeding操作</span></span><br><span class="line">        x = self.transformer(x)  <span class="comment"># transformer操作</span></span><br><span class="line"> </span><br><span class="line">        x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)  <span class="comment"># 线性输出</span></span><br></pre></td></tr></table></figure>
<p><br></p>
<h2 id="4-5-数学公式描述"><a href="#4-5-数学公式描述" class="headerlink" title="4.5 数学公式描述"></a>4.5 数学公式描述</h2><p><img src="https://pbs.twimg.com/media/F_txvGTW4AEru5m?format=jpg&amp;name=large" alt=""></p>
<p>（1）Xp​表示图像块的patch，一共有N个patch，E表示线性投影的全连接层，得到一些patch embedding。在它前面拼接一个class embedding（Xclass）。得到所有的tokens后，将位置编码信息Epos也加进去。</p>
<ul>
<li>Z0就是整个transformer的输入。</li>
</ul>
<p><br></p>
<p>（2）-（3）循环</p>
<p>对于每个transformer block来说，里面都有两个操作：一个是多头自注意力，一个是MLP。在做这两个操作之前，都要先经过layer norm，每一层出来的结果都要再去用一个残差连接</p>
<ul>
<li>ZL’就是每一个多头自注意力出来的结果</li>
<li>ZL就是每一个transformer block整体做完之后出来的结果</li>
</ul>
<Br>

<p>4）L层循环结束之后将ZL（最后一层的输出）的第一个位置上的ZL0，也就是class token所对应的输出当作整体图像的特征，去做最后的分类任务</p>
<p><br></p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, einsum</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embed</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size, patch_size, dim, </span></span><br><span class="line"><span class="params">                 pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span></span><br><span class="line"><span class="params">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> image_size % patch_size == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span>  <span class="comment"># 保证一定能够完整切块</span></span><br><span class="line">        num_patches = (image_size // patch_size) ** <span class="number">2</span>  <span class="comment"># 获取图像切块的个数</span></span><br><span class="line">        patch_dim = channels * patch_size ** <span class="number">2</span>  <span class="comment"># 线性变换时的输入大小，即每一个图像宽、高、通道的乘积</span></span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span>  <span class="comment"># 池化方法必须为cls或者mean</span></span><br><span class="line"> </span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_size, p2 = patch_size),  <span class="comment"># 把b张c通道的图像分割成b*（h*w）张大小为P1*p2*c的图像块</span></span><br><span class="line">            nn.Linear(patch_dim, dim),  <span class="comment"># 对分割好的图像块进行线性处理（全连接），输入维度为每一个小块的所有像素个数，输出为dim（函数传入的参数）</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))  <span class="comment"># 位置编码，获取一组正态分布的数据用于训练</span></span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))  <span class="comment"># 分类令牌，可训练</span></span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)  <span class="comment"># 切块操作，shape (b, n, dim)，b为批量，n为切块数目，dim为最终线性操作时输入的神经元个数</span></span><br><span class="line">        b, n, _ = x.shape  <span class="comment"># shape (b, n, 1024)</span></span><br><span class="line"> </span><br><span class="line">        cls_tokens = self.cls_token.repeat([b, <span class="number">1</span>, <span class="number">1</span>])  </span><br><span class="line">        <span class="comment"># 分类令牌，将self.cls_token（形状为1, 1, dim）赋值为shape (b, 1, dim)</span></span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)  <span class="comment"># 将分类令牌拼接到输入中，x的shape (b, n+1, 1024)</span></span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]  <span class="comment"># 进行位置编码，shape (b, n+1, 1024)</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x  </span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)  <span class="comment"># 正则化</span></span><br><span class="line">        self.fn = fn  <span class="comment"># 具体的操作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim) <span class="comment"># 多头注意力或输入和输出维度不相同时为True</span></span><br><span class="line"> </span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span>  <span class="comment"># 缩放操作</span></span><br><span class="line"> </span><br><span class="line">        self.attend = nn.Softmax(dim = -<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"> </span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>) <span class="comment"># 对Q、K、V三组向量线性操作</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 线性全连接，如果多头注意力或输入和输出维度不相同，进行映射，变换维度</span></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)  <span class="comment"># 先对Q、K、V进行线性操作，然后chunk成三份</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = self.heads), qkv) <span class="comment"># 整理维度，获得Q、K、V</span></span><br><span class="line"> </span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale  <span class="comment"># 计算相关性</span></span><br><span class="line"> </span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"> </span><br><span class="line">        out = torch.matmul(attn, v)  <span class="comment"># # Softmax运算结果与Value向量相乘，得到最终结果</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)  <span class="comment"># 重新整理维度</span></span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)  <span class="comment"># 做线性的全连接操作或者空操作（空操作直接输出out）</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])  <span class="comment"># Transformer包含多个编码器的叠加</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            <span class="comment"># 编码器包含两大块：自注意力模块和前向传播模块</span></span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),  <span class="comment"># 多头自注意力模块</span></span><br><span class="line">                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))  <span class="comment"># 前向传播模块</span></span><br><span class="line">            ]))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># 自注意力模块和前向传播模块都使用了残差的模式</span></span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, image_size, patch_size, num_classes, </span></span><br><span class="line"><span class="params">                 dim, depth, heads, mlp_dim, pool = <span class="string">&#x27;cls&#x27;</span>, </span></span><br><span class="line"><span class="params">                 channels = <span class="number">3</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span>, </span></span><br><span class="line"><span class="params">                 emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embed = Embed(image_size, patch_size, dim, </span><br><span class="line">                 pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, </span><br><span class="line">                 dim_head = <span class="number">64</span>, emb_dropout = <span class="number">0.</span>)</span><br><span class="line"> </span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)  <span class="comment"># Transformer模块</span></span><br><span class="line"> </span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()  <span class="comment"># 占位操作</span></span><br><span class="line"> </span><br><span class="line">        self.mlp_head = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),  <span class="comment"># 正则化</span></span><br><span class="line">            nn.Linear(dim, num_classes)  <span class="comment"># 线性输出</span></span><br><span class="line">        )</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.embed(img)  </span><br><span class="line">        x = self.transformer(x)  <span class="comment"># transformer操作</span></span><br><span class="line"> </span><br><span class="line">        x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)  <span class="comment"># 线性输出</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>image_size</strong>：int 类型参数，图片大小。 如果矩形图像，为宽度和高度的最大值</li>
<li><strong>patch_size</strong>：int 类型参数，patch大小。image_size 必须能够被 patch_size整除。n must be greater than 16</li>
<li><strong>num_classes</strong>：int 类型参数，分类数目。</li>
<li><strong>dim</strong>：int 类型参数，embedding的维度。</li>
<li><strong>depth</strong>：int 类型参数，Transformer模块的个数。</li>
<li><strong>heads</strong>：int 类型参数，多头注意力中“头”的个数。</li>
<li><strong>mlp_dim</strong>：int 类型参数，多层感知机中隐藏层的神经元个数。</li>
<li><strong>channels</strong>：int 类型参数，输入图像的通道数，默认为3。</li>
<li><strong>dropout</strong>：float类型参数，Dropout几率，取值范围为[0, 1]，默认为 0.。</li>
<li><strong>emb_dropout</strong>：float类型参数，进行Embedding操作时Dropout几率，取值范围为[0, 1]，默认为0。</li>
<li><strong>pool</strong>：string类型参数，取值为 cls或者 mean 。</li>
</ul>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUkWsAAoYn0?format=jpg&amp;name=small" alt=""></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUnWkAEqdX1?format=jpg&amp;name=small" alt=""></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUqXcAA2aaT?format=jpg&amp;name=small" alt=""></p>
<p><img src="https://pbs.twimg.com/media/F_uDbUmXAAE6U8-?format=jpg&amp;name=small" alt=""></p>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/11/24/paper-AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE/" data-id="clpc3438t00003j69bc9w5hxq" data-title="paper:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/" rel="tag">paper</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vit/" rel="tag">vit</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="d2l:线性回归"
                         class="lazyload">
                
                <a href="/2023/11/25/d2l-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        d2l:线性回归
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="Pytorch:nn.Conv2d"
                         class="lazyload">
                
                <a href="/2023/11/24/Pytorch-nn-Conv2d/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        Pytorch:nn.Conv2d
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="あまのひな" class="lazyload">
            <div class="sidebar-author-name">あまのひな</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">253</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">22</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">306</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=28854246&auto=1&height=66"></iframe>

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Accumulate/">Accumulate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bug/">bug</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/internship/SNN/">SNN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikeBERT/">spikeBERT</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/spikingjelly/">spikingjelly</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Database/">Database</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/ML/">ML</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/Missing-Semester-of-CS/">Missing Semester of CS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/NNDL/">NNDL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/OS/">OS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/SE/">SE</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/d2l/">d2l</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/">智能计算系统</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 10.63px;">1</a> <a href="/tags/11-11/" style="font-size: 10px;">11.11</a> <a href="/tags/17/" style="font-size: 10px;">17</a> <a href="/tags/2/" style="font-size: 11.88px;">2</a> <a href="/tags/2-2/" style="font-size: 10px;">2-2</a> <a href="/tags/3/" style="font-size: 10.63px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/4/" style="font-size: 10.63px;">4</a> <a href="/tags/5/" style="font-size: 10px;">5</a> <a href="/tags/6/" style="font-size: 10px;">6</a> <a href="/tags/7/" style="font-size: 10px;">7</a> <a href="/tags/A4/" style="font-size: 10px;">A4</a> <a href="/tags/A6/" style="font-size: 10px;">A6</a> <a href="/tags/A9/" style="font-size: 11.25px;">A9</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Accumulate/" style="font-size: 12.5px;">Accumulate</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.25px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Ai-Ethics/" style="font-size: 10px;">Ai Ethics</a> <a href="/tags/AimGraduate/" style="font-size: 12.5px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BLIP/" style="font-size: 10px;">BLIP</a> <a href="/tags/BLIP-2/" style="font-size: 10px;">BLIP-2</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 11.88px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10px;">CAS</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CV/" style="font-size: 10.63px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 13.13px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Chapter01/" style="font-size: 10px;">Chapter01</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Database/" style="font-size: 16.25px;">Database</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.63px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Fine-Tuning/" style="font-size: 10px;">Fine-Tuning</a> <a href="/tags/GNN/" style="font-size: 10px;">GNN</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/Git/" style="font-size: 10.63px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.88px;">GoAbroad</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 10.63px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/LLM/" style="font-size: 10px;">LLM</a> <a href="/tags/LMUFORMER/" style="font-size: 10px;">LMUFORMER</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lab3/" style="font-size: 10px;">Lab3</a> <a href="/tags/Lab4/" style="font-size: 10px;">Lab4</a> <a href="/tags/Lec01/" style="font-size: 11.25px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.63px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 11.25px;">Linux</a> <a href="/tags/M2/" style="font-size: 10.63px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 12.5px;">MIT6.S081</a> <a href="/tags/ML/" style="font-size: 12.5px;">ML</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.63px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 10px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/NNDL/" style="font-size: 17.5px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/OS/" style="font-size: 12.5px;">OS</a> <a href="/tags/PSN/" style="font-size: 10px;">PSN</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.63px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/Readme/" style="font-size: 10px;">Readme</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 11.25px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 12.5px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.63px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1-fighting/" style="font-size: 10.63px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/University/" style="font-size: 13.13px;">University</a> <a href="/tags/VSCode/" style="font-size: 10px;">VSCode</a> <a href="/tags/ViT/" style="font-size: 10.63px;">ViT</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.63px;">Yuxiao Dong</a> <a href="/tags/Zero/" style="font-size: 10px;">Zero</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/arxiv/" style="font-size: 10px;">arxiv</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 11.88px;">bert</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/bug/" style="font-size: 14.38px;">bug</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 11.25px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10.63px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10.63px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10.63px;">chapter05</a> <a href="/tags/chatgpt/" style="font-size: 10px;">chatgpt</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/code/" style="font-size: 11.25px;">code</a> <a href="/tags/coding/" style="font-size: 10px;">coding</a> <a href="/tags/commit/" style="font-size: 10px;">commit</a> <a href="/tags/conv2d/" style="font-size: 10px;">conv2d</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/cuda/" style="font-size: 10px;">cuda</a> <a href="/tags/d2l/" style="font-size: 13.13px;">d2l</a> <a href="/tags/database/" style="font-size: 13.75px;">database</a> <a href="/tags/dataloader/" style="font-size: 10px;">dataloader</a> <a href="/tags/debug/" style="font-size: 10px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.63px;">deep neural network</a> <a href="/tags/delete/" style="font-size: 10px;">delete</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/dowhy/" style="font-size: 10.63px;">dowhy</a> <a href="/tags/dp/" style="font-size: 10px;">dp</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/explainer/" style="font-size: 10.63px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/file/" style="font-size: 10px;">file</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/github/" style="font-size: 11.25px;">github</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gpu/" style="font-size: 10.63px;">gpu</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/instructor/" style="font-size: 11.88px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 11.88px;">intern00</a> <a href="/tags/internship/" style="font-size: 18.75px;">internship</a> <a href="/tags/introduction/" style="font-size: 11.25px;">introduction</a> <a href="/tags/iterm2/" style="font-size: 10px;">iterm2</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.63px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/llava/" style="font-size: 10px;">llava</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/loss/" style="font-size: 10px;">loss</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/mac/" style="font-size: 11.25px;">mac</a> <a href="/tags/mentor/" style="font-size: 10px;">mentor</a> <a href="/tags/mid/" style="font-size: 10.63px;">mid</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/mysqlclient/" style="font-size: 10px;">mysqlclient</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.63px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.63px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/nvidia/" style="font-size: 10px;">nvidia</a> <a href="/tags/ohmyzsh/" style="font-size: 10px;">ohmyzsh</a> <a href="/tags/os/" style="font-size: 15.63px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.25px;">p2</a> <a href="/tags/p3/" style="font-size: 10px;">p3</a> <a href="/tags/paper/" style="font-size: 19.38px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/player/" style="font-size: 10px;">player</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 11.88px;">prml</a> <a href="/tags/profile/" style="font-size: 10px;">profile</a> <a href="/tags/pytorch/" style="font-size: 11.88px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/regression/" style="font-size: 10px;">regression</a> <a href="/tags/review/" style="font-size: 15px;">review</a> <a href="/tags/rnn/" style="font-size: 10px;">rnn</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.63px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/shap/" style="font-size: 10px;">shap</a> <a href="/tags/shell/" style="font-size: 10px;">shell</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/solution/" style="font-size: 10px;">solution</a> <a href="/tags/spike/" style="font-size: 10.63px;">spike</a> <a href="/tags/spikeBERT/" style="font-size: 10.63px;">spikeBERT</a> <a href="/tags/spikeBert/" style="font-size: 10px;">spikeBert</a> <a href="/tags/spikingjelly/" style="font-size: 12.5px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.63px;">spikngjelly</a> <a href="/tags/ssh/" style="font-size: 10.63px;">ssh</a> <a href="/tags/test/" style="font-size: 10px;">test</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.63px;">tips</a> <a href="/tags/tool/" style="font-size: 18.13px;">tool</a> <a href="/tags/transformer/" style="font-size: 11.88px;">transformer</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/vit/" style="font-size: 10px;">vit</a> <a href="/tags/vscode/" style="font-size: 10px;">vscode</a> <a href="/tags/wakatime/" style="font-size: 10px;">wakatime</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 20px;">专业知识</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E5%85%AC%E9%80%89%E8%AF%BE/" style="font-size: 10px;">公选课</a> <a href="/tags/%E5%86%85%E5%AD%98/" style="font-size: 10.63px;">内存</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" style="font-size: 10px;">分布式训练</a> <a href="/tags/%E5%8A%A0%E5%88%86/" style="font-size: 10px;">加分</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">动手学深度学习</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90/" style="font-size: 10px;">图像描述生成</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 10px;">基础优化方法</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">复习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 10px;">多模态</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%A4%A7%E4%BD%9C%E4%B8%9A/" style="font-size: 10px;">大作业</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.63px;">审稿意见</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BD%A2%E5%8A%BF%E4%B8%8E%E6%94%BF%E7%AD%96/" style="font-size: 10px;">形势与政策</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%94%B9%E7%BB%B4%E5%BA%A6/" style="font-size: 10px;">改维度</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 10px;">数据操作+预处理</a> <a href="/tags/%E6%98%BE%E5%8D%A1/" style="font-size: 10px;">显卡</a> <a href="/tags/%E6%98%BE%E5%AD%98/" style="font-size: 10.63px;">显存</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 13.13px;">智能计算系统</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 10.63px;">服务器</a> <a href="/tags/%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期中复习</a> <a href="/tags/%E6%9C%9F%E6%9C%AB/" style="font-size: 10px;">期末</a> <a href="/tags/%E6%9C%B1%E8%80%81%E5%B8%88/" style="font-size: 10px;">朱老师</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 10px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10.63px;">李宏毅</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" style="font-size: 10px;">模型训练流程</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 13.13px;">毛概</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0/" style="font-size: 10.63px;">特征学习</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/" style="font-size: 10px;">矩阵计算</a> <a href="/tags/%E7%AC%AC%E4%B8%89%E7%AB%A0/" style="font-size: 10px;">第三章</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 10px;">线性代数</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">线性回归</a> <a href="/tags/%E8%84%91%E6%9C%BA%E6%8E%A5%E5%8F%A3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/" style="font-size: 10px;">脑机接口信号处理</a> <a href="/tags/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/" style="font-size: 10px;">自动求导</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%A7%84%E5%88%99/" style="font-size: 10px;">规则</a> <a href="/tags/%E8%A7%A3%E5%8E%8B%E7%BC%A9/" style="font-size: 10px;">解压缩</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/" style="font-size: 10px;">评测指标</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E6%A6%82%E8%A7%88/" style="font-size: 10px;">课程概览</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 11.25px;">调研</a> <a href="/tags/%E8%B4%A1%E7%8C%AE%E8%80%85/" style="font-size: 10px;">贡献者</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1/" style="font-size: 10px;">软件概要设计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E8%BE%93%E5%85%A5%E6%B3%95/" style="font-size: 10px;">输入法</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a>
        </div>
    </div>


    
        

    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">十二月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    

    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2024
                <span class="footer-info-sep"></span>
                あまのひな
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    521.4k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    33:16
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="あまのひな" class="lazyload">
            <div class="sidebar-author-name">あまのひな</div>
            <div class="sidebar-description"></div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">253</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">22</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">306</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

