
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <meta charset="utf-8">
    

    

    
    <title>
        spikingjelly:Basic Conception |
        
        Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   基本概念  1. 激活值的表示方法 spikingjelly.activation_based 使用取值仅为0或1的张量表示脉冲,例如: 1234567import torchv &#x3D; torch.rand([8]) v">
<meta property="og:type" content="article">
<meta property="og:title" content="spikingjelly:Basic Conception">
<meta property="og:url" content="https://abinzzz.github.io/2023/11/04/spikingjelly/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   基本概念  1. 激活值的表示方法 spikingjelly.activation_based 使用取值仅为0或1的张量表示脉冲,例如: 1234567import torchv &#x3D; torch.rand([8]) v">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://spikingjelly.readthedocs.io/zh-cn/latest/_images/step-by-step1.png">
<meta property="og:image" content="https://spikingjelly.readthedocs.io/zh-cn/latest/_images/layer-by-layer1.png">
<meta property="article:published_time" content="2023-11-03T17:03:36.000Z">
<meta property="article:modified_time" content="2023-11-04T07:38:41.314Z">
<meta property="article:author" content="野中晴">
<meta property="article:tag" content="internship">
<meta property="article:tag" content="spikingjelly">
<meta property="article:tag" content="BasciConception">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://spikingjelly.readthedocs.io/zh-cn/latest/_images/step-by-step1.png">
    
        <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
        <img data-src="https://pbs.twimg.com/media/F-DvzIvWAAAnSrs?format=png&amp;name=900x900" data-sizes="auto" alt="spikingjelly:Basic Conception" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>spikingjelly:Basic Conception</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-spikingjelly" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/11/04/spikingjelly/" class="article-date-link">
        <time datetime="2023-11-03T17:03:36.000Z"
              itemprop="datePublished">2023-11-04</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/internship/">internship</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h2 id="基本概念"><a class="markdownIt-Anchor" href="#基本概念"></a> 基本概念</h2>
<h2 id="1-激活值的表示方法"><a class="markdownIt-Anchor" href="#1-激活值的表示方法"></a> 1. 激活值的表示方法</h2>
<p><code>spikingjelly.activation_based</code> 使用取值仅为0或1的张量表示脉冲,例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">v = torch.rand([<span class="number">8</span>]) </span><br><span class="line">v_th = <span class="number">0.5</span></span><br><span class="line">spike = (v &gt;= v_th).to(v)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;spike =&#x27;</span>, spike)</span><br><span class="line"><span class="comment"># spike = tensor([0., 0., 0., 1., 1., 0., 1., 0.])</span></span><br></pre></td></tr></table></figure>
<br>
<h2 id="2-数据格式"><a class="markdownIt-Anchor" href="#2-数据格式"></a> 2. <strong>数据格式</strong></h2>
<p>在 <code>spikingjelly.activation_based</code> 中,数据有两种格式,分别为:</p>
<ul>
<li>
<p>表示单个时刻的数据,其 <code>shape = [N, *]</code>,其中 <code>N</code> 是batch维度,<code>*</code> 表示任意额外的维度</p>
</li>
<li>
<p>表示多个时刻的数据,其 <code>shape = [T, N, *]</code>,其中 <code>T</code> 是数据的时间维度, <code>N</code> 是batch维度,<code>*</code> 表示任意额外的维度</p>
</li>
</ul>
<br>
<h2 id="3-步进模式"><a class="markdownIt-Anchor" href="#3-步进模式"></a> 3. <strong>步进模式</strong></h2>
<p><code>spikingjelly.activation_based</code> 中的模块,具有两种传播模式,分别是</p>
<ul>
<li>单步模式(single-step)：数据使用 <code>shape = [N, *]</code> 的格式</li>
<li>多步模式(multi-step)：数据使用 <code>shape = [T, N, *]</code> 的格式</li>
</ul>
<br>
<p>模块在初始化时可以指定其使用的步进模式 <code>step_mode</code>,也可以在构建后直接进行修改:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron</span><br><span class="line"></span><br><span class="line">net = neuron.IFNode(step_mode=<span class="string">&#x27;m&#x27;</span>)</span><br><span class="line"><span class="comment"># &#x27;m&#x27; is the multi-step mode </span></span><br><span class="line">net.step_mode = <span class="string">&#x27;s&#x27;</span></span><br><span class="line"><span class="comment"># &#x27;s&#x27; is the single-step mode</span></span><br></pre></td></tr></table></figure>
<br>
<p>如果我们想给单步模式的模块输入 <code>shape = [T, N, *]</code> 的序列数据,通常需要手动做一个时间上的循环,将数据拆成 <code>T</code> 个 <code>shape = [N, *]</code> 的数据并逐步输入进去。让我们新建一层IF神经元,设置为单步模式,将数据逐步输入并得到输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron</span><br><span class="line"></span><br><span class="line">net_s = neuron.IFNode(step_mode=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">T = <span class="number">4</span> <span class="comment">#时间步</span></span><br><span class="line">N = <span class="number">1</span> <span class="comment">#batchsize</span></span><br><span class="line">C = <span class="number">3</span> <span class="comment">#通道数</span></span><br><span class="line">H = <span class="number">8</span> <span class="comment">#数据的高</span></span><br><span class="line">W = <span class="number">8</span> <span class="comment">#数据的宽</span></span><br><span class="line">x_seq = torch.rand([T, N, C, H, W])</span><br><span class="line">y_seq = []</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">    x = x_seq[t]  <span class="comment"># x.shape = [N, C, H, W]</span></span><br><span class="line">    y = net_s(x)  <span class="comment"># y.shape = [N, C, H, W]</span></span><br><span class="line">    y_seq.append(y.unsqueeze(<span class="number">0</span>)) <span class="comment">#将y增加一个维度 添加到列表y_seq中</span></span><br><span class="line"></span><br><span class="line">y_seq = torch.cat(y_seq) <span class="comment">#将所有输出张量沿着新的时间维度拼起来，形成新的张量</span></span><br><span class="line"><span class="comment"># y_seq.shape = [T, N, C, H, W]</span></span><br></pre></td></tr></table></figure>
<br>
<p><code>multi_step_forward</code> 提供了将 <code>shape = [T, N, *]</code> 的序列数据输入到单步模块进行逐步的前向传播的封装,即将上面的函数进行了封装，使用起来更加方便:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron, functional</span><br><span class="line">net_s = neuron.IFNode(step_mode=<span class="string">&#x27;s&#x27;</span>)  </span><br><span class="line">T = <span class="number">4</span></span><br><span class="line">N = <span class="number">1</span></span><br><span class="line">C = <span class="number">3</span></span><br><span class="line">H = <span class="number">8</span></span><br><span class="line">W = <span class="number">8</span></span><br><span class="line">x_seq = torch.rand([T, N, C, H, W])</span><br><span class="line">y_seq = functional.multi_step_forward(x_seq, net_s)</span><br><span class="line"><span class="comment"># y_seq.shape = [T, N, C, H, W]</span></span><br></pre></td></tr></table></figure>
<br>
<p>但是,直接将模块设置成多步模块,其实更为便捷:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron</span><br><span class="line"></span><br><span class="line">net_m = neuron.IFNode(step_mode=<span class="string">&#x27;m&#x27;</span>)</span><br><span class="line">T = <span class="number">4</span>  </span><br><span class="line">N = <span class="number">1</span></span><br><span class="line">C = <span class="number">3</span></span><br><span class="line">H = <span class="number">8</span></span><br><span class="line">W = <span class="number">8</span></span><br><span class="line">x_seq = torch.rand([T, N, C, H, W])</span><br><span class="line">y_seq = net_m(x_seq)</span><br><span class="line"><span class="comment"># y_seq.shape = [T, N, C, H, W] </span></span><br></pre></td></tr></table></figure>
<p>为了保持与老版本SpikingJelly代码的兼容性,所有模块的默认步进模式都是单步。</p>
<BR>
<h2 id="4-状态保存与重置"><a class="markdownIt-Anchor" href="#4-状态保存与重置"></a> 4. 状态保存与重置</h2>
<p>SNN中的神经元等模块,与RNN类似,带有隐藏状态,其输出y[t]不仅仅与当前时刻的输入x[t]有关,还与上一个时末的状态h[t-1]有关,即y[t]=f(x[t],h[t-1])。</p>
<p>PyTorch的设计为RNN将状态也一并输出,可以参考<code>torch.nn.RNN</code>的API文档。而在<code>spikingjelly.activation_based</code>中,状态会被保存在模块内部。例如,我们新建一层IF神经元,设置为单步模式,查看给与输入前的默认电压,和给与输入后的电压:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron</span><br><span class="line"></span><br><span class="line">net_s = neuron.IFNode(step_mode=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">x = torch.rand([<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(net_s)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;the initial v=<span class="subst">&#123;net_s.v&#125;</span>&#x27;</span>)</span><br><span class="line">y = net_s(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;x=<span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;y=<span class="subst">&#123;y&#125;</span>&#x27;</span>)  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;v=<span class="subst">&#123;net_s.v&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># outputs are: </span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">IFNode(</span></span><br><span class="line"><span class="string">v_threshold=1.0, v_reset=0.0, detach_reset=False</span></span><br><span class="line"><span class="string">(surrogate_function): Sigmoid(alpha=4.0, spiking=True)  </span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">the initial v=0.0</span></span><br><span class="line"><span class="string">x=tensor([0.5543, 0.0350, 0.2171, 0.6740])</span></span><br><span class="line"><span class="string">y=tensor([0., 0., 0., 0.])</span></span><br><span class="line"><span class="string">v=tensor([0.5543, 0.0350, 0.2171, 0.6740])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<br>
<p>在初始化后,IF神经元层的<code>v</code>会被设置为0,首次给与输入后<code>v</code>会自动广播到与输入相同的<code>shape</code>。</p>
<p>若我们给与一个新的输入,则应该先清除神经元之前的状态,让其恢复到初始化状态,可以通过调用模块的<code>self.reset()</code>函数实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron</span><br><span class="line"></span><br><span class="line">net_s = neuron.IFNode(step_mode=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">x = torch.rand([<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;check point 0: v=<span class="subst">&#123;net_s.v&#125;</span>&#x27;</span>)</span><br><span class="line">y = net_s(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;check point 1: v=<span class="subst">&#123;net_s.v&#125;</span>&#x27;</span>)  </span><br><span class="line">net_s.reset()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;check point 2: v=<span class="subst">&#123;net_s.v&#125;</span>&#x27;</span>)</span><br><span class="line">x = torch.rand([<span class="number">8</span>])</span><br><span class="line">y = net_s(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;check point 3: v=<span class="subst">&#123;net_s.v&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># outputs are:</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">check point 0: v=0.0</span></span><br><span class="line"><span class="string">check point 1: v=tensor([0.9775, 0.6598, 0.7577, 0.2952]) </span></span><br><span class="line"><span class="string">check point 2: v=0.0</span></span><br><span class="line"><span class="string">check point 3: v=tensor([0.8728, 0.9031, 0.2278, 0.5089, 0.1059, 0.0479, 0.5008, 0.8530])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<br>
<p>方便起见,还可以通过调用<code>spikingjelly.activation_based.functional.reset_net</code>将整个网络中的所有有状态模块进行重置。</p>
<p>若网络使用了有状态的模块,在训练和推理时,务必在处理完毕一个batch的数据后进行重置:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> functional</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="keyword">for</span> x, label <span class="keyword">in</span> tqdm(train_data_loader):</span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  optimizer.zero_grad()</span><br><span class="line">  y = net(x)</span><br><span class="line">  loss = criterion(y, label)</span><br><span class="line">  loss.backward()</span><br><span class="line">  optimizer.step()</span><br><span class="line"></span><br><span class="line">  functional.reset_net(net) <span class="comment">#重置网络状态</span></span><br><span class="line">  <span class="comment"># Never forget to reset the network!</span></span><br></pre></td></tr></table></figure>
<br>
<p>如果忘了重置,在推理时可能输出错误的结果,而在训练时则会直接报错:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). </span><br><span class="line">Saved intermediate values of the graph are freed when you call .backward() or autograd.grad().  </span><br><span class="line">Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward.</span><br></pre></td></tr></table></figure>
<br>
<h2 id="5-传播模式"><a class="markdownIt-Anchor" href="#5-传播模式"></a> 5. 传播模式</h2>
<p>若一个网络全部由单步模块构成,则整个网络的计算顺序是按照逐步传播(step-by-step)的模式进行,例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">    x = x_seq[t]  </span><br><span class="line">    y = net(x) <span class="comment">#将输入数据传递给网络</span></span><br><span class="line">    y_seq_step_by_step.append(y.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">y_seq_step_by_step = torch.cat(y_seq_step_by_step, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<br>
<p>如果网络全部由多步模块构成,则整个网络的计算顺序是按照逐层传播(layer-by-layer)的模式进行,例如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron, functional, layer</span><br><span class="line">T = <span class="number">4</span> <span class="comment">#时间步</span></span><br><span class="line">N = <span class="number">2</span> <span class="comment"># batchsize</span></span><br><span class="line">C = <span class="number">8</span> <span class="comment"># 输入特征数量</span></span><br><span class="line">x_seq = torch.rand([T, N, C]) * <span class="number">64.</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    layer.Linear(C, <span class="number">4</span>),</span><br><span class="line">    neuron.IFNode(),</span><br><span class="line">    layer.Linear(<span class="number">4</span>, <span class="number">2</span>), </span><br><span class="line">    neuron.IFNode()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">functional.set_step_mode(net, step_mode=<span class="string">&#x27;m&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y_seq_layer_by_layer = x_seq</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(net.__len__()):</span><br><span class="line">        y_seq_layer_by_layer = net[i](y_seq_layer_by_layer)</span><br></pre></td></tr></table></figure>
<Br>
<p>在绝大多数情况下我们不需要显式的实现 <code>for i in range(net.__len__())</code> 这样的循环,因为 <code>torch.nn.Sequential</code> 已经帮我们实现过了,因此实际上我们可以这样做:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_seq_layer_by_layer = net(x_seq)</span><br></pre></td></tr></table></figure>
<BR>
<p>逐步传播和逐层传播,实际上只是计算顺序不同,它们的计算结果是完全相同的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> spikingjelly.activation_based <span class="keyword">import</span> neuron, functional, layer</span><br><span class="line">T = <span class="number">4</span></span><br><span class="line">N = <span class="number">2</span>  </span><br><span class="line">C = <span class="number">3</span></span><br><span class="line">H = <span class="number">8</span></span><br><span class="line">W = <span class="number">8</span></span><br><span class="line">x_seq = torch.rand([T, N, C, H, W]) * <span class="number">64.</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">  layer.Conv2d(<span class="number">3</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">  neuron.IFNode(),</span><br><span class="line">  layer.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">  neuron.IFNode(),</span><br><span class="line">  layer.Flatten(start_dim=<span class="number">1</span>),</span><br><span class="line">  layer.Linear(<span class="number">8</span> * H // <span class="number">2</span> * W // <span class="number">2</span>, <span class="number">10</span>),</span><br><span class="line">  neuron.IFNode(),  </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;net=<span class="subst">&#123;net&#125;</span>&#x27;</span>) <span class="comment">#打印网络信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y_seq_step_by_step = []</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        x = x_seq[t]</span><br><span class="line">        y = net(x)</span><br><span class="line">        y_seq_step_by_step.append(y.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    y_seq_step_by_step = torch.cat(y_seq_step_by_step, <span class="number">0</span>)</span><br><span class="line">    <span class="comment"># we can also use `y_seq_step_by_step = functional.multi_step_forward(x_seq, net)` to get the same results</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;y_seq_step_by_step=\n<span class="subst">&#123;y_seq_step_by_step&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    functional.reset_net(net)</span><br><span class="line">    functional.set_step_mode(net, step_mode=<span class="string">&#x27;m&#x27;</span>)  </span><br><span class="line">    y_seq_layer_by_layer = net(x_seq)</span><br><span class="line"></span><br><span class="line">    max_error = (y_seq_layer_by_layer - y_seq_step_by_step).<span class="built_in">abs</span>().<span class="built_in">max</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;max_error=<span class="subst">&#123;max_error&#125;</span>&#x27;</span>) <span class="comment">#表示两次输出是没有差别的</span></span><br></pre></td></tr></table></figure>
<Br>
<p>上面这段代码的输出为:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">net=Sequential(</span><br><span class="line">  (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, step_mode=s)</span><br><span class="line">  (1): IFNode(</span><br><span class="line">      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s</span><br><span class="line">      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)</span><br><span class="line">  )</span><br><span class="line">  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, step_mode=s)</span><br><span class="line">  (3): IFNode(</span><br><span class="line">      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s</span><br><span class="line">      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)</span><br><span class="line">  )</span><br><span class="line">  (4): Flatten(start_dim=1, end_dim=-1, step_mode=s)</span><br><span class="line">  (5): Linear(in_features=128, out_features=10, bias=True)</span><br><span class="line">  (6): IFNode(</span><br><span class="line">      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s</span><br><span class="line">      (surrogate_function): Sigmoid(alpha=4.0, spiking=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">y_seq_step_by_step=  </span><br><span class="line">tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 1., 0., 0., 0., 0., 0., 1., 1., 0.],</span><br><span class="line">         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">         [0., 1., 0., 1., 0., 0., 1., 0., 0., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 1., 0., 0., 0., 0., 1., 0., 1., 0.],</span><br><span class="line">         [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.]]])</span><br><span class="line">max_error=0.0</span><br></pre></td></tr></table></figure>
<br>
<p>下面的图片展示了逐步传播构建计算图的顺序:</p>
<p><img src="https://spikingjelly.readthedocs.io/zh-cn/latest/_images/step-by-step1.png" alt="" /></p>
<Br>
<p>下面的图片展示了逐层传播构建计算图的顺序:</p>
<p><img src="https://spikingjelly.readthedocs.io/zh-cn/latest/_images/layer-by-layer1.png" alt="" /></p>
<br>
<p>SNN的计算图有2个维度,分别是时间步数和网络深度,网络的传播实际上就是生成完整计算图的过程,正如上面的2张图片所示。实际上,逐步传播是深度优先遍历,而逐层传播是广度优先遍历。</p>
<p>尽管两者区别仅在于计算顺序,但计算速度和内存消耗上会略有区别。</p>
<ul>
<li>
<p>在使用梯度替代法训练时,通常推荐使用逐层传播。在正确构建网络的情况下,逐层传播的并行度更大,速度更快</p>
</li>
<li>
<p>在内存受限时使用逐步传播,例如ANN2SNN任务中需要用到非常大的<code>T</code>。因为在逐层传播模式下,对无状态的层而言,真正的batch size是<code>TN</code>而不是<code>N</code>(参见下一个教程),当<code>T</code>太大时内存消耗极大</p>
</li>
</ul>
<Br>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/11/04/spikingjelly/" data-id="clojgsvph0001e36940xe77x6" data-title="spikingjelly:Basic Conception"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BasciConception/" rel="tag">BasciConception</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/internship/" rel="tag">internship</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spikingjelly/" rel="tag">spikingjelly</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://pbs.twimg.com/media/F-A07I-aMAAx1SU?format=jpg&amp;name=medium" data-sizes="auto" alt="spikingjelly:Container"
                         class="lazyload">
                
                <a href="/2023/11/04/spikingjelly-Container/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        spikingjelly:Container
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="intern-02"
                         class="lazyload">
                
                <a href="/2023/11/04/intern-02/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        intern-02
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="野中晴" class="lazyload">
            <div class="sidebar-author-name">野中晴</div>
            <div class="sidebar-description">Love is selfish.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">187</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">10</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">243</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Essay/">Essay</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 10px;">1</a> <a href="/tags/2/" style="font-size: 10px;">2</a> <a href="/tags/3/" style="font-size: 10px;">3</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Advanced-SQL/" style="font-size: 10px;">Advanced SQL</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.54px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/AimGraduate/" style="font-size: 10.77px;">AimGraduate</a> <a href="/tags/An-Overview-of-the-BLITZ-Computer-Hardware/" style="font-size: 10px;">An Overview of the BLITZ Computer Hardware</a> <a href="/tags/An-Overview-of-the-BLITZ-System/" style="font-size: 10px;">An Overview of the BLITZ System</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/BasciConception/" style="font-size: 10px;">BasciConception</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Blitz/" style="font-size: 12.31px;">Blitz</a> <a href="/tags/CAS/" style="font-size: 10px;">CAS</a> <a href="/tags/CAS%E5%AE%9E%E4%B9%A0offer/" style="font-size: 10px;">CAS实习offer</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 13.85px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Container/" style="font-size: 10px;">Container</a> <a href="/tags/Convolutional-SNN-to-Classify-FMNIST/" style="font-size: 10px;">Convolutional SNN to Classify FMNIST</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.77px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Essay/" style="font-size: 20px;">Essay</a> <a href="/tags/GEAR-5/" style="font-size: 10px;">GEAR-5</a> <a href="/tags/Git/" style="font-size: 10.77px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.15px;">GoAbroad</a> <a href="/tags/Gumayusi/" style="font-size: 10px;">Gumayusi</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 11.54px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lec01/" style="font-size: 11.54px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.77px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 10.77px;">Linux</a> <a href="/tags/M2/" style="font-size: 10.77px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 13.08px;">MIT6.S081</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.77px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 10px;">Missing Semester</a> <a href="/tags/Monitor/" style="font-size: 10px;">Monitor</a> <a href="/tags/NNDL/" style="font-size: 11.54px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neural-Network-from-Shallow-to-Deep/" style="font-size: 10px;">Neural Network from Shallow to Deep</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Neuron/" style="font-size: 10px;">Neuron</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.77px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 10px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 11.54px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/SPIKEBERT/" style="font-size: 10px;">SPIKEBERT</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Single-Fully-Connected-Layer-SNN-to-Classify-MNIST/" style="font-size: 10px;">Single Fully Connected Layer SNN to Classify MNIST</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.77px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/SpikingBERT/" style="font-size: 10px;">SpikingBERT</a> <a href="/tags/StarBucks/" style="font-size: 12.31px;">StarBucks</a> <a href="/tags/Surrogate-Gradient-Method/" style="font-size: 10px;">Surrogate Gradient Method</a> <a href="/tags/T1/" style="font-size: 13.08px;">T1</a> <a href="/tags/T1-fighting/" style="font-size: 10px;">T1 fighting</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/The-Thread-Scheduler-and-Concurrency-Control-Primitives/" style="font-size: 10px;">The Thread Scheduler and Concurrency Control Primitives</a> <a href="/tags/University/" style="font-size: 13.85px;">University</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.77px;">Yuxiao Dong</a> <a href="/tags/ai-ethics/" style="font-size: 10px;">ai ethics</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 12.31px;">bert</a> <a href="/tags/bing/" style="font-size: 10px;">bing</a> <a href="/tags/blitz/" style="font-size: 10px;">blitz</a> <a href="/tags/bug/" style="font-size: 10px;">bug</a> <a href="/tags/causal-churn-word/" style="font-size: 10px;">causal churn word</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 10.77px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10.77px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10px;">chapter04</a> <a href="/tags/chapter05/" style="font-size: 10px;">chapter05</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/code/" style="font-size: 11.54px;">code</a> <a href="/tags/coding/" style="font-size: 16.92px;">coding</a> <a href="/tags/cold%F0%9F%98%B7/" style="font-size: 10px;">cold😷</a> <a href="/tags/courseinfo/" style="font-size: 10px;">courseinfo</a> <a href="/tags/dalle3/" style="font-size: 10px;">dalle3</a> <a href="/tags/database/" style="font-size: 14.62px;">database</a> <a href="/tags/debug/" style="font-size: 11.54px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.77px;">deep neural network</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/dowhy/" style="font-size: 10.77px;">dowhy</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/explainer/" style="font-size: 10.77px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/game/" style="font-size: 10px;">game</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gym/" style="font-size: 11.54px;">gym</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/happy/" style="font-size: 10px;">happy</a> <a href="/tags/homework/" style="font-size: 10px;">homework</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/instructor/" style="font-size: 12.31px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 12.31px;">intern00</a> <a href="/tags/internship/" style="font-size: 17.69px;">internship</a> <a href="/tags/introduction/" style="font-size: 11.54px;">introduction</a> <a href="/tags/kfc/" style="font-size: 10px;">kfc</a> <a href="/tags/knowledge-distillaion/" style="font-size: 10px;">knowledge distillaion</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/l3/" style="font-size: 10px;">l3</a> <a href="/tags/lab1/" style="font-size: 10px;">lab1</a> <a href="/tags/lab2/" style="font-size: 10.77px;">lab2</a> <a href="/tags/lec01/" style="font-size: 10px;">lec01</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/m/" style="font-size: 10px;">m</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.77px;">neuromorphic computing</a> <a href="/tags/nndl/" style="font-size: 10.77px;">nndl</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/one-piece/" style="font-size: 10px;">one piece</a> <a href="/tags/openai/" style="font-size: 10px;">openai</a> <a href="/tags/os/" style="font-size: 15.38px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/overview/" style="font-size: 10px;">overview</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 11.54px;">p2</a> <a href="/tags/paper/" style="font-size: 18.46px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 12.31px;">prml</a> <a href="/tags/pytorch/" style="font-size: 10px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/question/" style="font-size: 10px;">question</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/redemption/" style="font-size: 10px;">redemption</a> <a href="/tags/research-vs-coursework/" style="font-size: 10px;">research vs coursework</a> <a href="/tags/rsa/" style="font-size: 10px;">rsa</a> <a href="/tags/se/" style="font-size: 15.38px;">se</a> <a href="/tags/self-attention/" style="font-size: 10px;">self-attention</a> <a href="/tags/shap/" style="font-size: 11.54px;">shap</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/simple/" style="font-size: 10px;">simple</a> <a href="/tags/spike/" style="font-size: 10.77px;">spike</a> <a href="/tags/spikingjelly/" style="font-size: 13.08px;">spikingjelly</a> <a href="/tags/spikngjelly/" style="font-size: 10.77px;">spikngjelly</a> <a href="/tags/starbucks/" style="font-size: 10px;">starbucks</a> <a href="/tags/tensor-vs-ndarray/" style="font-size: 10px;">tensor vs ndarray</a> <a href="/tags/third-place/" style="font-size: 10px;">third place</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.77px;">tips</a> <a href="/tags/tool/" style="font-size: 15.38px;">tool</a> <a href="/tags/transformer/" style="font-size: 11.54px;">transformer</a> <a href="/tags/uml/" style="font-size: 10px;">uml</a> <a href="/tags/wbg%E8%AF%AD%E9%9F%B3-uzi/" style="font-size: 10px;">wbg语音-uzi</a> <a href="/tags/word/" style="font-size: 10px;">word</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/youth/" style="font-size: 10px;">youth</a> <a href="/tags/zero/" style="font-size: 10px;">zero</a> <a href="/tags/zeus/" style="font-size: 10px;">zeus</a> <a href="/tags/%E4%B8%83%E5%A4%95/" style="font-size: 10px;">七夕</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 19.23px;">专业知识</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E5%86%8D%E4%B9%9F%E4%B8%8D%E7%94%A8%E8%A2%AB%E7%BA%A6%E5%AE%9A%E6%9D%9F%E7%BC%9A%E4%BA%86/" style="font-size: 10px;">再也不用被约定束缚了</a> <a href="/tags/%E5%86%8D%E8%A7%81%E7%BB%98%E6%A2%A8/" style="font-size: 10px;">再见绘梨</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%8D%9A%E4%BA%BA%E4%BC%A0/" style="font-size: 10px;">博人传</a> <a href="/tags/%E5%8F%AA%E8%A6%81%E6%9C%89%E7%9C%9F%E5%BF%83%E5%96%9C%E6%AC%A2%E7%9A%84%E4%B8%9C%E8%A5%BF-%E5%B0%B1%E8%83%BD%E5%8F%91%E5%87%BA%E5%85%89%E6%9D%A5/" style="font-size: 10px;">只要有真心喜欢的东西,就能发出光来</a> <a href="/tags/%E5%93%88%E5%B8%8C%E5%80%BC/" style="font-size: 10px;">哈希值</a> <a href="/tags/%E5%9C%A3%E5%A2%83/" style="font-size: 10px;">圣境</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.77px;">审稿意见</a> <a href="/tags/%E5%BC%BA%E5%BC%B1com/" style="font-size: 10px;">强弱com</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%80%80%E6%8F%A3%E7%9D%80%E4%B8%80%E5%AE%9A%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%A5%BD%E7%9A%84%E7%A1%AE%E4%BF%A1/" style="font-size: 10px;">怀揣着一定可以做好的确信</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%84%9F%E5%86%92/" style="font-size: 10px;">感冒</a> <a href="/tags/%E6%84%9F%E5%86%92%E7%97%8A%E6%84%88/" style="font-size: 10px;">感冒痊愈</a> <a href="/tags/%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84/" style="font-size: 10px;">拓扑结构</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%90%AC%E5%AE%B6/" style="font-size: 10px;">搬家</a> <a href="/tags/%E6%94%BE%E4%B8%8B/" style="font-size: 10px;">放下</a> <a href="/tags/%E6%95%99%E5%B8%88%E8%8A%82/" style="font-size: 10px;">教师节</a> <a href="/tags/%E6%99%BA%E6%85%A7%E6%A0%91/" style="font-size: 10px;">智慧树</a> <a href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/" style="font-size: 10px;">智能计算系统</a> <a href="/tags/%E6%9C%80%E9%95%BF%E7%9A%84%E7%94%B5%E5%BD%B1/" style="font-size: 10.77px;">最长的电影</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 10px;">杂项</a> <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/" style="font-size: 10px;">李宏毅</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 13.85px;">毛概</a> <a href="/tags/%E6%B2%88%E6%9C%88/" style="font-size: 10px;">沈月</a> <a href="/tags/%E6%B2%A1%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95/" style="font-size: 10px;">没那么简单</a> <a href="/tags/%E7%81%AB%E9%BE%99%E5%A4%A7%E7%82%AC/" style="font-size: 10px;">火龙大炬</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%94%A8%E4%BE%8B%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">用例模型</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E8%8E%93/" style="font-size: 10px;">莓</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%BE%E5%A0%82%E8%AE%A8%E8%AE%BA/" style="font-size: 10px;">课堂讨论</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%AF%BE%E8%AE%BE/" style="font-size: 10px;">课设</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 10.77px;">调研</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/" style="font-size: 10px;">需求分析</a> <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">面向对象的需求分析建模</a> <a href="/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">领域模型</a> <a href="/tags/%F0%9F%93%A6/" style="font-size: 10px;">📦</a> <a href="/tags/%F0%9F%9B%80/" style="font-size: 10px;">🛀</a>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">十一月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2023
                <span class="footer-info-sep"></span>
                野中晴
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    361.1k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    22:46
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="野中晴" class="lazyload">
            <div class="sidebar-author-name">野中晴</div>
            <div class="sidebar-description">Love is selfish.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">187</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">10</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">243</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

