<!DOCTYPE html>

<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <!--
        hexo-theme-suka © SukkaW
        GitHub: https://github.com/SukkaW/hexo-theme-suka
    -->

    <!-- ### Resource Hint ### -->

    <!-- ## DNS Prefetch ## -->
    <meta http-equiv="x-dns-prefetch-control" content="on">

<!-- busuanzi -->

    <link rel="dns-prefetch" href="//busuanzi.ibruce.info">


<!-- comment -->







<!-- analytics -->







    <!-- ## Preload ## -->
    
    <!-- Busuanzi -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" as="script">







    <!-- ### Meta & Title & Info ### -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <meta name="renderer" content="webkit">

    <!-- Title -->
    <title>pytorch面经 | blog</title>

    <!-- Favicons -->
    <link rel="icon" type="image&#x2F;ico" href="/img/bot.ico">

    <!-- ### Import File ### -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spectre.css@0.5.3"><style>
    body {
        background-color: #f8f9fa;
    }

    a, a:visited {
        color: blue;
    }

    a:active, a:focus, a:hover {
        color: blue;
        opacity: .75;
    }

    #post-content a,
    #post-content a:hover,
    #post-content a:focus,
    #post-content a:visited {
        color: blue;
        opacity: 1;
    }

    

    .post-entry .card-body a {
        color: red;
    }

    .avatar {
        background: red;
    }

    .navbar-link,
    .navbar-link:visited,
    .timeline .timeline-item .timeline-icon.icon-lg {
        color: red;
    }

    .navbar-link:hover {
        color: red;
        opacity: .8;
    }

    #search-input .btn,
    #disqus_click_btn,
    #disqus-switch-to-direct,
    #disqus-loadmore-button {
        background: red;
        border-color: red;
        color: #fff;
    }

    #post-toc a.post-toc-link,
    #post-toc a.post-toc-link:visited,
    .share-menu.menu .menu-item>a {
        color: red;
    }

    .share-menu.menu .menu-item>a:hover,
    .share-menu.menu .menu-item>a:focus,
    .share-menu.menu .menu-item>a:visited {
        color: #50596c;
        background: #f8f9fa;
        opacity: .85;
    }
</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.4.0/source/css/style.min.css">








    <!-- Prettify Theme -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.4.0/source/css/highlight/[theme-name].min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.4.0/source/css/highlight/[theme-name].min.css"></noscript>





<script>
/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
!function(t){"use strict";t.loadCSS||(t.loadCSS=function(){});var e=loadCSS.relpreload={};if(e.support=function(){var e;try{e=t.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),e.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.addEventListener?t.removeEventListener("load",a):t.attachEvent&&t.detachEvent("onload",a),t.setAttribute("onload",null),t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},e.poly=function(){if(!e.support())for(var a=t.document.getElementsByTagName("link"),n=0;n<a.length;n++){var o=a[n];"preload"!==o.rel||"style"!==o.getAttribute("as")||o.getAttribute("data-loadcss")||(o.setAttribute("data-loadcss",!0),e.bindMediaToggle(o))}},!e.support()){e.poly();var a=t.setInterval(e.poly,500);t.addEventListener?t.addEventListener("load",function(){e.poly(),t.clearInterval(a)}):t.attachEvent&&t.attachEvent("onload",function(){e.poly(),t.clearInterval(a)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:t.loadCSS=loadCSS}("undefined"!=typeof global?global:this);
</script>

    <!-- ### Site Verification ### -->
    


    <meta name="mobile-web-app-capable" content="yes"><meta name="application-name" content="blog"><meta name="msapplication-starturl" content="https://abinzzz.github.io"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="blog">

    <!-- ### The Open Graph & Twitter Card Protocol ### -->
    <meta property="og:title" content="pytorch面经 | blog"><meta property="og:site_name" content="blog"><meta property="og:type" content="article"><meta property="og:url" content="https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/"><meta property="og:locale" content="zh-CN"><meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;, &amp;apos;$&amp;apos;]]}, messageStyle: &quot;none&quot; });   1.conv2d的参数以及含义  1.in_channels (int) – 输入图像中的通道数 2.out_channels (int) – 由卷积产生的通道数&#x3D;卷积核的数量 3.kernel_size (int or tupl - ab - blog"><meta name="keywords" content="internship, pytorch, blog"><meta property="article:published_time" content="2023-09-25T02:31:25.000Z"><meta property="article:modified_time" content="2023-09-25T07:24:07.905Z"><meta property="og:updated_time" content="2023-09-25T07:24:07.905Z"><meta property="article:author" content="ab"><meta property="article:tag" content="internship, pytorch, blog"><meta name="twitter:card" content="summary">

    

    <!-- ### Canonical link ### -->
    <link rel="canonical" href="https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/">

    <meta name="generator" content="Hexo 5.4.2">

    <!-- ### Analytics ### -->
    







    <!-- ### Structured Data ### -->
    



<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/",
    "@type": "BlogPosting",
    "logo": "https://abinzzz.github.io/img/bot.ico",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/"
    },
    "headline": "pytorch面经 | blog",
    
    "image": {
        "@type": "ImageObject",
        "url": "https://abinzzz.github.io/img/bot.ico"
    },
    
    "datePublished": "2023-09-25T02:31:25.000Z",
    "dateModified": "2023-09-25T07:24:07.905Z",
    "author": {
        "@type": "Person",
        "name": "ab",
        "image": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/avatar.jpg"
        },
        "description": "Welcome to my blog!"
    },
    "publisher": {
        "@type": "Organization",
        "name": "blog",
        "logo": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/bot.ico"
        }
    },
    
    "potentialAction": {
        "@type": "SearchAction",
        "target": "https://abinzzz.github.io/search?s={search_term_string}",
        "query-input": "required name=search_term_string"
    },
    
    "keywords": "internship, pytorch, blog",
    "description": "MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;, &amp;apos;$&amp;apos;]]}, messageStyle: &amp;quot;none&amp;quot; });   1.conv2d的参数以及含义  1.in_channels (int) – 输入图像中的通道数 2.out_channels (int) – 由卷积产生的通道数=卷积核的数量 3.kernel_size (int or tupl - ab - blog"
}
</script>



    <!-- ### Custom Head ### -->
    
</head>

    <body>
            

            <!-- ### Main content ### -->
            <!-- ## Header ##-->
<header>
    <h1 class="header-title text-center"><a href="/">blog</a></h1>

    <p class="text-center header-slogan">
        
            
                Welcome to my blog!
            
        
    </p>

    <nav class="navbar-section text-center">
    
        <a href="/" class="navbar-link">首页</a>
    
    
        <a href="/archives/" class="navbar-link">归档</a>
    
    
        <a href="/search" class="navbar-link">搜索</a>
    
    
    
        <div class="dropdown dropdown-right">
    <a class="navbar-link dropdown-toggle" tabindex="0">分享</a>
    <ul class="menu share-menu">

        <!-- Share Weibo -->
        
        <li class="menu-item">
            <a href="http://service.weibo.com/share/share.php?appkey=&title=blog&url=https://abinzzz.github.io&pic=https://abinzzz.github.io/img/bot.ico&searchPic=false&style=simple" target="_blank" rel="external noopener noreferrer nofollow">分享到微博</a>
        </li>
        

        <!-- Share Twitter -->
        
        <li class="menu-item">
            <a href="https://twitter.com/intent/tweet?text=blog&url=https://abinzzz.github.io&via=ab" target="_blank" rel="external noopener noreferrer nofollow">分享到 Twitter</a>
        </li>
        

        <!-- Share Facebook -->
        
        <li class="menu-item">
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://abinzzz.github.io" target="_blank" rel="external noopener noreferrer nofollow">分享到 Facebook</a>
        </li>
        

        <!-- Share Google+ -->
        
        <li class="menu-item">
            <a href="https://plus.google.com/share?url=https://abinzzz.github.io" target="_blank" rel="external noopener noreferrer nofollow">分享到 Google+</a>
        </li>
        

        <!-- Share LinkedIn -->
        
        <li class="menu-item">
            <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://abinzzz.github.io&title=pytorch面经" target="_blank" rel="external noopener noreferrer nofollow">分享到 LinkedIn</a>
        </li>
        

        <!-- Share QQ -->
        
        <li class="menu-item">
            <a href="http://connect.qq.com/widget/shareqq/index.html?site=blog&title=pytorch面经&summary=&pics=https://abinzzz.github.io/img/bot.ico&url=https://abinzzz.github.io" target="_blank" rel="external noopener noreferrer nofollow"> 分享到 QQ</a>
        </li>
        

        <!-- Share Telegram -->
        
        <li class="menu-item">
            <a href="https://t.me/share/url?url=https://abinzzz.github.io&text=pytorch面经" target="_blank" rel="external noopener noreferrer nofollow">分享到 Telegram</a>
        </li>
        

        <!-- QRCode -->
        
        <li class="menu-item">
            <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKQAAACkCAAAAAA83tqdAAACO0lEQVR42u2cQW6EQAwE+f+nk/OuBnd5QpTNuLigAGEKCZt229rr6x9sl5BCCinkB0BexbY6n469LFKcT+sKORty+dIubv6+X91jtVh1/+q4kLMhV4HQ2a+C5e5vsq6QQpYXQzCS5IUU8onAIcFxd+1d8hZSyF2BUQVIErckCB9RQUIeB5mS8W/tH60WhTwGEhlHRcKtrquC7XFXTchjIImJVC1YCZIdcRK/OEIeD1n9YzI/afB0izchhUyiNJkDpNDCYFXgCDkOsls4EVMgBU5Z4Ak5ErIq7MmLTg386iOBDCshx0Cm5mcSr8RQTcn79sGFHAnZNUWJ8EgBuGXsCzkKkooAEljkflWTVEghU5LtDDd1TCqS1IWcCUkb5lVwkHPVQ98eF3IkJGm+k6FiKnppg0DI2ZDkZlXjkzRPqySOB0WEHAPZERApsHaKs1ZTXsjjIVPDkzxIlaBT4JXnhBwJSUVAGuJMhRyFb09HC3kcZGoIkRediFkqfluDIkIeD0nM/Z2GU6dRhScHhDwWsttop40pashigSHkOMhq4R3h0BkyueUQciwkLbrItamBv90RE3IcJBGnpODvDHNi0SvkKEhizO+Y8t3gi015IcdAphe5LJQu9mMHlchtdcSEHANJA4YOthPTnwSbkLMh02Bw12DdHWZqdx+EHAmZzChSwKWPAi7EhBwP2TH2kyHQNQ2EnA1Jh0BIgqbCJQllIedCdgbksGDdGNT7cbUo5BGQn7wJKaSQQv7h9g0OgBVTD808WQAAAABJRU5ErkJggg==" alr="QRCode">
        </li>
        

    </ul>
</div>
    
    
</nav>
</header>

            
    <!-- ## Post ## -->
    <div class="post-container">
    <div id="post-card" class="card">
        
        <div class="card-item-container">
            <div class="card-inner-cell">
                <!-- # Post Header Info # -->
                <div class="card-header">
                    
    <h1 class="card-title h3 mb-2">pytorch面经</h1>




<div class="post-header-info">
    <p class="post-header-info-left text-gray">
        <img class="author-thumb lazyload" data-src="/img/avatar.jpg" src="/img/suka-lazyload.gif" alt="ab's Avatar">
        <span>2023-09-25</span>
        
            <span class="suka-devide-dot"></span>
            <a class="category-link" href="/categories/internship/">internship</a>
        
        
            <!-- Busuanzi Post Views -->
<span id="busuanzi_container_page_pv" hidden>
    <span class="suka-devide-dot"></span>
    <span></span>
    <span id="busuanzi_value_page_pv"></span>
    <span>Views</span>
</span>
        
        
    </p>
    <div class="post-header-info-right">
        
            <div class="dropdown dropdown-right">
<a class="dropdown-toggle" tabindex="0">分享本文</a>
<ul class="menu share-menu">
    <!-- Share Weibo -->
    
    <li class="menu-item">
        <a href="http://service.weibo.com/share/share.php?appkey=&title=pytorch面经&url=https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/&pic=https://abinzzz.github.io/img/bot.ico&searchPic=false&style=simple" target="_blank" rel="external noopener noreferrer nofollow">分享到微博</a>
    </li>
    

    <!-- Share Twitter -->
    
    <li class="menu-item">
        <a href="https://twitter.com/intent/tweet?text=pytorch面经&url=https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/&via=ab" target="_blank" rel="external noopener noreferrer nofollow">分享到 Twitter</a>
    </li>
    

    <!-- Share Facebook -->
    
    <li class="menu-item">
        <a href="https://www.facebook.com/sharer/sharer.php?u=https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/" target="_blank" rel="external noopener noreferrer nofollow">分享到 Facebook</a>
    </li>
    

    <!-- Share Google+ -->
    
    <li class="menu-item">
        <a href="https://plus.google.com/share?url=https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/" target="_blank" rel="external noopener noreferrer nofollow">分享到 Google+</a>
    </li>
    

    <!-- Share LinkedIn -->
    
    <li class="menu-item">
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/&title=blog" target="_blank" rel="external noopener noreferrer nofollow">分享到 LinkedIn</a>
    </li>
    

    <!-- Share QQ -->
    
    <li class="menu-item">
        <a href="http://connect.qq.com/widget/shareqq/index.html?site=blog&title=blog&summary=&pics=https://abinzzz.github.io/img/bot.ico&url=https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/" target="_blank" rel="external noopener noreferrer nofollow"> 分享到 QQ</a>
    </li>
    

    <!-- Share Telegram -->
    
    <li class="menu-item">
        <a href="https://t.me/share/url?url=https://abinzzz.github.io/2023/09/25/pytorch%E9%9D%A2%E7%BB%8F/&text=blog" target="_blank" rel="external noopener noreferrer nofollow">分享到 Telegram</a>
    </li>
    

    <!-- QRCode -->
    
    <li class="menu-item">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKQAAACkCAAAAAA83tqdAAACO0lEQVR42u2cQW6EQAwE+f+nk/OuBnd5QpTNuLigAGEKCZt229rr6x9sl5BCCinkB0BexbY6n469LFKcT+sKORty+dIubv6+X91jtVh1/+q4kLMhV4HQ2a+C5e5vsq6QQpYXQzCS5IUU8onAIcFxd+1d8hZSyF2BUQVIErckCB9RQUIeB5mS8W/tH60WhTwGEhlHRcKtrquC7XFXTchjIImJVC1YCZIdcRK/OEIeD1n9YzI/afB0izchhUyiNJkDpNDCYFXgCDkOsls4EVMgBU5Z4Ak5ErIq7MmLTg386iOBDCshx0Cm5mcSr8RQTcn79sGFHAnZNUWJ8EgBuGXsCzkKkooAEljkflWTVEghU5LtDDd1TCqS1IWcCUkb5lVwkHPVQ98eF3IkJGm+k6FiKnppg0DI2ZDkZlXjkzRPqySOB0WEHAPZERApsHaKs1ZTXsjjIVPDkzxIlaBT4JXnhBwJSUVAGuJMhRyFb09HC3kcZGoIkRediFkqfluDIkIeD0nM/Z2GU6dRhScHhDwWsttop40pashigSHkOMhq4R3h0BkyueUQciwkLbrItamBv90RE3IcJBGnpODvDHNi0SvkKEhizO+Y8t3gi015IcdAphe5LJQu9mMHlchtdcSEHANJA4YOthPTnwSbkLMh02Bw12DdHWZqdx+EHAmZzChSwKWPAi7EhBwP2TH2kyHQNQ2EnA1Jh0BIgqbCJQllIedCdgbksGDdGNT7cbUo5BGQn7wJKaSQQv7h9g0OgBVTD808WQAAAABJRU5ErkJggg==" alt="QRCode">
    </li>
    

</ul>
</div>
        
    </div>
</div>
                </div>
                <div class="card-body">
                    
                        
                        
                            <div id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1conv2d%E7%9A%84%E5%8F%82%E6%95%B0%E4%BB%A5%E5%8F%8A%E5%90%AB%E4%B9%89"><span class="post-toc-number">1.</span> <span class="post-toc-text"> 1.conv2d的参数以及含义</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2pytorch%E5%A6%82%E4%BD%95%E5%BE%AE%E8%B0%83fine-tuning"><span class="post-toc-number">2.</span> <span class="post-toc-text"> 2.pytorch如何微调fine tuning</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3pytorch%E4%BD%BF%E7%94%A8%E5%A4%9Agpu"><span class="post-toc-number">3.</span> <span class="post-toc-text"> 3.pytorch使用多gpu</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4torchnn"><span class="post-toc-number">4.</span> <span class="post-toc-text"> 4.torch.nn</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5sequential%E7%9A%84%E4%B8%89%E7%A7%8D%E5%86%99%E6%B3%95"><span class="post-toc-number">5.</span> <span class="post-toc-text"> 5.Sequential的三种写法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6tips"><span class="post-toc-number">6.</span> <span class="post-toc-text"> 6.tips</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7torchoptim"><span class="post-toc-number">7.</span> <span class="post-toc-text"> 7.torch.optim</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#8%E5%AF%B9%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE%E4%B8%8D%E5%90%8C%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="post-toc-number">8.</span> <span class="post-toc-text"> 8.对于不同网络设置不同的学习率</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#9%E4%BF%AE%E6%94%B9%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E6%96%B9%E6%B3%95"><span class="post-toc-number">9.</span> <span class="post-toc-text"> 9.修改学习率的方法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#10nnfunctional%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E5%92%8Cnnmodule%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB"><span class="post-toc-number">10.</span> <span class="post-toc-text"> 10.nn.functional中的函数和nn.Module主要区别</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#11%E5%B0%86module%E6%94%BE%E5%9C%A8gpu%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%8F%AA%E9%9C%80%E4%B8%A4%E6%AD%A5%E5%88%86%E5%88%AB%E5%B0%86%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%94%BE%E5%9C%A8gpu%E4%B8%8A"><span class="post-toc-number">11.</span> <span class="post-toc-text"> 11.将Module放在gpu上运行只需两步：分别将模型与数据放在gpu上</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#12%E5%9C%A8%E5%A4%9A%E4%B8%AAgpu%E4%B8%8A%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="post-toc-number">12.</span> <span class="post-toc-text"> 12.在多个gpu上并行计算</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#13torchvision"><span class="post-toc-number">13.</span> <span class="post-toc-text"> 13.torchvision，</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#14pil-image%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="post-toc-number">14.</span> <span class="post-toc-text"> 14.PIL Image的操作</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#15imagefolder%E5%81%87%E8%AE%BE%E6%89%80%E6%9C%89%E7%9A%84%E6%96%87%E4%BB%B6%E6%8C%89%E6%96%87%E4%BB%B6%E5%A4%B9%E4%BF%9D%E5%AD%98%E6%AF%8F%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E5%AD%98%E5%82%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E7%B1%BB%E5%88%AB%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E5%90%8D%E4%B8%BA%E7%B1%BB%E5%90%8D"><span class="post-toc-number">15.</span> <span class="post-toc-text"> 15.ImageFolder:假设所有的文件按文件夹保存，每个文件夹下存储同一个类别图片，文件夹名为类名</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#16dataloader%E5%87%BD%E6%95%B0%E5%AF%B9batch%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E6%93%8D%E4%BD%9C%E5%90%8C%E6%97%B6%E8%BF%98%E9%9C%80%E8%A6%81%E5%AF%B9%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8Cshuffle%E5%92%8C%E5%B9%B6%E8%A1%8C%E5%8A%A0%E9%80%9F"><span class="post-toc-number">16.</span> <span class="post-toc-text"> 16.DataLoader函数：对batch的数据进行操作，同时还需要对数据进行shuffle和并行加速</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#17pytorch%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%8A%A0%E4%B8%80%E4%B8%AA%E7%BB%B4%E5%BA%A6%E7%94%A8%E4%BB%80%E4%B9%88%E5%87%BD%E6%95%B0unsequeeze"><span class="post-toc-number">17.</span> <span class="post-toc-text"> 17.pytorch数据增加一个维度用什么函数：unsequeeze()</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#18pytorch%E6%98%AF%E4%BB%80%E4%B9%88"><span class="post-toc-number">18.</span> <span class="post-toc-text"> 18.pytorch是什么</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#19pytorch-%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0"><span class="post-toc-number">19.</span> <span class="post-toc-text"> 19.PyTorch 的基本要素</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#20%E5%BC%A0%E9%87%8F"><span class="post-toc-number">20.</span> <span class="post-toc-text"> 20.张量</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#21%E6%8A%BD%E8%B1%A1%E7%BA%A7%E5%88%AB"><span class="post-toc-number">21.</span> <span class="post-toc-text"> 21.抽象级别</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#22mseloss-ctcloss-bceloss%E5%87%BD%E6%95%B0%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8"><span class="post-toc-number">22.</span> <span class="post-toc-text"> 22.MSELoss、CTCLoss、BCELoss函数有什么用？</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#23%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%98%AF%E4%BB%80%E4%B9%88"><span class="post-toc-number">23.</span> <span class="post-toc-text"> 23.反向传播是什么</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#24pytorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%8E%9F%E7%90%86"><span class="post-toc-number">24.</span> <span class="post-toc-text"> 24.pytorch多卡训练原理</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#25pytorch%E4%B8%ADtrain%E5%92%8Ceval%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C"><span class="post-toc-number">25.</span> <span class="post-toc-text"> 25.pytorch中train和eval有什么不同</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#26%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9Acnn%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%A0%B8%E9%80%9A%E9%81%93%E6%95%B0%E5%92%8C%E5%8D%B7%E7%A7%AF%E8%BE%93%E5%87%BA%E7%9A%84%E9%80%9A%E9%81%93%E6%95%B0"><span class="post-toc-number">26.</span> <span class="post-toc-text"> 26.如何确定cnn的卷积核通道数和卷积输出的通道数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#27cnn%E7%9A%84%E6%B1%A0%E5%8C%96pool%E5%B1%82"><span class="post-toc-number">27.</span> <span class="post-toc-text"> 27.cnn的池化pool层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#28%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C"><span class="post-toc-number">28.</span> <span class="post-toc-text"> 28.生成对抗网络</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#29cnn"><span class="post-toc-number">29.</span> <span class="post-toc-text"> 29.cnn</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#30%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%95%E5%85%A5%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="post-toc-number">30.</span> <span class="post-toc-text"> 30.为什么引入非线性激励函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#31batch_size%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E6%AD%A3%E7%A1%AE%E7%8E%87"><span class="post-toc-number">31.</span> <span class="post-toc-text"> 31.batch_size如何影响正确率</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#32"><span class="post-toc-number">32.</span> <span class="post-toc-text"> 32.</span></a></li></ol></div>
                        
                    
                    <article id="post-content">
                        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h2 id="1conv2d的参数以及含义"><a class="markdownIt-Anchor" href="#1conv2d的参数以及含义"></a> 1.conv2d的参数以及含义</h2>
<ul>
<li>1.in_channels (int) – 输入图像中的通道数</li>
<li>2.out_channels (int) – 由卷积产生的通道数=卷积核的数量</li>
<li>3.kernel_size (int or tuple) – 卷积核的大小</li>
<li>4.stride (int or tuple, optional) – 卷积的步幅. Default: 1</li>
<li>4.padding (int, tuple or str, optional) – 填充添加到输入的所有四个边，边界补0的层数. Default: 0 (控制卷积层输出;避免信息丢失)</li>
<li>5.padding_mode (str, optional) – ‘zeros’, ‘reflect’, ‘replicate’ or ‘circular’. Default: ‘zeros’</li>
<li>6.dilation (int or tuple, optional) – 核元素间距. Default: 1</li>
<li>7.groups (int, optional) – 分组卷积的分组数量. Default: 1</li>
<li>8.bias (bool, optional) – 如果为True，则在输出中添加一个可学习的偏差. Default: True</li>
</ul>
<h2 id="2pytorch如何微调fine-tuning"><a class="markdownIt-Anchor" href="#2pytorch如何微调fine-tuning"></a> 2.pytorch如何微调fine tuning</h2>
<p>局部微调：加载了模型参数后，只想调节最后几层，其它层不训练，也就是不进行梯度计算，pytorch提供的requires_grad使得对训练的控制变得非常简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 替换最后的全连接层， 改为训练100类</span></span><br><span class="line"><span class="comment"># 新构造的模块的参数默认requires_grad为True</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 只优化最后的分类层</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<Br>
<p>全局微调：对全局微调时，只不过我们希望改换过的层和其他层的学习速率不一样，这时候把其它层和新层在optimizer中单独赋予不同的学习速率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ignored_params = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, model.fc.parameters()))</span><br><span class="line">base_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> ignored_params,</span><br><span class="line">                     model.parameters())</span><br><span class="line"> </span><br><span class="line">optimizer = torch.optim.SGD([</span><br><span class="line">            &#123;<span class="string">&#x27;params&#x27;</span>: base_params&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;params&#x27;</span>: model.fc.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line">            ], lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="3pytorch使用多gpu"><a class="markdownIt-Anchor" href="#3pytorch使用多gpu"></a> 3.pytorch使用多gpu</h2>
<p>model.gpu() 把模型放在gpu上</p>
<p>model = nn . DataParallel ( model ) 。DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，各个GPU得到的梯度累加。与Module相关的所有数据也都会以浅复制的方式复制多份，在此需要注意，在module中属性应该是只读的。<br />
对模型和相应的数据进行.cuda()处理，可以将内存中的数据复制到gpu显存中去</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = Model(input_size, output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">  <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">  model = nn.DataParallel(model)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">   model.cuda()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4torchnn"><a class="markdownIt-Anchor" href="#4torchnn"></a> 4.torch.nn</h2>
<p>torch.nn:</p>
<ul>
<li>核心数据结构是Module,抽象的概念，既可以表示神经网络某个层layer，也可以表示一个包含很多层的神经网络。常见做法是继承nn.Module,编写自己的层。</li>
<li>自定义层必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，super(xx,self).<strong>init</strong>()</li>
<li>在构造函数__init__中必须自定义可学习的参数，并封装成Parameter</li>
<li>forward函数实现前向传播过程，其输入可以是一个或者多个tensor。无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播，这比function简单的多</li>
<li>Module中可学习参数可以通过named_parameters()或者parameters()返回迭代器，前者会给每个parameter附上名字，使其更具有辨识度。</li>
<li>pytorch实现了大部分的layer,这些layer都继承于nn.Module
<ul>
<li>nn.conv2d卷积层</li>
<li>AvgPool,Maxpool,AdaptiveAvgPool</li>
<li>TransposeConv逆卷积</li>
<li>nn.Linear全连接层</li>
<li>nn.BatchNorm1d(1d,2d,3d)</li>
<li>nn.dropout</li>
<li>nn.ReLU</li>
<li>nn.Sequential</li>
</ul>
</li>
</ul>
<h2 id="5sequential的三种写法"><a class="markdownIt-Anchor" href="#5sequential的三种写法"></a> 5.Sequential的三种写法</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net1 = nn.Sequential()</span><br><span class="line">net1.add_module(<span class="string">&#x27;conv&#x27;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">&#x27;batchnorm&#x27;</span>, nn.BatchNorm2d(<span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">&#x27;activation_layer&#x27;</span>, nn.ReLU())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net2 = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">3</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net3= nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">          (<span class="string">&#x27;bn1&#x27;</span>, nn.BatchNorm2d(<span class="number">3</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="6tips"><a class="markdownIt-Anchor" href="#6tips"></a> 6.tips</h2>
<ul>
<li>nn.ModuleList（），可以包含几个子module，可以像list一样使用它，但不能直接把输入传给MuduleList</li>
<li>nn.LSTM(4,3,1) 输入向量4维，隐藏元3,1层   nn.LSTMCell(4,3) 对应层数只能是一层</li>
<li>nn.Embedding(4,5)4个词，每个词使用5个向量表示</li>
<li>损失函数也是nn.Module的子类。nn.CrossEntropLoss()     loss = criterion(score,label)</li>
</ul>
<h2 id="7torchoptim"><a class="markdownIt-Anchor" href="#7torchoptim"></a> 7.torch.optim</h2>
<p>将深度学习常用优化方法全部封装在torch.optim中，所有优化方法继承基类optim.</p>
<ul>
<li>optimizer = optim.SGD(param=net.parameters(),lr=1)</li>
<li>optimizer.zero_grad() #梯度清零，等价于net.zero_grad()</li>
<li>input = t.randn(1,3,32,32)</li>
<li>output = net(input)</li>
<li>output.backward(output)</li>
<li>optimizer.step()</li>
</ul>
<h2 id="8对于不同网络设置不同的学习率"><a class="markdownIt-Anchor" href="#8对于不同网络设置不同的学习率"></a> 8.对于不同网络设置不同的学习率</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只为两个全连接层设置较大的学习率，其余层的学习率较小</span></span><br><span class="line">special_layers = nn.ModuleList([net.classifier[<span class="number">0</span>], net.classifier[<span class="number">3</span>]])</span><br><span class="line">special_layers_params = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, special_layers.parameters()))</span><br><span class="line">base_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> special_layers_params,</span><br><span class="line">                     net.parameters())</span><br><span class="line"> </span><br><span class="line">optimizer = t.optim.SGD([</span><br><span class="line">            &#123;<span class="string">&#x27;params&#x27;</span>: base_params&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;params&#x27;</span>: special_layers.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">        ], lr=<span class="number">0.001</span> )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="9修改学习率的方法"><a class="markdownIt-Anchor" href="#9修改学习率的方法"></a> 9.修改学习率的方法</h2>
<ul>
<li>修改optimizer.param_groups中的lr</li>
<li>新建优化器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1: 调整学习率，新建一个optimizer</span></span><br><span class="line">old_lr = <span class="number">0.1</span></span><br><span class="line">optimizer1 =optim.SGD([</span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net.features.parameters()&#125;,</span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: old_lr*<span class="number">0.1</span>&#125;</span><br><span class="line">            ], lr=<span class="number">1e-5</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 方法2: 调整学习率, 手动decay, 保存动量</span></span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">&#x27;lr&#x27;</span>] *= <span class="number">0.1</span> <span class="comment"># 学习率为之前的0.1倍</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="10nnfunctional中的函数和nnmodule主要区别"><a class="markdownIt-Anchor" href="#10nnfunctional中的函数和nnmodule主要区别"></a> 10.nn.functional中的函数和nn.Module主要区别</h2>
<ul>
<li>nn.Module实现的layers是一个特殊的类，都是有class layer(nn.Module)定义，会自动提取可学习的参数</li>
<li>nn.functional中的函数更像是纯函数，由def function(input)定义<br />
也就是说如果模型有可学习的参数，最好用nn.Module否则使用哪个都可以，二者在性能上没多大差异，<br />
对于卷积，全连接等具有可学习参数的网络建议使用nn.Module<br />
激活函数（ReLU,sigmoid,tanh），池化等可以使用functional替代。对于不具有可学习参数的层，将他们用函数代替，这样可以不用放在构造函数__init__中。</li>
</ul>
<h2 id="11将module放在gpu上运行只需两步分别将模型与数据放在gpu上"><a class="markdownIt-Anchor" href="#11将module放在gpu上运行只需两步分别将模型与数据放在gpu上"></a> 11.将Module放在gpu上运行只需两步：分别将模型与数据放在gpu上</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model=model.cuda()  <span class="comment">#将模型的所有参数转到gpu</span></span><br><span class="line"><span class="built_in">input</span>.cuda()   <span class="comment">#将输入数据也放置到GPU上</span></span><br></pre></td></tr></table></figure>
<h2 id="12在多个gpu上并行计算"><a class="markdownIt-Anchor" href="#12在多个gpu上并行计算"></a> 12.在多个gpu上并行计算</h2>
<h2 id="13torchvision"><a class="markdownIt-Anchor" href="#13torchvision"></a> 13.torchvision，</h2>
<p>视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。主要包含三部分：</p>
<ul>
<li>models：提供深度学习中各种经典网络的网络结构以及预训练好的模型，包括AlexNet、VGG系列、ResNet系列、Inception系列等。</li>
<li>datasets： 提供常用的数据集加载，设计上都是继承torhc.utils.data.Dataset，主要包括MNIST、CIFAR10/100、ImageNet、COCO等。</li>
<li>transforms：提供常用的数据预处理操作，主要包括对Tensor以及PIL Image对象的操作。</li>
</ul>
<h2 id="14pil-image的操作"><a class="markdownIt-Anchor" href="#14pil-image的操作"></a> 14.PIL Image的操作</h2>
<ul>
<li>Scale:调整图片大小，长宽比保持不变</li>
<li>CenterCrop,RandomCrop,RandomResizedCrop : 裁剪图片</li>
<li>Pad：填充</li>
<li>ToTensor: 将PIL Image对象转成Tensor，会自动将[0,255]归一化至[0,1]</li>
</ul>
<h2 id="15imagefolder假设所有的文件按文件夹保存每个文件夹下存储同一个类别图片文件夹名为类名"><a class="markdownIt-Anchor" href="#15imagefolder假设所有的文件按文件夹保存每个文件夹下存储同一个类别图片文件夹名为类名"></a> 15.ImageFolder:假设所有的文件按文件夹保存，每个文件夹下存储同一个类别图片，文件夹名为类名</h2>
<p>ImageFolder(root, transform=None, target_transform=None, loader=default_loader)</p>
<ul>
<li>root：在root指定的路径下寻找图片</li>
<li>transform：对PIL Image进行的转换操作，transform的输入是使用loader读取图片的返回对象</li>
<li>target_transform：对label的转换</li>
<li>loader：给定路径后如何读取图片，默认读取为RGB格式的PIL Image对象</li>
</ul>
<h2 id="16dataloader函数对batch的数据进行操作同时还需要对数据进行shuffle和并行加速"><a class="markdownIt-Anchor" href="#16dataloader函数对batch的数据进行操作同时还需要对数据进行shuffle和并行加速"></a> 16.DataLoader函数：对batch的数据进行操作，同时还需要对数据进行shuffle和并行加速</h2>
<p>DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False)</p>
<ul>
<li>dataset：加载的数据集(Dataset对象)</li>
<li>batch_size：batch size</li>
<li>shuffle:：是否将数据打乱</li>
<li>sampler： 样本抽样，后续会详细介绍</li>
<li>num_workers：使用多进程加载的进程数，0代表不使用多进程</li>
<li>collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可</li>
<li>pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些</li>
<li>drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</li>
</ul>
<h2 id="17pytorch数据增加一个维度用什么函数unsequeeze"><a class="markdownIt-Anchor" href="#17pytorch数据增加一个维度用什么函数unsequeeze"></a> 17.pytorch数据增加一个维度用什么函数：unsequeeze()</h2>
<h2 id="18pytorch是什么"><a class="markdownIt-Anchor" href="#18pytorch是什么"></a> 18.pytorch是什么</h2>
<p>PyTorch 是基于 Torch 库的计算机软件的一部分，它是 Python 的开源机器学习库。它是由 Facebook 人工智能研究小组开发的深度学习框架。它用于自然语言处理和计算机视觉等应用。</p>
<h2 id="19pytorch-的基本要素"><a class="markdownIt-Anchor" href="#19pytorch-的基本要素"></a> 19.PyTorch 的基本要素</h2>
<ul>
<li>PyTorch 张量</li>
<li>PyTorch NumPy</li>
<li>数学运算</li>
<li>Autograd 模块</li>
<li>优化模块</li>
<li>nn 模块</li>
</ul>
<h2 id="20张量"><a class="markdownIt-Anchor" href="#20张量"></a> 20.张量</h2>
<p>张量在 PyTorch 的深度学习中发挥着重要作用。简单来说，我们可以说，这个框架完全是基于张量的。张量被视为广义矩阵。它可以是 1D 张量（矢量）、2D 张量（矩阵）、3D 张量（立方体）或 4D 张量（立方体矢量）。</p>
<h2 id="21抽象级别"><a class="markdownIt-Anchor" href="#21抽象级别"></a> 21.抽象级别</h2>
<ul>
<li>张量：在gpu上运行的n维数组</li>
<li>变量：计算图中的一个节点，存储数据和梯度</li>
<li>模块：神经网络层讲存储状态</li>
</ul>
<h2 id="22mseloss-ctcloss-bceloss函数有什么用"><a class="markdownIt-Anchor" href="#22mseloss-ctcloss-bceloss函数有什么用"></a> 22.MSELoss、CTCLoss、BCELoss函数有什么用？</h2>
<ul>
<li>MSE 代表 Mean Squared Error，它用于创建衡量输入 x 和目标 y 中每个元素之间的均方误差的标准。</li>
<li>CTCLoss代表Connectionist Temporal Classification Loss，用于计算连续时间序列和目标序列之间的损失。</li>
<li>BCELoss(Binary Cross Entropy) 用于创建衡量目标和输出之间的二元交叉熵的标准。</li>
</ul>
<h2 id="23反向传播是什么"><a class="markdownIt-Anchor" href="#23反向传播是什么"></a> 23.反向传播是什么</h2>
<p>计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，根据梯度方向更新权值。</p>
<ul>
<li>将训练集数据输入到ANN的输入层，经过隐藏层，最后达到输出层并输出结果，这是ANN的前向传播过程；</li>
<li>由于ANN的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；</li>
<li>在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。</li>
</ul>
<h2 id="24pytorch多卡训练原理"><a class="markdownIt-Anchor" href="#24pytorch多卡训练原理"></a> 24.pytorch多卡训练原理</h2>
<ul>
<li>（1）将模型加载到一个指定的主GPU上，然后将模型浅拷贝到其它的从GPU上；</li>
<li>（2）将总的batch数据等分到不同的GPU上（坑：需要先将数据加载到主GPU上）；</li>
<li>（3）每个GPU根据自己分配到的数据进行forward计算得到loss，并通过backward得到权重梯度；</li>
<li>（4）主GPU将所有从GPU得到的梯度进行合并并用于更新模型的参数。</li>
</ul>
<p>模型方面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device_ids = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">model = Model(input_size, output_size)</span><br><span class="line">model = nn.DataParallel(model, device_ids=device_ids) <span class="comment">#单卡没有这行代码</span></span><br><span class="line">model = model.cuda(device_ids[<span class="number">1</span>]) <span class="comment">#指定哪块卡为主GPU，默认是0卡</span></span><br></pre></td></tr></table></figure>
<Br>
<p>数据方面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> data_loader:</span><br><span class="line">    input_var = Variable(data.cuda(device_ids[<span class="number">1</span>])) <span class="comment">#默认指定用0卡先加载数据就会报错</span></span><br><span class="line">    output = model(input_var)</span><br></pre></td></tr></table></figure>
<h2 id="25pytorch中train和eval有什么不同"><a class="markdownIt-Anchor" href="#25pytorch中train和eval有什么不同"></a> 25.pytorch中train和eval有什么不同</h2>
<ul>
<li>(1). model.train()——训练时候启用<br />
启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True</li>
<li>(2). model.eval()——验证和测试时候启用<br />
不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False</li>
</ul>
<p>train模式会计算梯度，eval模式不会计算梯度。</p>
<h2 id="26如何确定cnn的卷积核通道数和卷积输出的通道数"><a class="markdownIt-Anchor" href="#26如何确定cnn的卷积核通道数和卷积输出的通道数"></a> 26.如何确定cnn的卷积核通道数和卷积输出的通道数</h2>
<p>cnn卷积核通道数=卷积输入层通道数</p>
<p>cnn卷积输出层通道数=卷积核个数</p>
<h2 id="27cnn的池化pool层"><a class="markdownIt-Anchor" href="#27cnn的池化pool层"></a> 27.cnn的池化pool层</h2>
<p>池化，简言之，即取区域平均或最大</p>
<h2 id="28生成对抗网络"><a class="markdownIt-Anchor" href="#28生成对抗网络"></a> 28.生成对抗网络</h2>
<p>GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。</p>
<p>生成对抗网络的一个简单解释如下：假设有两个模型，一个是生成模型(Generative Model,下文简写为G)，一个是判别模型(Discriminative Model,下文简写为D)，判别模型(D)的任务就是判断一个实例是真实的还是由模型生成的，生成模型(G)的任务是生成一个实例来骗过判别模型(D) ，两个模型互相对抗，发展下去就会达到一个平衡，生成模型生成的实例与真实的没有区别，判别模型无法区分自然的还是模型生成的。</p>
<h2 id="29cnn"><a class="markdownIt-Anchor" href="#29cnn"></a> 29.cnn</h2>
<p>每个输入特征的组合特征</p>
<h2 id="30为什么引入非线性激励函数"><a class="markdownIt-Anchor" href="#30为什么引入非线性激励函数"></a> 30.为什么引入非线性激励函数</h2>
<p>深度学习的前提是神经网络的隐层加上了非线性激活函数，提升了模型的非线性表达能力，使得神经网络可以逼近任意复杂的函数。</p>
<h2 id="31batch_size如何影响正确率"><a class="markdownIt-Anchor" href="#31batch_size如何影响正确率"></a> 31.batch_size如何影响正确率</h2>
<p>运行时间确实随着批大小的增加而下降。然而，这导致了测试正确率的妥协，因为测试正确率随着批大小的增加而单调递减。</p>
<h2 id="32"><a class="markdownIt-Anchor" href="#32"></a> 32.</h2>

                    </article>
                    


    <blockquote id="date-expire-notification" class="post-expired-notify">本文最后更新于 <span id="date-expire-num"></span> 天前，文中所描述的信息可能已发生改变</blockquote>
    <script>
    (function() {
        var dateUpdate = Date.parse("2023-09-25");
        var nowDate = new Date();
        var a = nowDate.getTime();
        var b = a - dateUpdate;
        var daysUpdateExpire = Math.floor(b/(24*3600*1000));
        if (daysUpdateExpire >= 120) {
            document.getElementById('date-expire-num').innerHTML = daysUpdateExpire;
        } else {
            document.getElementById('date-expire-notification').style.display = 'none';
        }
    })();
    </script>


<p class="post-footer-info mb-0 pt-0">本文发表于&nbsp;<time datetime="2023-09-25T02:31:25.000Z" itemprop="datePublished">2023-09-25</time>

</p>
<p class="post-footer-info mb-0 pt-2">

<span class="post-categories-list mt-2">

<a class="post-categories-list-item" href='/categories/internship/'>internship</a>

</span>



<span class="post-tags-list mt-2">

<a class="post-tags-list-item" href="/tags/internship/" rel="tag">#&nbsp;internship</a>

<a class="post-tags-list-item" href="/tags/pytorch/" rel="tag">#&nbsp;pytorch</a>

</span>


</p>

                </div>
                <div class="post-nav px-2 bg-gray">
<ul class="pagination">
    <!-- Prev Nav -->
    
        <li class="page-item page-prev">
            <a href="/2023/09/25/intern-00/" rel="prev">
                <div class="page-item-title"><i class="icon icon-back" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Intern:00</div>
            </a>
        </li>
    

    <!-- Next Nav -->
    
        <li class="page-item page-next">
            <a href="/2023/09/22/Git/" rel="next">
                <div class="page-item-title"><i class="icon icon-forward" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Git</div>
            </a>
        </li>
    
</ul>
</div>

                
                    <!-- # Comment # -->
                    
                
            </div>
        </div>
    </div>
</div>

            <!-- ### Footer ### -->
            <footer class="text-center">
    <!-- footer copyright -->
    
        <p class="footer-copyright mb-0">Copyright&nbsp;©&nbsp;<span id="copyright-year"></span>
            <a class="footer-copyright-a" href="https://abinzzz.github.io">blog</a>
        </p>

    <!-- footer custom text -->
    <p class="footer-text mb-0">
    
    </p>
    <!-- footer develop info -->
    <p class="footer-develop mb-0">
        
    <!-- Busuanzi User Views -->
    <span id="busuanzi_container_site_uv" hidden>
        <span></span>
        <span id="busuanzi_value_site_uv"></span>
        <span>Viewers</span>
        
            <span>|</span>
        
    </span>




        
        Powered by&nbsp;<!--
         --><a href="https://hexo.io" target="_blank" class="footer-develop-a" rel="external nofollow noopener noreferrer">Hexo</a><span class="footer-develop-divider"></span>Theme&nbsp;-&nbsp;<!--
         --><a href="https://github.com/SukkaW/hexo-theme-suka" target="_blank" class="footer-develop-a" rel="external noopener">Suka</a>
    </p>
</footer>


        <!-- ### Import File ### -->
        <!-- ### Footer JS Import ### -->

<script>

    
window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 50
};

(function() {
    var copyrightNow = new Date().getFullYear();
    var copyrightContent = document.getElementById('copyright-year');
    var copyrightSince = 2023;
    if (copyrightSince === copyrightNow) {
        copyrightContent.textContent = copyrightNow;
    } else {
        copyrightContent.textContent = copyrightSince + ' - ' + copyrightNow;
    }
})();
console.log('\n %c Suka Theme (hexo-theme-suka) | © SukkaW | Verision 1.3.3 %c https://github.com/SukkaW/hexo-theme-suka \n', 'color: #fff; background: #444; padding:5px 0;', 'background: #bbb; padding:5px 0;');

</script>

<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@8.9.0" async></script>
    <script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" async></script>


<!-- Offset -->




<!-- Comment -->


<!-- ### Custom Footer ### -->

    </body>

</html>