
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <meta charset="utf-8">
    

    

    
    <title>
        NNDL:L2-Neural Network |
        
        Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="oner加油！🔗：Oner: 怀揣着一定可以做好的确信 目录 1.引例：数字识别 2.生物神经元与人工神经元 2.1 生物神经元 2.2 人工神经元   3.典型的人工神经网络结构 3.1 感知机 3.2 多层感知机 3.3 激活函数 3.3.1 激活函数的特性 3.3.2 常用的激活函数   3.4 神经网络   4.前馈神经网络 4.1 前馈神经网络基本介绍 4.2 前馈神经网络：示例">
<meta property="og:type" content="article">
<meta property="og:title" content="NNDL:L2-Neural Network">
<meta property="og:url" content="https://abinzzz.github.io/2023/10/17/NNDL-Neural-Network/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="oner加油！🔗：Oner: 怀揣着一定可以做好的确信 目录 1.引例：数字识别 2.生物神经元与人工神经元 2.1 生物神经元 2.2 人工神经元   3.典型的人工神经网络结构 3.1 感知机 3.2 多层感知机 3.3 激活函数 3.3.1 激活函数的特性 3.3.2 常用的激活函数   3.4 神经网络   4.前馈神经网络 4.1 前馈神经网络基本介绍 4.2 前馈神经网络：示例">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/F8okWYVaIAAbp2m?format=png&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F8olegJa8AAhB6Z?format=png&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F8opUFabIAAZhIA?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F8oxO0OaQAAfRsP?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8o26dAaMAAVUAW?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8o4drHbIAAt7iJ?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8o3tc1aoAAKbx1?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8pAAQtagAAHLjN?format=png&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F8o_VfOawAAZS5u?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8pNvutawAAcVR2?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8pAmqGaUAAOPxH?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8pBmysbkAA3aKV?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8pG5jHawAALVRM?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F8pObl5a4AACUE9?format=jpg&amp;name=medium">
<meta property="og:image" content="https://pbs.twimg.com/media/F8pPRVzbIAIC2mr?format=jpg&amp;name=medium">
<meta property="article:published_time" content="2023-10-17T10:05:42.000Z">
<meta property="article:modified_time" content="2023-10-17T18:17:42.111Z">
<meta property="article:author" content="野中晴">
<meta property="article:tag" content="专业知识">
<meta property="article:tag" content="NNDL">
<meta property="article:tag" content="l2">
<meta property="article:tag" content="Neural Network">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/F8okWYVaIAAbp2m?format=png&amp;name=small">
    
        <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
        <img data-src="https://pbs.twimg.com/media/F8otusybIAAACEv?format=jpg&amp;name=medium" data-sizes="auto" alt="NNDL:L2-Neural Network" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>NNDL:L2-Neural Network</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-NNDL-Neural-Network" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/10/17/NNDL-Neural-Network/" class="article-date-link">
        <time datetime="2023-10-17T10:05:42.000Z"
              itemprop="datePublished">2023-10-17</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <h2 id="oner加油！"><a href="#oner加油！" class="headerlink" title="oner加油！"></a>oner加油！</h2><p>🔗：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1yG41127k2/?share_source=copy_web&amp;vd_source=d8d8cd49f932177e1995e230d7816d44">Oner: 怀揣着一定可以做好的确信</a></p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li>1.引例：数字识别</li>
<li>2.生物神经元与人工神经元<ul>
<li>2.1 生物神经元</li>
<li>2.2 人工神经元</li>
</ul>
</li>
<li>3.典型的人工神经网络结构<ul>
<li>3.1 感知机</li>
<li>3.2 多层感知机</li>
<li>3.3 激活函数<ul>
<li>3.3.1 激活函数的特性</li>
<li>3.3.2 常用的激活函数</li>
</ul>
</li>
<li>3.4 神经网络</li>
</ul>
</li>
<li>4.前馈神经网络<ul>
<li>4.1 前馈神经网络基本介绍</li>
<li>4.2 前馈神经网络：示例</li>
</ul>
</li>
<li>5.反向传播算法<ul>
<li>5.1 变量的定义</li>
<li>5.2 四个基本等式</li>
<li>5.3 反向传播算法流程</li>
</ul>
</li>
<li>6.应用</li>
<li>7.思考题<ul>
<li>7.1 为什么说神经网络的优化问题是一个非凸优化问题，请举例说明</li>
<li>7.2 如何控制梯度消失问题</li>
<li>7.3 如何使用感知机来计算基本逻辑功能，如AND，OR和NAND？</li>
<li>7.4 具有线性激活函数的神经元的反向传播: 假设在网络中通常使用的非线性激活函数 $\sigma(z) = a$,试着推导这种情况下的反向传播算法</li>
</ul>
</li>
</ul>
<p><br></p>
<h2 id="1-引例：手写数字识别"><a href="#1-引例：手写数字识别" class="headerlink" title="1.引例：手写数字识别"></a>1.引例：手写数字识别</h2><BR>

<p><img src="https://pbs.twimg.com/media/F8okWYVaIAAbp2m?format=png&amp;name=small" alt=""></p>
<p><br></p>
<h2 id="2-生物神经元与人工神经元"><a href="#2-生物神经元与人工神经元" class="headerlink" title="2.生物神经元与人工神经元"></a>2.生物神经元与人工神经元</h2><h2 id="2-1-生物神经元"><a href="#2-1-生物神经元" class="headerlink" title="2.1 生物神经元"></a>2.1 生物神经元</h2><p><br></p>
<p><img src="https://pbs.twimg.com/media/F8olegJa8AAhB6Z?format=png&amp;name=small" alt=""></p>
<ul>
<li><strong>加权求和</strong>：神经元是一个多输入单输出的信息处理单元</li>
<li><strong>非线性变换(激活函数)</strong>：二值化输出</li>
</ul>
<p><br></p>
<h2 id="2-2-人工神经元"><a href="#2-2-人工神经元" class="headerlink" title="2.2 人工神经元"></a>2.2 人工神经元</h2><p>基本结构：</p>
<p><img src="https://pbs.twimg.com/media/F8opUFabIAAZhIA?format=png&amp;name=900x900" alt=""></p>
<p><br></p>
<h2 id="3-典型的人工神经网络结构"><a href="#3-典型的人工神经网络结构" class="headerlink" title="3.典型的人工神经网络结构"></a>3.典型的人工神经网络结构</h2><h2 id="3-1-感知机perceptron"><a href="#3-1-感知机perceptron" class="headerlink" title="3.1 感知机perceptron"></a>3.1 感知机perceptron</h2><p>神经网络中一种模拟神经元(neuron)的结构，有输入(input)、输出(output)、权重(weight)、前馈运算(feed forward)、激活函数(activation function)等部分。单层感知器能模拟逻辑与、逻辑或、逻辑非和逻辑与非等操作，但不能实现逻辑异或！</p>
<p><strong>激活函数</strong>：y = f(X) = f(w·x + b)</p>
<ul>
<li><strong>x</strong>: 输入</li>
<li><strong>w</strong>: 权重</li>
<li><strong>b</strong>: 偏置</li>
<li><strong>y</strong>: 与其计算结果</li>
<li><strong>f</strong>: sign函数</li>
</ul>
<p><strong>前馈运算</strong>：还没进入激活函数进行计算的结果，写作logit或者logits</p>
<p>logit = (w·x + b)</p>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F8oxO0OaQAAfRsP?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<h2 id="3-2-多层感知机-mlp-multi-layer-perceptron-神经网络-neural-network"><a href="#3-2-多层感知机-mlp-multi-layer-perceptron-神经网络-neural-network" class="headerlink" title="3.2 多层感知机(mlp=multi-layer perceptron)/神经网络(neural network)"></a>3.2 多层感知机(mlp=multi-layer perceptron)/神经网络(neural network)</h2><p>与<strong>感知器</strong>的不同：</p>
<ul>
<li><strong>结构</strong>：mlp引入了隐层(hidden layer)概念</li>
<li><strong>计算能力</strong>：感知机只能处理线性可分数据</li>
<li><strong>激活函数</strong>：mlp的激活函数更多：sigmoid，tanh，relu</li>
<li><strong>学习算法</strong>：perceptron基于错误更新权重，而mlp使用反向传播算法</li>
</ul>
<p><strong>隐层</strong>：不包括输入层和输出层，在输入层和输出层中间的所有N层神经元就称作隐层！通常输入层不算作神经网络的一部分，</p>
<p>隐层的特性:</p>
<ul>
<li>L层的每一个神经元与 L-1 层的每一个神经元的输出相连；</li>
<li>L层的每一个神经元互相没有连接；</li>
</ul>
<h2 id="3-3-激活函数"><a href="#3-3-激活函数" class="headerlink" title="3.3 激活函数"></a>3.3 激活函数</h2><p><strong>Perceptron和Mlp激活函数不同的原因</strong>：由于要是用反向传播算法，需要进行求导，perceptron中的sign函数就被换成了连续可导函数</p>
<p><br></p>
<h2 id="3-3-1-激活函数的特性"><a href="#3-3-1-激活函数的特性" class="headerlink" title="3.3.1 激活函数的特性"></a>3.3.1 激活函数的特性</h2><p>决定神经元的函数特性的<strong>最关键</strong>部分：激活函数</p>
<p>激活函数：连续可导的非线性函数(可导的激活函数可以直接利用数值优化的方法来学习网络参数)</p>
<p>激活函数及其导数要尽量<strong>简单</strong>： 提高网络计算效率</p>
<p>激活函数的导函数的值域要在一个<strong>合适的区间</strong>： 与训练的效率和稳定性有关</p>
<p><br></p>
<h2 id="3-3-2-常用的激活函数"><a href="#3-3-2-常用的激活函数" class="headerlink" title="3.3.2 常用的激活函数"></a>3.3.2 常用的激活函数</h2><p>(1)<strong>sigmoid/logistic</strong>: f(x) = $\frac{1}{1+e^{-x}}$</p>
<p><img src="https://pbs.twimg.com/media/F8o26dAaMAAVUAW?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<BR>

<p>(2)<strong>tanh</strong>: tanh(x) = $\frac{1 - e^{-2x}}{1 + e^{-2x}}$</p>
<p><img src="https://pbs.twimg.com/media/F8o4drHbIAAt7iJ?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<Br>

<p>(3)<strong>RELU(Rectified Linear Units)</strong>: RELU(x) = max(0,x)</p>
<p><img src="https://pbs.twimg.com/media/F8o3tc1aoAAKbx1?format=jpg&amp;name=medium" alt=""></p>
<p><br><br><br></p>
<p>(4)<strong>Leaky RELU</strong>:</p>
<script type="math/tex; mode=display">
  f(x) =
    \begin{cases}
      x,  & \text{if $x \geq$ 0} \\
      ax, & \text{if $x <$ 0}
    \end{cases}</script><p><img src="https://pbs.twimg.com/media/F8pAAQtagAAHLjN?format=png&amp;name=small" alt=""></p>
<p><br></p>
<p>(5)<strong>ELU</strong>:</p>
<script type="math/tex; mode=display">
  f(x) =
    \begin{cases}
      x,  & \text{if $x \geq$ 0} \\
      a(e^x -1), & \text{if $x <$ 0}
    \end{cases}</script><p><img src="https://pbs.twimg.com/media/F8o_VfOawAAZS5u?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<p><br></p>
<p>(6)<strong>softmax</strong>: $Softmax(z_i) = \frac{\exp(z_i)}{\sum_j \exp(z_j)}$</p>
<p><img src="https://pbs.twimg.com/media/F8pNvutawAAcVR2?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<Br>



<p>(7)常用激活函数及其导函数：</p>
<p><img src="https://pbs.twimg.com/media/F8pAmqGaUAAOPxH?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<h2 id="3-4-神经网络"><a href="#3-4-神经网络" class="headerlink" title="3.4 神经网络"></a>3.4 神经网络</h2><p>神经网络主要由大量的神经元以及它们之间的有向连接构成</p>
<p>需要考虑的三个方面：</p>
<ul>
<li><strong>拓扑结构</strong>：不同神经元以什么样的方式连接在一起</li>
<li><strong>网络的表示</strong>：从输入神经元到输出神经元的函数映射关系</li>
<li><strong>学习算法</strong>：采用何种方法通过训练数据学习神经网络的参数</li>
</ul>
<p><img src="https://pbs.twimg.com/media/F8pBmysbkAA3aKV?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<h2 id="4-前馈神经网络"><a href="#4-前馈神经网络" class="headerlink" title="4.前馈神经网络"></a>4.前馈神经网络</h2><h2 id="4-1-前馈神经网络基本介绍"><a href="#4-1-前馈神经网络基本介绍" class="headerlink" title="4.1 前馈神经网络基本介绍"></a>4.1 前馈神经网络基本介绍</h2><p>(1)前馈神经网络(FNN): 一层的输出用作下一层的输入</p>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F8pG5jHawAALVRM?format=png&amp;name=900x900" alt=""></p>
<ul>
<li>$\omega_{jk}^l$: 从第l-1层的第k个神经元到第l层的第j个神经元的连接权重</li>
<li>$b_j^l$: 第l层中第j个神经元的偏置</li>
<li>$a_j^l$: 第l层中第j个神经元的激活值</li>
</ul>
<p><br></p>
<p>(2)$a_j^l = \sigma(\sum_k \omega_{jk}^l a_k^{l-1} + b_j^l)$</p>
<ul>
<li>$\sigma$: 第j个神经元的激活函数</li>
<li>总和包括第l-1层的所有神经元k</li>
</ul>
<p><br></p>
<p>(3)<strong>某一层的激活向量和前一层的激活向量相关</strong>：$a^l = \sigma(w^la^{l-1} + b^l)$</p>
<ul>
<li>$\omega^l$: 第l层的权重矩阵，矩阵中第j行和第k列的条目为$w_{jk}^l$</li>
<li>$b^l$: 第l层的偏置向量</li>
<li>$a^l$: 第l层的激活向量</li>
<li>$\sigma$: 激活函数向量化</li>
</ul>
<p><br></p>
<p>(4)$z^l \equiv \omega^l a^{l-1} + b^l$</p>
<ul>
<li>$z^l$: 第l层的加权输入</li>
</ul>
<p><br></p>
<p>(5)$z^l_j = \sum_k \omega_{jk}^l a_k^{l-1} + b_j^l$</p>
<ul>
<li>$z^l_j$: 第l层第j个神经元激活函数的加权输入值</li>
</ul>
<p><br></p>
<p>(6) <strong>(3)中的式子</strong>可进一步简化为：$a^l = \sigma (z^l)$</p>
<p><br></p>
<p>(7)输出层：根据任务确定输出层的激活函数</p>
<ul>
<li><strong>回归</strong>任务：根据输出的值域选择激活函数</li>
<li><strong>分类</strong>任务：softmax函数</li>
</ul>
<p><br></p>
<p>softmax函数：</p>
<p><img src="https://pbs.twimg.com/media/F8pObl5a4AACUE9?format=jpg&amp;name=medium" alt=""></p>
<p><br></p>
<h2 id="4-2-前馈神经网络：示例"><a href="#4-2-前馈神经网络：示例" class="headerlink" title="4.2 前馈神经网络：示例"></a>4.2 前馈神经网络：示例</h2><p><strong>流程</strong>：</p>
<ul>
<li>输入图像</li>
<li>分割图像</li>
<li>对每一个数字进行分类</li>
</ul>
<p><br></p>
<p><strong>神经网络结构</strong>：</p>
<p><img src="https://pbs.twimg.com/media/F8pPRVzbIAIC2mr?format=jpg&amp;name=medium" alt=""></p>
<p><strong>输入层</strong>：784个神经元</p>
<ul>
<li>28x28像素</li>
<li>灰度，1表示白色，0表示黑色，在0～1之间的值表示渐变的灰色程度</li>
</ul>
<p><br></p>
<p><strong>隐藏层</strong>：15个神经元</p>
<ul>
<li>隐藏层的设计需要一些技巧</li>
<li>尝试不同的隐藏层节点数</li>
</ul>
<p><br></p>
<p><strong>输出层</strong>：10个神经元</p>
<ul>
<li>如果第一个神经元触发，即输出位1，那么这表明网络认为该数字为0，以此类推</li>
</ul>
<p><br></p>
<h2 id="5-反向传播算法"><a href="#5-反向传播算法" class="headerlink" title="5.反向传播算法"></a>5.反向传播算法</h2><h2 id="5-1-变量的定义"><a href="#5-1-变量的定义" class="headerlink" title="5.1 变量的定义"></a>5.1 变量的定义</h2><p><strong>误差</strong>：第l层中第j个神经元的误差 $\delta_j^l = \frac{\partial J}{\partial z_j^l}$</p>
<p>$\delta^l$: 第l层的误差向量</p>
<p>$\delta^L$: 输出层的误差向量</p>
<p><br></p>
<h2 id="5-2-四个基本等式"><a href="#5-2-四个基本等式" class="headerlink" title="5.2 四个基本等式"></a>5.2 四个基本等式</h2><ul>
<li>输出层的误差向量: $\delta_j^L = \frac{\partial J}{\partial a_j^L} \sigma^{‘}(z_j^L)$</li>
</ul>
<p>$\delta_j^L = \frac{\partial J}{\partial z_j^L} =  \frac{\partial J}{\partial a_j^L} \frac{\partial a_j^L}{\partial z_j^L} =\frac{\partial J}{\partial a_j^L} \sigma^{‘}(z_j^L)$</p>
<p><br></p>
<ul>
<li>回传误差: $\delta^l = ((\omega^{l+1})^T \delta^{l+1}) \bigodot \sigma^{‘}(z^l)$</li>
</ul>
<p>$\delta_j^l = \frac{\partial J}{\partial z_j^l} = \sum_k \frac{\partial J}{\partial z_k^{l+1}}\frac{\partial z_k^{l+1}}{\partial z_j^l} = \sum_k \delta_k^{l+1} \frac{\partial z_k^{l+1}}{\partial z_j^l}$</p>
<p>$z_k^{l+1} = \sum_j \omega_{kj}^{l+1}a_j^l + b_k^{l+1} = \sum_j \omega_{kj}^{l+1}\sigma(z_j^l) + b_k^{l+1}$</p>
<p>$\frac{\partial z_k^{l+1}}{\partial z_j^l} = \omega_{kj}^{l+1}\sigma^{‘}(z_j^l)$<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">求和符号去掉的原因：求和是对前j个求和，而我这里是第j个，所以只对第j个求导</span><br></pre></td></tr></table></figure></p>
<p>$\delta_j^l = \sum_k \omega_{kj}^{l+1} \delta_k^{l+1} \sigma^{‘}(z_j^l)$</p>
<p><br></p>
<ul>
<li>权重一阶导:$\frac{\partial J}{\partial \omega_{jk}^l} = a_k^{l-1}\delta_j^l$</li>
</ul>
<p>$\frac{\partial J}{\partial \omega_{jk}^l} = \frac{\partial J}{\partial z_j^l} \frac{\partial z_j^l}{\partial \omega_{jk}^l} = \delta_j^l · \frac{\partial (\omega_{jk}^{l}a_k^{l-1} + b_j^l)}{\partial \omega_{jk}^l}  = a_k^{l-1}\delta_j^l$</p>
<p><br></p>
<ul>
<li>偏置一阶导：$\frac{\partial J}{\partial b_{j}^l} = \delta_j^l$</li>
</ul>
<p>$\frac{\partial J}{\partial b_{j}^l} = \frac{\partial J}{\partial z_j^l} \frac{\partial z_j^l}{\partial b_{j}^l} = \delta_j^l · \frac{\partial (\omega_{jk}^{l}a_k^{l-1} + b_j^l)}{\partial b_{j}^l}  = \delta_j^l$</p>
<p><br></p>
<h2 id="5-3-反向传播算法流程"><a href="#5-3-反向传播算法流程" class="headerlink" title="5.3 反向传播算法流程"></a>5.3 反向传播算法流程</h2><p>(1)输入：首先为输入层设置相应的激活值$a^1$</p>
<p>(2)前馈：对每一层l= 2,3,…,L, 计算：<br>$z^l = \omega^la^{l-1} + b^l$ 和 $a^l = \sigma(z^l)$</p>
<p>(3)输出层误差$\delta^L$: 计算向量$\delta^L = \bigtriangledown_a J \bigodot \sigma^{‘}(z^L)$</p>
<p>(4)误差反向传播: 对每一层l=L-1,L-2,…,2计算<br>$\delta^l = ((\omega^{l+1})^T \delta^{l+1}) \bigodot \sigma^{‘}(z^l)$</p>
<p>(5)输出：代价函数的梯度公式：<br>$\frac{\partial J}{\partial \omega_{jk}^l} = a_k^{l-1}\delta_j^l$   </p>
<p>$\frac{\partial J}{\partial b_{j}^l} = \delta_j^l$</p>
<p><br></p>
<h2 id="6-应用实例"><a href="#6-应用实例" class="headerlink" title="6.应用实例"></a>6.应用实例</h2><ul>
<li>图像分类</li>
<li>网络结构</li>
<li>推荐</li>
</ul>
<h2 id="7-思考题"><a href="#7-思考题" class="headerlink" title="7.思考题"></a>7.思考题</h2><h2 id="7-1-为什么说神经网络的优化问题是一个非凸优化问题，请举例说明"><a href="#7-1-为什么说神经网络的优化问题是一个非凸优化问题，请举例说明" class="headerlink" title="7.1 为什么说神经网络的优化问题是一个非凸优化问题，请举例说明"></a>7.1 为什么说神经网络的优化问题是一个非凸优化问题，请举例说明</h2><ul>
<li><p><strong>定义</strong>：首先，简单地说，一个优化问题是凸的(convex)如果其目标函数是凸的，并且其约束是凸集。对于一个函数来说，如果任意两点之间的直线段上的点的函数值都大于等于该直线段上的点所对应的函数值，那么这个函数是凸的。但在神经网络中，损失函数通常不满足这个性质，因此优化问题是非凸的。</p>
</li>
<li><p><strong>局部最小值与鞍点</strong>：神经网络的损失函数可能有多个局部最小值和鞍点。这意味着，如果你从一个初始点开始使用梯度下降法或其它优化算法，你可能会陷入一个局部最小值或鞍点，而不是全局最小值。对于凸优化问题，只有一个全局最小值，不存在局部最小值。</p>
</li>
<li><p><strong>复杂性</strong>：随着神经网络的层数增加，其非凸性也随之增加。深度神经网络的损失曲面更复杂，有更多的局部最小值和鞍点。</p>
</li>
<li><p><strong>举例</strong>：考虑一个简单的两层神经网络，即具有一个隐藏层的网络。如果激活函数是非线性的（如ReLU、Sigmoid等），那么损失函数是参数权值的非线性函数。这种非线性导致损失函数的形状在整个参数空间中都是扭曲的，因此存在多个局部最小值。为了更直观地想象，你可以想象一个山丘和谷地组成的地形，其中有多个低谷（即局部最小值）。一个旅行者想找到最低的谷地（即全局最小值），但如果他只根据当前的坡度向下走（即梯度下降），他可能会陷入一个较高的谷地（即局部最小值）。<br>尽管神经网络优化是一个非凸问题，但实际上，许多深度学习模型在实践中都能够得到令人满意的结果。这部分是因为，尽管存在许多局部最小值，但它们的函数值（即损失）可能与全局最小值相差不大，或者说大部分局部最小值都是“好”的。此外，近年来的研究也发现，对于高维神经网络，鞍点比局部最小值更常见，而梯度下降法在鞍点附近的行为与在局部最小值附近的行为是不同的。</p>
</li>
</ul>
<h2 id="7-2-如何控制梯度消失问题"><a href="#7-2-如何控制梯度消失问题" class="headerlink" title="7.2 如何控制梯度消失问题"></a>7.2 如何控制梯度消失问题</h2><p><strong>改变激活函数</strong>：</p>
<ul>
<li>使用<code>ReLU</code>（Rectified Linear Unit）及其变种，如<code>Leaky ReLU</code>、<code>Parametric ReLU</code>和<code>Exponential Linear Units (ELU)</code>。这些激活函数在正数区间上都不会饱和，因此在正数区域不会导致梯度消失。</li>
<li>注意，ReLU有其自身的问题，即“死ReLU”问题，其中某些神经元可能在整个训练过程中都不再激活。<code>Leaky ReLU</code>和<code>Parametric ReLU</code>等变种可以解决这一问题。</li>
</ul>
<p><strong>权重初始化</strong>：</p>
<ul>
<li>使用特定的权重初始化策略，如He初始化或Glorot/Xavier初始化。这些方法根据输入和输出神经元的数量来调整初始化的权重尺度，帮助确保初始梯度不会太大也不会太小。</li>
</ul>
<p><strong>批量归一化（Batch Normalization）</strong>：</p>
<ul>
<li>通过对每一层的输入进行归一化，Batch Normalization可以使激活值保持在一个适中的尺度，从而缓解梯度消失和梯度爆炸问题。</li>
<li>除了加速训练，它还可以作为一个轻微的正则化方法。</li>
</ul>
<p><strong>残差结构（Residual Networks, ResNets）</strong>：</p>
<ul>
<li>通过引入跳跃连接或残差连接，ResNets允许梯度直接流经网络的多个层。这极大地缓解了梯度消失问题，使得可以训练几百甚至上千层的神经网络。</li>
</ul>
<p><strong>使用门控单元</strong>：</p>
<ul>
<li>在循环神经网络（RNN）中，梯度消失是一个特别严重的问题。长短时记忆网络（LSTM）和门控循环单元（GRU）是为解决这一问题而设计的特殊RNN架构。它们内部有门控机制来控制信息流，从而缓解了梯度消失问题。</li>
</ul>
<p><strong>适当的学习率和优化器</strong>：</p>
<ul>
<li>选择一个合适的学习率或使用能够自适应学习率的优化器（如Adam、RMSprop等）可以帮助缓解梯度消失。</li>
</ul>
<p><strong>正则化技术</strong>：</p>
<ul>
<li>如dropout，虽然它主要用于防止过拟合，但在某些情况下，它也可以帮助缓解梯度消失问题。</li>
</ul>
<h2 id="7-3-如何使用感知机来计算基本逻辑功能，如AND，OR和NAND？"><a href="#7-3-如何使用感知机来计算基本逻辑功能，如AND，OR和NAND？" class="headerlink" title="7.3 如何使用感知机来计算基本逻辑功能，如AND，OR和NAND？"></a>7.3 如何使用感知机来计算基本逻辑功能，如AND，OR和NAND？</h2><p><strong>AND逻辑</strong>：<br>对于AND逻辑，当且仅当两个输入都是1时，输出为1。</p>
<p>我们可以使用如下的感知机来实现AND逻辑：</p>
<ul>
<li>输入：( x_1, x_2 )（均为0或1）</li>
<li>权重：( w_1 = 1, w_2 = 1 )</li>
<li>偏置：( b = -1.5 )</li>
<li>激活函数：阶跃函数，如果输入大于0则输出1，否则输出0。</li>
</ul>
<p>那么，输出 ( y ) 可以计算为：</p>
<p>y = step($x_1$ x $\omega_1$ + $x_2$ x $\omega_2$ + b)</p>
<BR>

<p><strong>OR逻辑</strong></p>
<p>对于OR逻辑，只要有一个输入是1，输出就为1。</p>
<p>使用如下的感知机实现OR逻辑：</p>
<ul>
<li>输入：( x_1, x_2 )</li>
<li>权重：( w_1 = 1, w_2 = 1 )</li>
<li>偏置：( b = -0.5 )</li>
</ul>
<p>输出 ( y ) 为：</p>
<p>y = step($x_1$ x $\omega_1$ + $x_2$ x $\omega_2$ + b)</p>
<p><br></p>
<p><strong>NAND逻辑</strong></p>
<p>对于NAND逻辑，当且仅当两个输入都是1时，输出为0。</p>
<p>使用如下的感知机实现NAND逻辑：</p>
<ul>
<li>输入：( x_1, x_2 )</li>
<li>权重：( w_1 = -1, w_2 = -1 )</li>
<li>偏置：( b = 1.5 )</li>
</ul>
<p>输出 ( y ) 为：</p>
<p>y = step($x_1$ x $\omega_1$ + $x_2$ x $\omega_2$ + b)</p>
<h2 id="7-4-具有线性激活函数的神经元的反向传播-假设在网络中通常使用的非线性激活函数-sigma-z-a-试着推导这种情况下的反向传播算法"><a href="#7-4-具有线性激活函数的神经元的反向传播-假设在网络中通常使用的非线性激活函数-sigma-z-a-试着推导这种情况下的反向传播算法" class="headerlink" title="7.4 具有线性激活函数的神经元的反向传播: 假设在网络中通常使用的非线性激活函数 $\sigma(z) = a$,试着推导这种情况下的反向传播算法"></a>7.4 具有线性激活函数的神经元的反向传播: 假设在网络中通常使用的非线性激活函数 $\sigma(z) = a$,试着推导这种情况下的反向传播算法</h2><p>当我们使用线性激活函数时,函数形式为:</p>
<script type="math/tex; mode=display">\sigma(z) = z</script><p>这意味着对于输入 $(z)$,输出 $(a)$ 就是 $(z)$。即,</p>
<script type="math/tex; mode=display">a = z</script><p>要推导反向传播算法,我们首先需要知道激活函数的导数。对于线性函数 $\sigma(z) = z$,其导数为:</p>
<script type="math/tex; mode=display">\frac{d\sigma(z)}{dz} = 1</script><p>反向传播的关键是计算每一层的误差项 $(\delta)$。</p>
<p><br><br><br></p>
<p><strong>输出层的误差</strong>:</p>
<script type="math/tex; mode=display">\delta^L = \nabla_a C \odot \sigma'(z^L)</script><p>其中,</p>
<script type="math/tex; mode=display">\nabla_a C</script><p>是损失函数 $(C)$ 关于激活的梯度,$\odot$ 是哈达玛积(元素之间的乘积),而 $\sigma’(z^L)$ 是输出层激活函数的导数,对于线性激活函数,</p>
<script type="math/tex; mode=display">\sigma'(z^L) = 1</script><p>因此,</p>
<script type="math/tex; mode=display">\delta^L = \nabla_a C</script><p><br><br><br></p>
<p><strong>隐藏层的误差</strong>:</p>
<p>对于隐藏层 $l$,误差 $(\delta^l)$ 与下一层 $(\delta^{l+1})$ 之间的关系为:</p>
<script type="math/tex; mode=display">\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)</script><p>对于线性激活函数,</p>
<script type="math/tex; mode=display">\sigma'(z^l) = 1</script><p>所以,</p>
<script type="math/tex; mode=display">\delta^l = (w^{l+1})^T \delta^{l+1}</script><p>现在,我们可以计算损失函数 $(C)$ 相对于任何权重 $(w)$ 或偏置 $(b)$ 的梯度:</p>
<script type="math/tex; mode=display">\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j</script><script type="math/tex; mode=display">\frac{\partial C}{\partial b^l_j} = \delta^l_j</script>
            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/10/17/NNDL-Neural-Network/" data-id="clnu7j4s900004s690hq6geuy" data-title="NNDL:L2-Neural Network"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NNDL/" rel="tag">NNDL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Network/" rel="tag">Neural Network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/l2/" rel="tag">l2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" rel="tag">专业知识</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://ravennonest.files.wordpress.com/2021/02/takayan-i-love-you-in-any-way.jpg" data-sizes="auto" alt="NNDL:Lab1-基于全连接神经网络的手写数字识别"
                         class="lazyload">
                
                <a href="/2023/10/18/NNDL-Lab1-%E5%9F%BA%E4%BA%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        NNDL:Lab1-基于全连接神经网络的手写数字识别
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://ravennonest.files.wordpress.com/2021/02/takayan-i-love-you-in-any-way.jpg" data-sizes="auto" alt="Intern:01"
                         class="lazyload">
                
                <a href="/2023/10/16/intern-01/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        Intern:01
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="野中晴" class="lazyload">
            <div class="sidebar-author-name">野中晴</div>
            <div class="sidebar-description">Love is selfish.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">145</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">10</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">192</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AimGraduate/">AimGraduate</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Essay/">Essay</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/1/" style="font-size: 10px;">1</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 11.67px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/AimGraduate/" style="font-size: 10px;">AimGraduate</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/CAS/" style="font-size: 10px;">CAS</a> <a href="/tags/CAS%E5%AE%9E%E4%B9%A0offer/" style="font-size: 10px;">CAS实习offer</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 14.17px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.83px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Essay/" style="font-size: 20px;">Essay</a> <a href="/tags/GEAR-5/" style="font-size: 10px;">GEAR-5</a> <a href="/tags/Git/" style="font-size: 10.83px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.67px;">GoAbroad</a> <a href="/tags/Gumayusi/" style="font-size: 10px;">Gumayusi</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 11.67px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Intermediate-SQL/" style="font-size: 10px;">Intermediate SQL</a> <a href="/tags/Introduction/" style="font-size: 10px;">Introduction</a> <a href="/tags/Introduction-to-SQL/" style="font-size: 10px;">Introduction to SQL</a> <a href="/tags/Introduction-to-the-Relational-Model/" style="font-size: 10px;">Introduction to the Relational Model</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Lab1/" style="font-size: 10px;">Lab1</a> <a href="/tags/Lec01/" style="font-size: 11.67px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.83px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 10.83px;">Linux</a> <a href="/tags/M2/" style="font-size: 10.83px;">M2</a> <a href="/tags/MIT6-S081/" style="font-size: 13.33px;">MIT6.S081</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Mac/" style="font-size: 10.83px;">Mac</a> <a href="/tags/Missing-Semester/" style="font-size: 10px;">Missing Semester</a> <a href="/tags/NNDL/" style="font-size: 11.67px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/PyTorch/" style="font-size: 10px;">PyTorch</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.83px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Rethinking-the-performance-comparison-between-SNNS-and-ANNS/" style="font-size: 10px;">Rethinking the performance comparison between SNNS and ANNS</a> <a href="/tags/SE/" style="font-size: 13.33px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 11.67px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10.83px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/StarBucks/" style="font-size: 12.5px;">StarBucks</a> <a href="/tags/T1/" style="font-size: 13.33px;">T1</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/University/" style="font-size: 14.17px;">University</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.83px;">Yuxiao Dong</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bert/" style="font-size: 10px;">bert</a> <a href="/tags/bing/" style="font-size: 10px;">bing</a> <a href="/tags/bug/" style="font-size: 10px;">bug</a> <a href="/tags/causal-churn-word/" style="font-size: 10px;">causal churn word</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 10px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chapter04/" style="font-size: 10px;">chapter04</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/code/" style="font-size: 10px;">code</a> <a href="/tags/coding/" style="font-size: 17.5px;">coding</a> <a href="/tags/cold%F0%9F%98%B7/" style="font-size: 10px;">cold😷</a> <a href="/tags/dalle3/" style="font-size: 10px;">dalle3</a> <a href="/tags/database/" style="font-size: 13.33px;">database</a> <a href="/tags/debug/" style="font-size: 11.67px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10.83px;">deep neural network</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/dowhy/" style="font-size: 10.83px;">dowhy</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/explainer/" style="font-size: 10.83px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/game/" style="font-size: 10px;">game</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gym/" style="font-size: 11.67px;">gym</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/happy/" style="font-size: 10px;">happy</a> <a href="/tags/homework/" style="font-size: 10px;">homework</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/instructor/" style="font-size: 12.5px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 12.5px;">intern00</a> <a href="/tags/internship/" style="font-size: 15.83px;">internship</a> <a href="/tags/introduction/" style="font-size: 11.67px;">introduction</a> <a href="/tags/kfc/" style="font-size: 10px;">kfc</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/l2/" style="font-size: 10px;">l2</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/m/" style="font-size: 10px;">m</a> <a href="/tags/mlp/" style="font-size: 10px;">mlp</a> <a href="/tags/mnist/" style="font-size: 10px;">mnist</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10.83px;">neuromorphic computing</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/one-piece/" style="font-size: 10px;">one piece</a> <a href="/tags/openai/" style="font-size: 10px;">openai</a> <a href="/tags/os/" style="font-size: 14.17px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/p1/" style="font-size: 10px;">p1</a> <a href="/tags/p2/" style="font-size: 10px;">p2</a> <a href="/tags/paper/" style="font-size: 19.17px;">paper</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 12.5px;">prml</a> <a href="/tags/pytorch/" style="font-size: 10px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/redemption/" style="font-size: 10px;">redemption</a> <a href="/tags/research-vs-coursework/" style="font-size: 10px;">research vs coursework</a> <a href="/tags/shap/" style="font-size: 11.67px;">shap</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/spike/" style="font-size: 10px;">spike</a> <a href="/tags/starbucks/" style="font-size: 10px;">starbucks</a> <a href="/tags/tensor-vs-ndarray/" style="font-size: 10px;">tensor vs ndarray</a> <a href="/tags/third-place/" style="font-size: 10px;">third place</a> <a href="/tags/thu/" style="font-size: 10px;">thu</a> <a href="/tags/tips/" style="font-size: 10.83px;">tips</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/wbg%E8%AF%AD%E9%9F%B3-uzi/" style="font-size: 10px;">wbg语音-uzi</a> <a href="/tags/word/" style="font-size: 10px;">word</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/youth/" style="font-size: 10px;">youth</a> <a href="/tags/zeus/" style="font-size: 10px;">zeus</a> <a href="/tags/%E4%B8%83%E5%A4%95/" style="font-size: 10px;">七夕</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 18.33px;">专业知识</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E5%86%8D%E4%B9%9F%E4%B8%8D%E7%94%A8%E8%A2%AB%E7%BA%A6%E5%AE%9A%E6%9D%9F%E7%BC%9A%E4%BA%86/" style="font-size: 10px;">再也不用被约定束缚了</a> <a href="/tags/%E5%86%8D%E8%A7%81%E7%BB%98%E6%A2%A8/" style="font-size: 10px;">再见绘梨</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%8D%9A%E4%BA%BA%E4%BC%A0/" style="font-size: 10px;">博人传</a> <a href="/tags/%E5%8F%AA%E8%A6%81%E6%9C%89%E7%9C%9F%E5%BF%83%E5%96%9C%E6%AC%A2%E7%9A%84%E4%B8%9C%E8%A5%BF-%E5%B0%B1%E8%83%BD%E5%8F%91%E5%87%BA%E5%85%89%E6%9D%A5/" style="font-size: 10px;">只要有真心喜欢的东西,就能发出光来</a> <a href="/tags/%E5%93%88%E5%B8%8C%E5%80%BC/" style="font-size: 10px;">哈希值</a> <a href="/tags/%E5%9C%A3%E5%A2%83/" style="font-size: 10px;">圣境</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.83px;">审稿意见</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%84%9F%E5%86%92/" style="font-size: 10px;">感冒</a> <a href="/tags/%E6%84%9F%E5%86%92%E7%97%8A%E6%84%88/" style="font-size: 10px;">感冒痊愈</a> <a href="/tags/%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84/" style="font-size: 10px;">拓扑结构</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%90%AC%E5%AE%B6/" style="font-size: 10px;">搬家</a> <a href="/tags/%E6%94%BE%E4%B8%8B/" style="font-size: 10px;">放下</a> <a href="/tags/%E6%95%99%E5%B8%88%E8%8A%82/" style="font-size: 10px;">教师节</a> <a href="/tags/%E6%9C%80%E9%95%BF%E7%9A%84%E7%94%B5%E5%BD%B1/" style="font-size: 10.83px;">最长的电影</a> <a href="/tags/%E6%9D%82%E9%A1%B9/" style="font-size: 10px;">杂项</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 14.17px;">毛概</a> <a href="/tags/%E6%B2%88%E6%9C%88/" style="font-size: 10px;">沈月</a> <a href="/tags/%E6%B2%A1%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95/" style="font-size: 10px;">没那么简单</a> <a href="/tags/%E7%81%AB%E9%BE%99%E5%A4%A7%E7%82%AC/" style="font-size: 10px;">火龙大炬</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E8%8E%93/" style="font-size: 10px;">莓</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 10px;">调研</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%F0%9F%93%A6/" style="font-size: 10px;">📦</a> <a href="/tags/%F0%9F%9B%80/" style="font-size: 10px;">🛀</a>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2023
                <span class="footer-info-sep"></span>
                野中晴
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    266.3k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    16:32
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="野中晴" class="lazyload">
            <div class="sidebar-author-name">野中晴</div>
            <div class="sidebar-description">Love is selfish.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">145</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">10</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">192</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

