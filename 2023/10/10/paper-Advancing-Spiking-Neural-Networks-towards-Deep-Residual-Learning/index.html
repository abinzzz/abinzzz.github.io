
    <!DOCTYPE html>
    <html lang="zh-CN"
            
          
    >
    <head>
    <meta charset="utf-8">
    

    

    
    <title>
        paper:Advancing Spiking Neural Networks towards Deep Residual Learning |
        
        Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CUbuntu%20Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
    
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/v4-font-face.min.css">

    
<link rel="stylesheet" href="/css/loader.css">

    <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   Advancing Spiking Neural Networks towards Deep Residual Learning十问 要了解深入，一个模型为什么好？ 以前的模型为什么不好？ 哪个关键点对性能提升最大？">
<meta property="og:type" content="article">
<meta property="og:title" content="paper:Advancing Spiking Neural Networks towards Deep Residual Learning">
<meta property="og:url" content="https://abinzzz.github.io/2023/10/10/paper-Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;, &#39;$&#39;]]}, messageStyle: &quot;none&quot; });   Advancing Spiking Neural Networks towards Deep Residual Learning十问 要了解深入，一个模型为什么好？ 以前的模型为什么不好？ 哪个关键点对性能提升最大？">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pbs.twimg.com/media/F8GNLfFbgAA8cyv?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F8Idyala0AEvq8v?format=png&amp;name=small">
<meta property="og:image" content="https://pbs.twimg.com/media/F8IdzJsaIAAhNix?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F8IgfLNaEAAfJMg?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F8IgfLNaEAAfJMg?format=png&amp;name=900x900">
<meta property="og:image" content="https://pbs.twimg.com/media/F8J3mHqaUAAw87q?format=png&amp;name=900x900">
<meta property="article:published_time" content="2023-10-10T06:45:05.000Z">
<meta property="article:modified_time" content="2023-10-11T13:33:46.831Z">
<meta property="article:author" content="野中晴">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="Spiking neural network">
<meta property="article:tag" content="neuromorphic computing">
<meta property="article:tag" content="deep neural network">
<meta property="article:tag" content="Advancing Spiking Neural Networks towards Deep Residual Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pbs.twimg.com/media/F8GNLfFbgAA8cyv?format=png&amp;name=900x900">
    
        <link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
        <link rel="shortcut icon" href="/images/favicon.ico">
    
    
        
<link rel="stylesheet" href="https://unpkg.com/typeface-source-code-pro@1.1.13/index.css">

    
    
<link rel="stylesheet" href="/css/style.css">

    
        
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

    
    
        
<link rel="stylesheet" href="https://unpkg.com/katex@0.16.7/dist/katex.min.css">

    
    
    
    
<script src="https://unpkg.com/pace-js@1.2.4/pace.min.js"></script>

    
        
<link rel="stylesheet" href="https://unpkg.com/wowjs@1.1.3/css/libs/animate.css">

        
<script src="https://unpkg.com/wowjs@1.1.3/dist/wow.min.js"></script>

        <script>
          new WOW({
            offset: 0,
            mobile: true,
            live: false
          }).init();
        </script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <body>
    
<div id='loader'>
  <div class="loading-left-bg"></div>
  <div class="loading-right-bg"></div>
  <div class="spinner-box">
    <div class="loading-taichi">
      <svg width="150" height="150" viewBox="0 0 1024 1024" class="icon" version="1.1" xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision">
      <path d="M303.5 432A80 80 0 0 1 291.5 592A80 80 0 0 1 303.5 432z" fill="#ff6e6b" />
      <path d="M512 65A447 447 0 0 1 512 959L512 929A417 417 0 0 0 512 95A417 417 0 0 0 512 929L512 959A447 447 0 0 1 512 65z" fill="#fd0d00" />
      <path d="M512 95A417 417 0 0 1 929 512A208.5 208.5 0 0 1 720.5 720.5L720.5 592A80 80 0 0 0 720.5 432A80 80 0 0 0 720.5 592L720.5 720.5A208.5 208.5 0 0 1 512 512A208.5 208.5 0 0 0 303.5 303.5A208.5 208.5 0 0 0 95 512A417 417 0 0 1 512 95" fill="#fd0d00" />
    </svg>
    </div>
    <div class="loading-word">Loading...</div>
  </div>
</div>
</div>

<script>
  const endLoading = function() {
    document.body.style.overflow = 'auto';
    document.getElementById('loader').classList.add("loading");
  }
  window.addEventListener('load', endLoading);
  document.getElementById('loader').addEventListener('click', endLoading);
</script>


    <div id="container">
        <div id="wrap">
            <header id="header">
    
        <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="paper:Advancing Spiking Neural Networks towards Deep Residual Learning" class="lazyload">
    
    <div id="header-outer" class="outer">
        <div id="header-title" class="inner">
            <div id="logo-wrap">
                
                    
                    
                        <a href="/" id="logo"><h1>paper:Advancing Spiking Neural Networks towards Deep Residual Learning</h1></a>
                    
                
            </div>
            
                
                
            
        </div>
        <div id="header-inner">
            <nav id="main-nav">
                <a id="main-nav-toggle" class="nav-icon"></a>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/">首页</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/archives">归档</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/about">关于</a>
                    </span>
                
                    <span class="main-nav-link-wrap">
                        <span class="main-nav-icon"></span>
                        <a class="main-nav-link" href="/friend">友链</a>
                    </span>
                
            </nav>
            <nav id="sub-nav">
                
                    <a id="nav-rss-link" class="nav-icon" href="/atom.xml"
                       title="RSS 订阅"></a>
                
                
            </nav>
            <div id="search-form-wrap">
                <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://abinzzz.github.io"></form>
            </div>
        </div>
    </div>
</header>

            <div id="content" class="outer">
                <section id="main"><article id="post-paper-Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning" class="h-entry article article-type-post"
         itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
    <div class="article-inner">
        <div class="article-meta">
            <div class="article-date wow slideInLeft">
    <a href="/2023/10/10/paper-Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" class="article-date-link">
        <time datetime="2023-10-10T06:45:05.000Z"
              itemprop="datePublished">2023-10-10</time>
    </a>
</div>

            
    <div class="article-category wow slideInLeft">
        <a class="article-category-link" href="/categories/paper/">paper</a>
    </div>


        </div>
        <div class="hr-line"></div>
        

        <div class="e-content article-entry" itemprop="articleBody">
            
                <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>

<h1 id="Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning"><a href="#Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning" class="headerlink" title="Advancing Spiking Neural Networks towards Deep Residual Learning"></a>Advancing Spiking Neural Networks towards Deep Residual Learning</h1><h2 id="十问"><a href="#十问" class="headerlink" title="十问"></a>十问</h2><ol>
<li>要了解深入，一个模型为什么好？</li>
<li>以前的模型为什么不好？</li>
<li>哪个关键点对性能提升最大？</li>
<li>编程怎么实现？</li>
<li>论文源代码和paper匹配度怎么样、都覆盖了吗  </li>
<li>哪些数学运算是关键的？</li>
<li>整个全流程是怎么走的？</li>
<li>数据是怎样流动的？其中是怎样变换的？各个变换有什么实际意义？</li>
<li>既要关注具体实现思路、也要关注上层抽象意义。作者灵感从何而来？</li>
<li>作者思考路线如何？</li>
</ol>
<h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p><strong>问题</strong>：尽管神经形态计算取得了快速进展，但<strong>脉冲神经网络（SNN）的容量和表示能力不足严重限制了其在实践中的应用范围</strong>。残差学习和捷径已被证明是训练深度神经网络的重要方法，但之前的工作很少评估它们对基于脉冲的通信和时空动力学特征的适用性。在本文中，我们首先发现<strong>这种疏忽会导致信息流受阻以及先前残差 SNN 中随之而来的退化问题</strong>。</p>
<p>为了解决问题而提出的<strong>想法</strong>：<strong>面向SNN的残差架构MS-ResNet</strong>，它<strong>建立了基于膜的捷径路径，并进一步证明了通过引入块动态等距理论可以在MS-ResNet中实现梯度范数相等，从而确保网络可以以对深度不敏感的方式表现良好</strong>。</p>
<p>所能够<strong>做到</strong>的事情：</p>
<ul>
<li>我们能够显着<strong>扩展直接训练的 SNN 的深度</strong>，例如，CIFAR-10 上最多 482 层，ImageNet 上最多 104 层，而不会观察到任何轻微的退化问题。</li>
<li><strong>为了验证 MS-ResNet 的有效性，在基于框架和神经形态数据集上进行了实验。</strong> MS-ResNet104 在 ImageNet 上取得了 <strong>76.02%</strong> 准确率的优异结果，这是我们在直接训练的 SNN 领域中所知的最高准确率。</li>
<li>还观察到了<strong>很高的能量效率</strong>，平均每个神经元只需要一个脉冲就可以对输入样本进行分类。</li>
</ul>
<p><br></p>
<h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1.INTRODUCTION"></a>1.INTRODUCTION</h2><p>脉冲神经网络具有丰富的神经元动力学和多样化的编码方案的独特特征，代表了典型的类脑计算模型。</p>
<p><strong>与ANN不同之处</strong>：SNN 能够以时空动态方式编码信息，并使用异步二进制脉冲活动进行事件驱动的通信。</p>
<p>神经形态计算的最新进展已经证明了它们在能源效率方面的巨大潜力。<strong>理论</strong>上，<strong>snn的计算能力至少与ann一样强大</strong>，万能近似定理也适用于snn。然而，在<strong>实践</strong>中，<strong>缺乏强大的SNN模型</strong>严重限制了它们执行复杂任务的能力。</p>
<p><strong>获得SNN模型</strong>的两种方法：</p>
<ul>
<li>从预训练的ANN模型转换</li>
<li>基于代理梯度的直接训练</li>
</ul>
<Br>

<h2 id="1-1-conversion-from-a-pretrained-ANN-model"><a href="#1-1-conversion-from-a-pretrained-ANN-model" class="headerlink" title="1.1 conversion from a pretrained ANN model"></a>1.1 conversion from a pretrained ANN model</h2><p>基本思想：<strong>基于 ReLU 的 ANN 中的激活值可以通过速率编码方案下 SNN 的平均触发率来近似</strong>。</p>
<p>在训练具有一定限制的 ANN 后，将预训练的 ANN 转换为其脉冲对应物是可行的。这样，转换方法就摆脱了不可微的脉冲激活函数带来的困境，在精度方面保持与人工神经网络的最小差距，并推广到大规模结构和数据集。然而，转换例程也存在固有缺陷。 ANN 模型的约束会导致精度差距，并且需要数百或数千个时间步长的长时间模拟才能完成推理，这会导致额外的延迟和能量消耗。因此，最近更多的工作开始关注时间步长的经济减少[15]、[16]，但当时间步长仅为几十时，仍然存在严重的精度损失。</p>
<p><br></p>
<h2 id="1-2-the-surrogate-gradient-based-direct-training"><a href="#1-2-the-surrogate-gradient-based-direct-training" class="headerlink" title="1.2 the surrogate gradient-based direct training"></a>1.2 the surrogate gradient-based direct training</h2><p>基本思想：<strong>代理梯度函数构成了非平滑脉冲活动的连续放松，以实现标准的时间反向传播(BPTT)，从头开始训练SNN</strong>。</p>
<p>直接训练算法表现出多种多样的形式，如梯度函数[19]和编码方案[20]，[21]。这些<strong>直接训练的网络学习有效地编码信息</strong>，因此需要比转换网络少得多的时间步长，这对于在低功耗的神经形态硬件上实现特别有吸引力。然而，<strong>直接训练SNNs的一个突出问题是模型规模有限</strong>。神经网络的深度对其成功无疑至关重要，但<strong>早期直接训练的SNN工作主要集中在浅层结构和简单任务</strong>。受深度人工神经网络表示能力的启发，最近的工作逐渐从全连接网络发展到卷积网络，再到更高级的ResNet。由于残差学习和捷径已经被证明是深化神经网络的重要方式，并且被广泛采用[27]-[31]，因此自然需要寻找将它们应用于snn的方法。</p>
<h2 id="1-3-引出观点"><a href="#1-3-引出观点" class="headerlink" title="1.3 引出观点"></a>1.3 引出观点</h2><p>然而，<strong>之前的工作很少评估残差学习对基于脉冲的通信方案和 SNN 时空动态的内在特征的适用性</strong>。<strong>直接采用成熟的 ANN 结构似乎已经成为 SNN 结构设计的刻板印象，但我们发现几乎所有以前的工作都采用的规范 ResNet 的简单移植方式并不适合 SNN 的训练</strong>。这个问题将在我们的深度分析实验中表现为，随着网络的加深，训练集和测试集的准确性都会下降，这被称为<strong>退化问题</strong>。因此，构建强大的深度 SNN 模型仍然是艰巨且无成果的，这严重阻碍了其在各种任务中的使用，并且非常<strong>需要面向 SNN 的残差块设计来获得具有良好可扩展性的 SNN 模型</strong>，以追求足够的表示能力足够的容量。</p>
<p>本文报告：</p>
<ul>
<li><strong>应用vanilla ResNet训练深度snn时的退化问题</strong>。随着深度的增加，spiking PlainNet和vanilla ResNet的精度提高趋势分别在14层和令人惊讶的是，只有20层。</li>
<li><strong>为解决退化问题并充分释放深度snn的潜力，本文提出一种面向snn的残差架构MS-ResNet</strong>: 其中删除了块间LIF(·)，以构建一个干净的快捷连接，在整个网络中进行身份映射。然后分别从前向传播和梯度范数两个角度分析了新结构的优越性。观察到MS-ResNet在推理时可以避免无效的残差表示，即块的残差路径对网络的整体容量没有贡献的情况。</li>
<li><strong>证明</strong>：<strong>MS-ResNet可以通过块动态等距框架实现梯度范数相等</strong>，而vanilla spike ResNet不能</li>
<li>好的可<strong>扩展性</strong>：可以在CIFAR-10上至少扩展到482层，在ImageNet上至少扩展到104层，而没有观察到退化问题。</li>
<li>评估了MS-ResNet在各种数据集上的<strong>有效性</strong>，并获得了<strong>精度结果</strong>：据我们所知，这些结果是直接训练的snn中最先进的，与转换方法有竞争，但时间步数极少。</li>
<li><strong>实验结果</strong>：在脉冲活动稀疏的情况下，该算法在处理事件流时最高可节省24倍的能量。</li>
</ul>
<p><br></p>
<p>本文的其余部分组织如下：</p>
<ul>
<li>在第二节中，我们介绍了 SNN 的预备知识以及我们源自退化问题的动机</li>
<li>在第三节中，我们提出了面向 SNN 的残差块并分析了其优越性</li>
<li>第四节提供了基准实验和能源效率估算</li>
<li>第五节中讨论一些有趣的细节</li>
<li>第六节中进行总结</li>
</ul>
<Br>

<h2 id="2-PRELIMINARIES-AND-MOTIVATION"><a href="#2-PRELIMINARIES-AND-MOTIVATION" class="headerlink" title="2.PRELIMINARIES AND MOTIVATION"></a>2.PRELIMINARIES AND MOTIVATION</h2><h2 id="2-1-Preliminaries-of-SNNs"><a href="#2-1-Preliminaries-of-SNNs" class="headerlink" title="2.1 Preliminaries of SNNs"></a>2.1 Preliminaries of SNNs</h2><p>SNN 和 ANN 的<strong>基本区别</strong>: 主要计算元素，即<strong>神经元</strong>。在人工神经网络中，生物神经元被抽象为具有非线性变换的信息聚合单元。相反，在 SNN 中，膜电位动力学和脉冲通信方案被更接近地模仿。</p>
<p>在这项工作中，我们选择 Wu 等人提出的<strong>迭代 LIF 模型</strong>。对于SNN建模，可以表示为：</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$u_i^t$ = $\tau_{mem}$ · $u_i^{t-1}$ + $\sum_{j=1}^{n} \omega_{ij} o_j^t$ <strong>式(1),下面是式(2)</strong></p>
<script type="math/tex; mode=display">o_i^t = g(u_i^t)=\left\{
\begin{matrix}
1 , if u_i^t - V_{th} \geq 0 \\
0 , otherwise 
\end{matrix}
\right.</script><ul>
<li>$u_i^t$: 时间步 t 时层中第 i 个神经元的膜电位</li>
<li>$\tau_{mem}$: 泄漏的衰减因子</li>
<li>突触输入是前一层输出脉冲的加权和</li>
<li>g(·): 由阈值$V_{th}$控制的放电活动</li>
<li>一旦脉冲放电，$u_i^t$将随后重置为 $V_{reset}$，即 $o^{t−1}_i$ = 1。</li>
</ul>
<p><br></p>
<p><strong>替代梯度</strong>定义为:&nbsp;&nbsp; $\frac{\partial o_i^t}{\partial u_i^t}$ = $\frac{1}{a}$sign(|$u_i^t - V_{th}$| $\leq \frac{a}{2}$) <strong>式(3)</strong></p>
<p>引入系数a保证函数的积分为1，这样BPTT就可以和主流深度学习库的autograd框架一起进行</p>
<p><br></p>
<p>对于计算机视觉任务，{Conv-BN-Nonlinearity} 的堆叠是一种通用架构，遵循 VGG 网络的主要原理，在本工作中被称为 <strong>PlainNet</strong>。我们采用最新的 <strong>TDBN</strong>(threshold-dependent batch normalization) 技术作为脉冲模型中的传统 BN，并将其表示为：</p>
<p>$u_i^t$ = $\tau_{mem}u_i^{t-1}$ + TDBN($I_i^t,\mu_{ci},\sigma_{ci}^2,V_{th}$) <strong>式(4)</strong></p>
<ul>
<li>$\mu_{ci}$: 在小批量连续输入 {$I_i^t= \sum_{j=1}^n\omega_{ij}o_j^t$ | t = 1, …, T} 上按维度计算的通道<strong>平均值</strong>；</li>
<li>$\sigma_{ci}^2$ : 在小批量连续输入 {$I_i^t= \sum_{j=1}^n\omega_{ij}o_j^t$|t = 1, …, T} 上按维度计算的通道<strong>变化值</strong>；</li>
</ul>
<p><br></p>
<p>可以将<strong>shortcut connection</strong>插入到 PlainNet 中，并将其转换为其残差对应项，可以写为:</p>
<p>$o^{l+1} = LIF(F(o^l;W^{l+1}) + o^l)$ <strong>式(5)</strong></p>
<ul>
<li>F(·): 以$W^{l+1}$作为参数的残差路径中的函数组</li>
<li>$o^l$: 模拟周期 T 上的脉冲序列输出向量</li>
<li>l：层索引</li>
</ul>
<p><img src="https://pbs.twimg.com/media/F8GNLfFbgAA8cyv?format=png&amp;name=900x900" alt=""><br>在模型的最后，全连接层会计算整个模拟过程中最后一个特征图的每个神经元的脉冲数量，并作为分类器做出最终决策。从非脉冲ResNet直接移植残差块如图1所示，这是之前作品中的常规做法</p>
<p><br></p>
<h2 id="2-2-The-Degradation-Problem-in-SNNs"><a href="#2-2-The-Degradation-Problem-in-SNNs" class="headerlink" title="2.2 The Degradation Problem in SNNs"></a>2.2 The Degradation Problem in SNNs</h2><p>训练更高质量模型的<strong>最直接方法是增加模型的大小</strong>，特别是考虑到大量标记训练数据的可用性。然而，在 SNN 领域，<strong>直接加深网络似乎从来都不是获得更令人满意</strong>的准确性的可靠方法</p>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F8Idyala0AEvq8v?format=png&amp;name=small" alt=""></p>
<p>我们想探索现有的<strong>面向 SNN 的 BN 技术</strong>和<strong>快捷连接</strong>如何有助于脉冲模型的<strong>可扩展性</strong>，因此在 CIFAR-10上进行了以<strong>深度为唯一变量</strong>的实验。应该指出的是，我们的重点是<strong>网络对其深度的响应和潜在的退化问题</strong>，而不是获得最先进的结果，因此我们使用深度但相对狭窄的架构，如表一所示。</p>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F8IdzJsaIAAhNix?format=png&amp;name=900x900" alt=""><br>表二显示了<strong>深度分析</strong>的结果。 PlainNet 的准确率在 14 层深度时开始下降，令人惊讶的是，<strong>采用快捷连接</strong>只会将峰值转移到 20 层。尽管峰值后有轻微的斜率，但当脉冲 ResNet 达到 56 层时，<strong>精度仍然严重下降</strong>。尽管采用了 TDBN 和快捷连接，但退化问题确实存在，这表明<strong>将 ResNet 直接移植到 SNN 并不能有效地发挥作用</strong>，并且使得构建足够深和强大的 SNN 模型成为一项艰巨的任务</p>
<p>有趣的是，我们注意到，当我们<strong>删除残差块之间的脉冲激活函数LIF(·)并将残差路径中的脉冲激活函数保持为非线性时，在56层深度内的退化问题可以得到缓解</strong>。基于这一观察，本文主要将退化的症结确定为块间LIF(·)，并将在下一节中提供进一步的见解以及所提出的架构。</p>
<p><br></p>
<h2 id="3-SPIKING-RESIDUAL-BLOCKS"><a href="#3-SPIKING-RESIDUAL-BLOCKS" class="headerlink" title="3.SPIKING RESIDUAL BLOCKS"></a>3.SPIKING RESIDUAL BLOCKS</h2><p>在本节中，我们将尝试解释<strong>为什么块间LIF(·) 阻碍了普通 ResNet 在 SNN 特性方面的适用性</strong>.</p>
<p>介绍了我们模型的相应优越性以及关于新设计的一些其他问题。我们的模型有三个<strong>优点</strong>:</p>
<ul>
<li><strong>畅通无阻的推理流</strong>:可以避免无效的剩余表达式和工作负载不平衡</li>
<li><strong>实现块动态等距</strong>:这是避免梯度消失或爆炸问题的重要指标</li>
<li>有意维护的<strong>主要节能功能</strong>将适合在神经形态硬件上进一步实现</li>
</ul>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F8IgfLNaEAAfJMg?format=png&amp;name=900x900" alt=""></p>
<p>我们提出的脉冲残差块如图 2a 所示。<strong>去掉层间LIF(·)</strong>，构建一条贯穿整个网络的捷径，主要处理来自不同块的剩余路径的汇合。同时，在<strong>残差路径的顶部放置一个额外的LIF(·)</strong>，将消息转换为稀疏脉冲并将其发送到后续神经元。shortcut的流量在概念上更接近神经元膜突触输入的总和，而不是原始结构中的脉冲活动，因此我们将新结构命名为Membrane-Shortcut ResNet (MS-ResNet) 以强调这一变化在快捷方式中。</p>
<p><br></p>
<h2 id="3-1-Residual-Representation-and-Workload-Balance-at-Inference"><a href="#3-1-Residual-Representation-and-Workload-Balance-at-Inference" class="headerlink" title="3.1 Residual Representation and Workload Balance at Inference"></a>3.1 Residual Representation and Workload Balance at Inference</h2><p>一般来说，在残差学习中，我们希望残差路径能够参考shortcut的恒等映射来学习相对较小的扰动，并且随着网络的加深，这些扰动将累积成最优变换。</p>
<p>在vanilla spiking ResNet 中，<strong>恒等映射可以通过块轻松实现</strong>，即，当我们将阈值 $V_{th}$ 设置为小于1（例如，0.5）并且残差部分接近于零：  </p>
<p>$o^{l+1}_i = LIF(F(o^l;W^{l+1}_i) + o^l_i) \approx g(o_i^l) = o_i^l$  <strong>式(6)</strong></p>
<p><br></p>
<p>因此，可以推断，<strong>在vanilla spiking ResNet的退化问题中，实现恒等映射并不是真正重要的</strong>。对于逐层累加，我们主要关注<strong>每个残差块输出的变化，并倾向于检查当后续神经元的触发状态与接收后施加恒等映射的神经元不同时需要满足什么条件来自剩余路径的信息</strong><br>: </p>
<p>$(o_i^{t,l},o_i^{t,l+1}) \in S$ = {$s_i = (o_i^{t,l} , o_i^{t,l+1})|o_i^{t,l} + o_i^{t,l+1} = 1 , o_i^t\in$ {0,1}}</p>
<p><br></p>
<p><strong>成功改变激活状态的概率</strong>可以分解为两个条件，可以写为:</p>
<p>P($s_i \in S$) = P($F(o^{t,l} + o_i^{t,l} &gt; V_{th}) | o_i^{t,l} = 0$)P($o_i^{t,l} = 0$) + P($F(o^{t,l} + o_i^{t,l} &lt; V_{th}) | o_i^{t,l} = 1$)P($o_i^{t,l} = 1$) <strong>式(7)</strong></p>
<p>为了简单起见，这里省略了<strong>衰减的膜电位</strong>部分。</p>
<p><br></p>
<p>假设残差路径的输出是具有概率密度函数 φ(u) 的连续随机变量，并且可以近似为独立于单个神经元的输入,我们有：</p>
<p>P($s_i \in S$) = P($o_i^{t,l} = 0$)$\int_{V_{th}}^{+ \infty} \phi(u)du$ + P($o_i^{t,l} = 1$)$\int_{- \infty}^{V_{th} -1} \phi(u)du$</p>
<p><br></p>
<p>如果残差输出满足正态分布F($o^{t,l}$) ~ N($0,\sigma^2_x$)，且郑等人采用$\sigma_x$ = $V_{th}$ = 0.5，则P ($s_i \in S$)≈16%。对于剩余的84%的神经元(占很大一部分)，剩余路径所传递的内容根本不会影响当前状态，只会作为衰减的膜电位对以后的时间步做出贡献。此外，残差路径贡献的增量会被激活神经元由于复位机制而遗忘，使得该块的残差表示在空间和时间维度上都完全无效.</p>
<p><br></p>
<p>特别是，当仅考虑来自在<strong>l层和l+1层之间不经历状态变化的神经元的输入</strong>时，我们有：</p>
<p>$o_i^{t,l+2}$ = LIF($F(o^{t,l+1};W^{l+2})+o_i^{t,l+1}$) = LIF($F(o^{t,l};W^{l+2})+o_i^{t,l}$)</p>
<p>这表明第l+1层的残差路径在时间步长上对网络的整体容量没有贡献，我们将其视为无效的残差表示。</p>
<p><br></p>
<p>一个简单的解决方案可能是<strong>增加残差输出$\sigma_x^2$的方差，这会导致改变神经元状态的概率更高，并且无效的残差路径更少</strong>。然而，我们无意在这里讨论最优方差，因为这种解决方案永远无法完全防止无效的残差表示，并且<strong>过分强调残差路径相当于降低恒等映射的重要性，这违反了ResNet的扰动学习规则</strong>。实际上，块间LIF(·) 中的门函数使块的输出在某种程度上只是两者之间的选择，而不是我们所期望的由残差路径辅助的主要shortcut.</p>
<p>而对于MS-ResNet，去除块间LIF(·)为信息流提供了一条干净的路径，并且残差路径的汇流不会被门控函数判断，因此<strong>无论神经元的放电状态是否实际改变，都不会存在无效的残差表示</strong>。<strong>小的残差</strong>表示总是可以累积为：</p>
<p>$I^{t,l+2}$=$I^{t,l+1} + F(LIF(I^{t,l+1};W^{l+2}))$=$I^{t,l}+\sum_{k=1}^{l+1}F(LIF(I^{t,k});W^{k+1}),$ <strong>式(10)</strong></p>
<ul>
<li>其中$I^{t,l}$表示第l层的突触输入向量</li>
</ul>
<p><br></p>
<p>为了更直观地验证激活模式如何随着不同层的残差块而变化，我们采用结构相似性指数度量（SSIM）来量化相似性，其可以表示为：</p>
<p>SSIM(x,y) = $\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}$ <strong>式(10)</strong></p>
<ul>
<li>$\mu_x,\mu_y$: 图像 x 和 y 的均值</li>
<li>$\sigma_x,\sigma_y$: 图像 x 和 y 的方差</li>
<li>$\sigma_{xy}$:x 和 y 的协方差</li>
<li>C1、C2: 两个稳定常数</li>
</ul>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F8IgfLNaEAAfJMg?format=png&amp;name=900x900" alt=""><br>图像的像素值是特征图中神经元的放电率，采样点如图 2a 所示。 SSIM的值范围为-1和1，并且只有当两个图像相同时才等于1。</p>
<p><br></p>
<p><img src="https://pbs.twimg.com/media/F8J3mHqaUAAw87q?format=png&amp;name=900x900" alt=""><br>如果各层之间的激活模式变化均匀，它将在图 2b 的雷达图中显示为一个圆圈。尽管由于下采样层导致的异质性，严格的圆形并不适用于整个神经网络，但我们的 <strong>MS-ResNet 表现出了比普通脉冲 ResNet 更圆的曲线</strong>。特别是，在普通 ResNet-56 中，有 5 个层的 SSIM 值为 1，这意味着当信息流经残差块时，激活模式不会改变，并且这些层无法帮助特征提取。<strong>因此，后续层需要补偿前面层的不作为，这在随后的第 6 层发生巨大的信息变化中体现出来。整个网络的工作负载不平衡</strong>，尤其是在训练深度网络时，无疑反映了<strong>脉冲ResNet 的不合理性，并在 MS-ResNet 中得到了有效缓解</strong>。</p>
<p><br></p>
<h2 id="3-2-Gradient-Evolvement-at-Backpropagation"><a href="#3-2-Gradient-Evolvement-at-Backpropagation" class="headerlink" title="3.2 Gradient Evolvement at Backpropagation"></a>3.2 Gradient Evolvement at Backpropagation</h2><p><strong>动态等距</strong>:输入输出雅可比矩阵奇异值的平衡,近年来得到发展，作为行为良好的神经网络的理论解释。</p>
<p>在本小节中，我们使用块动态等距框架[32]分析 MS-ResNet 可以实现梯度范数相等，而普通脉冲ResNet 则不能。</p>
<p><br></p>
<p>不失一般性，<strong>神经网络可以被视为一系列块</strong>：</p>
<p>f($x_0$)=$f^L_{\theta^L}\circ f^{L-1}_{\theta^{L-1}}\circ ··· \circ<br> f^1_{\theta_1}(x_0)$, <strong>式(12)</strong></p>
<ul>
<li>$θ^i$: 第i层的参数矩阵</li>
</ul>
<p><br></p>
<p><strong>简单的约定</strong>：</p>
<ul>
<li>$\frac{\partial f^j}{\partial f^{j-1}}$ = $J_j$</li>
<li>$\phi(J)$ = $tr(J)$的期望</li>
<li>$\psi(J)$ = $\phi(J^2)-\phi(J)^2$</li>
</ul>
<p><br></p>
<p><strong>定义 1(块动态等距)</strong>: 考虑一个可以表示为<strong>式(12)</strong> 的神经网络，第j个块的雅可比矩阵表示为$J_j$。<strong>如果任意的j, $\phi(J_j,J_j^T)$ ≈ 1 且$\psi(J_j,J_j^T)$≈ 0，则网络实现块动态测距</strong>。</p>
<p><br></p>
<p><strong>引理1</strong>: 假设对于神经网络中的 L 个连续块中的每一个，我们有$\phi(J_iJ_i^T)=\omega + \tau\phi(\tilde{J_i}\tilde{J_i^T})$其中 $J_i$ 是其雅可比矩阵。给定 λ ∈ $N^+$ &lt; L，如果if $C_L^{\lambda}(1 − ω)^λ$ 和$C_L^{\lambda}\tau^{\lambda}$足够小，当两个网络都有任意的i , $\phi(J_iJ_i^T)$≈1 时，网络将像 λ 层网络一样稳定。</p>
<p>根据定义1和引理1，我们可以判断网络是否能够在残差SNN的具体情况中实现梯度范数相等。</p>
<p><strong>命题 1</strong>：假设两个神经网络均由 L 个连续块组成，普通脉冲ResNet 无法实现块动态等距，而 MS-ResNet 可以作为 λ 层网络保持稳定，满足 $\phi(J_iJ_i^T)$≈1 和 λ ∈ $N^+$ &lt; L</p>
<p>证明。根据附录中的引理2，整个网络的雅可比矩阵可以分解为其块的雅可比矩阵的乘法。我们期望每个块应该满足 ∀ i,$\phi(J_iJ_i^T)$≈1，从而提供稳定的梯度演化。神经网络的常见组成部分及其谱矩已在文献中进行了总结，因此这里主要关注LIF(·)的分析。</p>
<p>式(2)中定义的门函数及其在式(3)中的代理梯度都是逐元素运算，因此g(·)的雅可比矩阵$J_g$是一个对角矩阵，其元素为0或$\frac{1}{a}$。其<strong>概率密度函数</strong>满足:</p>
<p>$\rho_{J_g}(z)$ = (1-p)$\delta(z)$ + p$\delta(z - \frac{1}{a})$ <strong>式(13)</strong></p>
<ul>
<li>p 表示$(x - V_{th}) \leq \frac{a}{2}$为真的概率</li>
</ul>
<p><br></p>
<p>因此我们有：</p>
<p>$\phi(J_gJ_g^T)$ = $\int_R z((1-p)\delta(z)+p\delta(z-\frac{1}{a^2}))dz$ = $\frac{p}{a^2}$ <strong>式(14)</strong></p>
<p>$\psi(J_gJ_g^T)$=$\int_R z^2((1-p)\delta(z)+p\delta(z-\frac{1}{a^2}))dz$ - $\phi(J_gJ_g^T)^2$ = $\frac{p-p^2}{a^4}$ <strong>式(15)</strong></p>

            
        </div>
        <footer class="article-footer">
            <a data-url="https://abinzzz.github.io/2023/10/10/paper-Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" data-id="clnkcp3750000lw69brkf3ay0" data-title="paper:Advancing Spiking Neural Networks towards Deep Residual Learning"
               class="article-share-link">分享</a>
            
            
            
            
    <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" rel="tag">Advancing Spiking Neural Networks towards Deep Residual Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spiking-neural-network/" rel="tag">Spiking neural network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-neural-network/" rel="tag">deep neural network</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/neuromorphic-computing/" rel="tag">neuromorphic computing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/" rel="tag">paper</a></li></ul>


        </footer>
    </div>
    
        
    <nav id="article-nav" class="wow fadeInUp">
        
            <div class="article-nav-link-wrap article-nav-link-left">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="ResNet"
                         class="lazyload">
                
                <a href="/2023/10/11/ResNet/"></a>
                <div class="article-nav-caption">前一篇</div>
                <h3 class="article-nav-title">
                    
                        ResNet
                    
                </h3>
            </div>
        
        
            <div class="article-nav-link-wrap article-nav-link-right">
                
                    <img data-src="https://singyesterday.com/cmn/images/gallery/l/pic_200325_22.jpg" data-sizes="auto" alt="2023.10.10"
                         class="lazyload">
                
                <a href="/2023/10/10/2023-10-10/"></a>
                <div class="article-nav-caption">后一篇</div>
                <h3 class="article-nav-title">
                    
                        2023.10.10
                    
                </h3>
            </div>
        
    </nav>


    
</article>











</section>
                
                    <aside id="sidebar">
    <div class="sidebar-wrap wow fadeInRight">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="野中晴" class="lazyload">
            <div class="sidebar-author-name">野中晴</div>
            <div class="sidebar-description">Love is selfish.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">135</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">8</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">168</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">分类</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Essay/">Essay</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/GoAbroad/">GoAbroad</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/internship/">internship</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reading/">reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/">专业知识</a></li></ul>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">标签云</h3>
        <div class="widget tagcloud">
            <a href="/tags/0/" style="font-size: 10px;">0</a> <a href="/tags/3-1/" style="font-size: 10px;">3-1</a> <a href="/tags/AI/" style="font-size: 10px;">AI</a> <a href="/tags/AI-Ethics/" style="font-size: 10px;">AI Ethics</a> <a href="/tags/Advancing-Spiking-Neural-Networks-towards-Deep-Residual-Learning/" style="font-size: 10.91px;">Advancing Spiking Neural Networks towards Deep Residual Learning</a> <a href="/tags/Anything/" style="font-size: 10px;">Anything</a> <a href="/tags/Artificial-neural-networks/" style="font-size: 10px;">Artificial neural networks</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/CAS/" style="font-size: 10px;">CAS</a> <a href="/tags/CAS%E5%AE%9E%E4%B9%A0offer/" style="font-size: 10px;">CAS实习offer</a> <a href="/tags/CMU15-445/" style="font-size: 10px;">CMU15-445</a> <a href="/tags/CV/" style="font-size: 10px;">CV</a> <a href="/tags/Causal-Analysis-Churn/" style="font-size: 14.55px;">Causal Analysis Churn</a> <a href="/tags/Causal-Reasoning/" style="font-size: 10px;">Causal Reasoning</a> <a href="/tags/Cover-Letter/" style="font-size: 10px;">Cover Letter</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Deep-learning/" style="font-size: 10px;">Deep learning</a> <a href="/tags/DeepFM/" style="font-size: 10px;">DeepFM</a> <a href="/tags/English/" style="font-size: 10.91px;">English</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Essay/" style="font-size: 20px;">Essay</a> <a href="/tags/GEAR-5/" style="font-size: 10px;">GEAR-5</a> <a href="/tags/Git/" style="font-size: 10.91px;">Git</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GoAbroad/" style="font-size: 16.36px;">GoAbroad</a> <a href="/tags/Gumayusi/" style="font-size: 10px;">Gumayusi</a> <a href="/tags/HKU/" style="font-size: 10px;">HKU</a> <a href="/tags/IC/" style="font-size: 10px;">IC</a> <a href="/tags/IELTS/" style="font-size: 11.82px;">IELTS</a> <a href="/tags/IntelliJ-IDEA/" style="font-size: 10px;">IntelliJ IDEA</a> <a href="/tags/Jianfei-Chen/" style="font-size: 10px;">Jianfei Chen</a> <a href="/tags/Lec01/" style="font-size: 11.82px;">Lec01</a> <a href="/tags/Lec01s/" style="font-size: 10.91px;">Lec01s</a> <a href="/tags/Lime/" style="font-size: 10px;">Lime</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MIT6-S081/" style="font-size: 13.64px;">MIT6.S081</a> <a href="/tags/MS-ResNet/" style="font-size: 10px;">MS-ResNet</a> <a href="/tags/Missing-Semester/" style="font-size: 10px;">Missing Semester</a> <a href="/tags/NNDL/" style="font-size: 10px;">NNDL</a> <a href="/tags/NTU/" style="font-size: 10px;">NTU</a> <a href="/tags/Neuromorphic-computing/" style="font-size: 10px;">Neuromorphic computing</a> <a href="/tags/Qingyao-Ai/" style="font-size: 10.91px;">Qingyao Ai</a> <a href="/tags/RISC-V/" style="font-size: 10px;">RISC-V</a> <a href="/tags/ReadMemory/" style="font-size: 10px;">ReadMemory</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/SE/" style="font-size: 13.64px;">SE</a> <a href="/tags/SE-3-0/" style="font-size: 10px;">SE-3.0</a> <a href="/tags/SNN/" style="font-size: 10.91px;">SNN</a> <a href="/tags/SNN-vs-RNN/" style="font-size: 10px;">SNN vs RNN</a> <a href="/tags/STGgameAI/" style="font-size: 10px;">STGgameAI</a> <a href="/tags/Spiking-neural-network/" style="font-size: 10px;">Spiking neural network</a> <a href="/tags/Spiking-neural-networks/" style="font-size: 10px;">Spiking neural networks</a> <a href="/tags/StarBucks/" style="font-size: 12.73px;">StarBucks</a> <a href="/tags/T1/" style="font-size: 13.64px;">T1</a> <a href="/tags/THU/" style="font-size: 10px;">THU</a> <a href="/tags/TUM/" style="font-size: 10px;">TUM</a> <a href="/tags/Tai-Jiang-Mu/" style="font-size: 10px;">Tai-Jiang Mu</a> <a href="/tags/University/" style="font-size: 14.55px;">University</a> <a href="/tags/Yuxiao-Dong/" style="font-size: 10.91px;">Yuxiao Dong</a> <a href="/tags/author/" style="font-size: 10px;">author</a> <a href="/tags/bing/" style="font-size: 10px;">bing</a> <a href="/tags/bug/" style="font-size: 10px;">bug</a> <a href="/tags/causal-churn-word/" style="font-size: 10px;">causal churn word</a> <a href="/tags/chapter00/" style="font-size: 10px;">chapter00</a> <a href="/tags/chapter01/" style="font-size: 10px;">chapter01</a> <a href="/tags/chapter02/" style="font-size: 10px;">chapter02</a> <a href="/tags/chapter03/" style="font-size: 10px;">chapter03</a> <a href="/tags/chatgpt-prompt/" style="font-size: 10px;">chatgpt prompt</a> <a href="/tags/coding/" style="font-size: 17.27px;">coding</a> <a href="/tags/cold%F0%9F%98%B7/" style="font-size: 10px;">cold😷</a> <a href="/tags/dalle3/" style="font-size: 10px;">dalle3</a> <a href="/tags/database/" style="font-size: 12.73px;">database</a> <a href="/tags/debug/" style="font-size: 11.82px;">debug</a> <a href="/tags/deep-neural-network/" style="font-size: 10px;">deep neural network</a> <a href="/tags/discussion/" style="font-size: 10px;">discussion</a> <a href="/tags/dowhy/" style="font-size: 10.91px;">dowhy</a> <a href="/tags/echo/" style="font-size: 10px;">echo</a> <a href="/tags/email/" style="font-size: 10px;">email</a> <a href="/tags/explainer/" style="font-size: 10.91px;">explainer</a> <a href="/tags/fee/" style="font-size: 10px;">fee</a> <a href="/tags/game/" style="font-size: 10px;">game</a> <a href="/tags/gpt/" style="font-size: 10px;">gpt</a> <a href="/tags/gym/" style="font-size: 11.82px;">gym</a> <a href="/tags/hacker/" style="font-size: 10px;">hacker</a> <a href="/tags/handout/" style="font-size: 10px;">handout</a> <a href="/tags/happy/" style="font-size: 10px;">happy</a> <a href="/tags/homework/" style="font-size: 10px;">homework</a> <a href="/tags/imap/" style="font-size: 10px;">imap</a> <a href="/tags/instructor/" style="font-size: 12.73px;">instructor</a> <a href="/tags/intern-00/" style="font-size: 10px;">intern-00</a> <a href="/tags/intern00/" style="font-size: 10px;">intern00</a> <a href="/tags/internship/" style="font-size: 15.45px;">internship</a> <a href="/tags/introduction/" style="font-size: 11.82px;">introduction</a> <a href="/tags/kfc/" style="font-size: 10px;">kfc</a> <a href="/tags/l1/" style="font-size: 10px;">l1</a> <a href="/tags/llm/" style="font-size: 10px;">llm</a> <a href="/tags/m/" style="font-size: 10px;">m</a> <a href="/tags/model-evaluation/" style="font-size: 10px;">model evaluation</a> <a href="/tags/neuromorphic-computing/" style="font-size: 10px;">neuromorphic computing</a> <a href="/tags/note/" style="font-size: 10px;">note</a> <a href="/tags/one-piece/" style="font-size: 10px;">one piece</a> <a href="/tags/openai/" style="font-size: 10px;">openai</a> <a href="/tags/os/" style="font-size: 13.64px;">os</a> <a href="/tags/outlook/" style="font-size: 10px;">outlook</a> <a href="/tags/paper/" style="font-size: 19.09px;">paper</a> <a href="/tags/pku/" style="font-size: 10px;">pku</a> <a href="/tags/preparation/" style="font-size: 10px;">preparation</a> <a href="/tags/prml/" style="font-size: 12.73px;">prml</a> <a href="/tags/pytorch/" style="font-size: 10px;">pytorch</a> <a href="/tags/qemu/" style="font-size: 10px;">qemu</a> <a href="/tags/reading/" style="font-size: 10px;">reading</a> <a href="/tags/redemption/" style="font-size: 10px;">redemption</a> <a href="/tags/research-vs-coursework/" style="font-size: 10px;">research vs coursework</a> <a href="/tags/shap/" style="font-size: 11.82px;">shap</a> <a href="/tags/shell-vs-terminal/" style="font-size: 10px;">shell vs terminal</a> <a href="/tags/starbucks/" style="font-size: 10px;">starbucks</a> <a href="/tags/tensor-vs-ndarray/" style="font-size: 10px;">tensor vs ndarray</a> <a href="/tags/third-place/" style="font-size: 10px;">third place</a> <a href="/tags/tips/" style="font-size: 10px;">tips</a> <a href="/tags/tool/" style="font-size: 13.64px;">tool</a> <a href="/tags/wbg%E8%AF%AD%E9%9F%B3-uzi/" style="font-size: 10px;">wbg语音-uzi</a> <a href="/tags/word/" style="font-size: 10px;">word</a> <a href="/tags/writing/" style="font-size: 10px;">writing</a> <a href="/tags/xv6/" style="font-size: 10px;">xv6</a> <a href="/tags/youth/" style="font-size: 10px;">youth</a> <a href="/tags/zeus/" style="font-size: 10px;">zeus</a> <a href="/tags/%E4%B8%83%E5%A4%95/" style="font-size: 10px;">七夕</a> <a href="/tags/%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86/" style="font-size: 18.18px;">专业知识</a> <a href="/tags/%E4%B8%AD%E4%BB%8B/" style="font-size: 10px;">中介</a> <a href="/tags/%E4%B8%AD%E7%A7%91%E9%99%A2/" style="font-size: 10px;">中科院</a> <a href="/tags/%E5%86%8D%E4%B9%9F%E4%B8%8D%E7%94%A8%E8%A2%AB%E7%BA%A6%E5%AE%9A%E6%9D%9F%E7%BC%9A%E4%BA%86/" style="font-size: 10px;">再也不用被约定束缚了</a> <a href="/tags/%E5%86%8D%E8%A7%81%E7%BB%98%E6%A2%A8/" style="font-size: 10px;">再见绘梨</a> <a href="/tags/%E5%86%99%E4%BD%9C%E5%BF%83%E5%BE%97/" style="font-size: 10px;">写作心得</a> <a href="/tags/%E5%8D%9A%E4%BA%BA%E4%BC%A0/" style="font-size: 10px;">博人传</a> <a href="/tags/%E5%8F%AA%E8%A6%81%E6%9C%89%E7%9C%9F%E5%BF%83%E5%96%9C%E6%AC%A2%E7%9A%84%E4%B8%9C%E8%A5%BF-%E5%B0%B1%E8%83%BD%E5%8F%91%E5%87%BA%E5%85%89%E6%9D%A5/" style="font-size: 10px;">只要有真心喜欢的东西,就能发出光来</a> <a href="/tags/%E5%93%88%E5%B8%8C%E5%80%BC/" style="font-size: 10px;">哈希值</a> <a href="/tags/%E5%9C%A3%E5%A2%83/" style="font-size: 10px;">圣境</a> <a href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8A/" style="font-size: 10px;">大三上</a> <a href="/tags/%E5%AE%A1%E7%A8%BF%E6%84%8F%E8%A7%81/" style="font-size: 10.91px;">审稿意见</a> <a href="/tags/%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 10px;">快捷键</a> <a href="/tags/%E6%83%85%E7%BB%AA%E7%9A%84%E7%A7%98%E5%AF%86/" style="font-size: 10px;">情绪的秘密</a> <a href="/tags/%E6%84%9F%E5%86%92/" style="font-size: 10px;">感冒</a> <a href="/tags/%E6%84%9F%E5%86%92%E7%97%8A%E6%84%88/" style="font-size: 10px;">感冒痊愈</a> <a href="/tags/%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84/" style="font-size: 10px;">拓扑结构</a> <a href="/tags/%E6%8F%90%E9%97%AE/" style="font-size: 10px;">提问</a> <a href="/tags/%E6%90%AC%E5%AE%B6/" style="font-size: 10px;">搬家</a> <a href="/tags/%E6%94%BE%E4%B8%8B/" style="font-size: 10px;">放下</a> <a href="/tags/%E6%95%99%E5%B8%88%E8%8A%82/" style="font-size: 10px;">教师节</a> <a href="/tags/%E6%9C%80%E9%95%BF%E7%9A%84%E7%94%B5%E5%BD%B1/" style="font-size: 10.91px;">最长的电影</a> <a href="/tags/%E6%A6%82%E8%AE%BA/" style="font-size: 10px;">概论</a> <a href="/tags/%E6%AF%9B%E6%A6%82/" style="font-size: 14.55px;">毛概</a> <a href="/tags/%E6%B2%88%E6%9C%88/" style="font-size: 10px;">沈月</a> <a href="/tags/%E6%B2%A1%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95/" style="font-size: 10px;">没那么简单</a> <a href="/tags/%E7%81%AB%E9%BE%99%E5%A4%A7%E7%82%AC/" style="font-size: 10px;">火龙大炬</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 10px;">环境搭建</a> <a href="/tags/%E7%9F%A5%E8%A1%8C%E5%90%88%E4%B8%80/" style="font-size: 10px;">知行合一</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E5%BB%BA%E8%AE%AE%E4%B9%A6/" style="font-size: 10px;">系统开发建议书</a> <a href="/tags/%E8%8E%93/" style="font-size: 10px;">莓</a> <a href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" style="font-size: 10px;">虚拟机</a> <a href="/tags/%E8%AE%A1%E7%BD%91/" style="font-size: 10px;">计网</a> <a href="/tags/%E8%AF%BE%E7%A8%8B%E8%A1%A8/" style="font-size: 10px;">课程表</a> <a href="/tags/%E8%B0%83%E7%A0%94/" style="font-size: 10px;">调研</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">软件生命周期模型</a> <a href="/tags/%E9%99%B6%E7%93%B7/" style="font-size: 10px;">陶瓷</a> <a href="/tags/%F0%9F%93%A6/" style="font-size: 10px;">📦</a> <a href="/tags/%F0%9F%9B%80/" style="font-size: 10px;">🛀</a>
        </div>
    </div>


    
        
    <div class="widget-wrap wow fadeInRight">
        <h3 class="widget-title">归档</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">十月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a></li></ul>
        </div>
    </div>


    
</aside>

                
            </div>
            <footer id="footer" class="wow fadeInUp">
    <div style="width: 100%; overflow: hidden"><div class="footer-line"></div></div>
    <div class="outer">
        <div id="footer-info" class="inner">
            
            <div>
                <span class="icon-copyright"></span>
                2020-2023
                <span class="footer-info-sep"></span>
                野中晴
            </div>
            
                <div>
                    基于&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;
                    Theme.<a href="https://github.com/D-Sketon/hexo-theme-reimu" target="_blank">Reimu</a>
                </div>
            
            
                <div>
                    <span class="icon-brush"></span>
                    242.7k
                    &nbsp;|&nbsp;
                    <span class="icon-coffee"></span>
                    15:06
                </div>
            
            
                <div>
                    <span class="icon-eye"></span>
                    <span id="busuanzi_container_site_pv">总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></span>
                    &nbsp;|&nbsp;
                    <span class="icon-user"></span>
                    <span id="busuanzi_container_site_uv">总访客量&nbsp;<span id="busuanzi_value_site_uv"></span></span>
                </div>
            
        </div>
    </div>
</footer>

        </div>
        <nav id="mobile-nav">
    <div class="sidebar-wrap">
        <div class="sidebar-author">
            <img data-src="/avatar/avatar.jpg" data-sizes="auto" alt="野中晴" class="lazyload">
            <div class="sidebar-author-name">野中晴</div>
            <div class="sidebar-description">Love is selfish.</div>
        </div>
        <div class="sidebar-state">
            <div class="sidebar-state-article">
                <div>文章</div>
                <div class="sidebar-state-number">135</div>
            </div>
            <div class="sidebar-state-category">
                <div>分类</div>
                <div class="sidebar-state-number">8</div>
            </div>
            <div class="sidebar-state-tag">
                <div>标签</div>
                <div class="sidebar-state-number">168</div>
            </div>
        </div>
        <div class="sidebar-social">
            
                <div class=icon-github>
                    <a href=https://github.com/abinzzz itemprop="url" target="_blank"></a>
                </div>
            
        </div>
        <div class="sidebar-menu">
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">首页</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/archives"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">归档</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/about"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">关于</div>
                </div>
            
                <div class="sidebar-menu-link-wrap">
                    <a class="sidebar-menu-link-dummy" href="/friend"></a>
                    <span class="sidebar-menu-icon"></span>
                    <div class="sidebar-menu-link">友链</div>
                </div>
            
        </div>
    </div>
</nav>

        
<script src="https://unpkg.com/jquery@3.7.0/dist/jquery.min.js"></script>


<script src="https://unpkg.com/lazysizes@5.3.2/lazysizes.min.js"></script>


<script src="https://unpkg.com/clipboard@2.0.11/dist/clipboard.min.js"></script>



    
<script src="https://unpkg.com/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>



    
<script src="https://unpkg.com/busuanzi@2.3.0/bsz.pure.mini.js"></script>






<script src="/js/script.js"></script>
















    </div>
    <div class="site-search">
        <div class="algolia-popup popup">
            <div class="algolia-search">
                <span class="algolia-search-input-icon"></span>
                <div class="algolia-search-input" id="algolia-search-input"></div>
            </div>

            <div class="algolia-results">
                <div id="algolia-stats"></div>
                <div id="algolia-hits"></div>
                <div id="algolia-pagination" class="algolia-pagination"></div>
            </div>

            <span class="popup-btn-close"></span>
        </div>
    </div>
    <!-- hexo injector body_end start -->
<script src="/js/insertHighlight.js"></script>
<!-- hexo injector body_end end --></body>
    </html>

