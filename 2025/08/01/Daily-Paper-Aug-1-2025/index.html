<!DOCTYPE html>

<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <!--
        hexo-theme-suka © SukkaW
        GitHub: https://github.com/SukkaW/hexo-theme-suka
    -->

    <!-- ### Resource Hint ### -->

    <!-- ## DNS Prefetch ## -->
    <meta http-equiv="x-dns-prefetch-control" content="on">

<!-- busuanzi -->

    <link rel="dns-prefetch" href="//busuanzi.ibruce.info">


<!-- comment -->


    <link rel="dns-prefetch" href="//disqus.com">
    <link rel="dns-prefetch" href="//robin02.disqus.com">






<!-- analytics -->







    <!-- ## Preload ## -->
    
    <!-- Busuanzi -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" as="script">







    <!-- ### Meta & Title & Info ### -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <meta name="renderer" content="webkit">

    <!-- Title -->
    <title>Daily Paper | Aug 1, 2025 | blog</title>

    <!-- Favicons -->
    <link rel="icon" type="image&#x2F;ico" href="/img/blog.ico">

    <!-- ### Import File ### -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spectre.css@0.5.3"><style>
    body {
        background-color: #f8f9fa;
    }

    a, a:visited {
        color: blue;
    }

    a:active, a:focus, a:hover {
        color: blue;
        opacity: .75;
    }

    #post-content a,
    #post-content a:hover,
    #post-content a:focus,
    #post-content a:visited {
        color: blue;
        opacity: 1;
    }

    

    .post-entry .card-body a {
        color: red;
    }

    .avatar {
        background: red;
    }

    .navbar-link,
    .navbar-link:visited,
    .timeline .timeline-item .timeline-icon.icon-lg {
        color: red;
    }

    .navbar-link:hover {
        color: red;
        opacity: .8;
    }

    #search-input .btn,
    #disqus_click_btn,
    #disqus-switch-to-direct,
    #disqus-loadmore-button {
        background: red;
        border-color: red;
        color: #fff;
    }

    #post-toc a.post-toc-link,
    #post-toc a.post-toc-link:visited,
    .share-menu.menu .menu-item>a {
        color: red;
    }

    .share-menu.menu .menu-item>a:hover,
    .share-menu.menu .menu-item>a:focus,
    .share-menu.menu .menu-item>a:visited {
        color: #50596c;
        background: #f8f9fa;
        opacity: .85;
    }
</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/style.min.css">








    <!-- Prettify Theme -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css"></noscript>





<script>
/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
!function(t){"use strict";t.loadCSS||(t.loadCSS=function(){});var e=loadCSS.relpreload={};if(e.support=function(){var e;try{e=t.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),e.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.addEventListener?t.removeEventListener("load",a):t.attachEvent&&t.detachEvent("onload",a),t.setAttribute("onload",null),t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},e.poly=function(){if(!e.support())for(var a=t.document.getElementsByTagName("link"),n=0;n<a.length;n++){var o=a[n];"preload"!==o.rel||"style"!==o.getAttribute("as")||o.getAttribute("data-loadcss")||(o.setAttribute("data-loadcss",!0),e.bindMediaToggle(o))}},!e.support()){e.poly();var a=t.setInterval(e.poly,500);t.addEventListener?t.addEventListener("load",function(){e.poly(),t.clearInterval(a)}):t.attachEvent&&t.attachEvent("onload",function(){e.poly(),t.clearInterval(a)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:t.loadCSS=loadCSS}("undefined"!=typeof global?global:this);
</script>

    <!-- ### Site Verification ### -->
    


    <meta name="mobile-web-app-capable" content="yes"><meta name="application-name" content="blog"><meta name="msapplication-starturl" content="https://abinzzz.github.io"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="blog">

    <!-- ### The Open Graph & Twitter Card Protocol ### -->
    <meta property="og:title" content="Daily Paper | Aug 1, 2025 | blog"><meta property="og:site_name" content="blog"><meta property="og:type" content="article"><meta property="og:url" content="https://abinzzz.github.io/2025/08/01/Daily-Paper-Aug-1-2025/"><meta property="og:locale" content="zh-CN"><meta name="description" content="Table of Content  RecGPT Technical Report RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents Seed-Prover: Deep and Broad Reasoning for Automated Theore - ab - blog"><meta name="keywords" content="blog"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cdf9258cb8da5c8fa8b93.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce1b158cb8da5c8fa8ccc.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce1f458cb8da5c8fa8e74.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce23558cb8da5c8fa9044.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce2dc58cb8da5c8fa9451.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce33b58cb8da5c8fa96b0.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce38758cb8da5c8fa97b5.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce55c58cb8da5c8fa9834.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688ce61858cb8da5c8fa9874.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cfb1258cb8da5c8fa9ba6.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cf98658cb8da5c8fa9b85.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cfb9358cb8da5c8fa9bab.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cfbb358cb8da5c8fa9bb1.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cfc8758cb8da5c8fa9bc2.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cfe5558cb8da5c8fa9be3.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cff1458cb8da5c8fa9bed.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cff7258cb8da5c8fa9bf1.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cffc058cb8da5c8fa9c02.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688cfff458cb8da5c8fa9c06.png"><meta property="article:published_time" content="2025-08-01T15:22:35.000Z"><meta property="article:modified_time" content="2025-08-01T17:58:30.127Z"><meta property="og:updated_time" content="2025-08-01T17:58:30.127Z"><meta property="article:author" content="ab"><meta property="article:tag" content="blog"><meta name="twitter:card" content="summary">

    

    <!-- ### Canonical link ### -->
    <link rel="canonical" href="https://abinzzz.github.io/2025/08/01/Daily-Paper-Aug-1-2025/">

    <meta name="generator" content="Hexo 5.4.2">

    <!-- ### Analytics ### -->
    







    <!-- ### Structured Data ### -->
    



<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://abinzzz.github.io/2025/08/01/Daily-Paper-Aug-1-2025/",
    "@type": "BlogPosting",
    "logo": "https://abinzzz.github.io/img/blog.ico",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://abinzzz.github.io/2025/08/01/Daily-Paper-Aug-1-2025/"
    },
    "headline": "Daily Paper | Aug 1, 2025 | blog",
    
    "image": {
        "@type": "ImageObject",
        "url": "https://abinzzz.github.io/img/blog.ico"
    },
    
    "datePublished": "2025-08-01T15:22:35.000Z",
    "dateModified": "2025-08-01T17:58:30.127Z",
    "author": {
        "@type": "Person",
        "name": "ab",
        "image": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/avatar.jpg"
        },
        "description": "Welcome to my blog!"
    },
    "publisher": {
        "@type": "Organization",
        "name": "blog",
        "logo": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/blog.ico"
        }
    },
    
    "potentialAction": {
        "@type": "SearchAction",
        "target": "https://abinzzz.github.io/search?s={search_term_string}",
        "query-input": "required name=search_term_string"
    },
    
    "keywords": "blog",
    "description": "Table of Content  RecGPT Technical Report RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents Seed-Prover: Deep and Broad Reasoning for Automated Theore - ab - blog"
}
</script>



    <!-- ### Custom Head ### -->
    
</head>

    <body>
            

            <!-- ### Main content ### -->
            <!-- ## Header ##-->
<header>
    <h1 class="header-title text-center"><a href="/">blog</a></h1>

    <p class="text-center header-slogan">
        
            
                Welcome to my blog!
            
        
    </p>

    <nav class="navbar-section text-center">
    
        <a href="/" class="navbar-link">首页</a>
    
    
    <a href="/categories/" class="navbar-link">分类</a>
    
        <a href="/archives/" class="navbar-link">归档</a>
    
    
        <a href="/search" class="navbar-link">搜索</a>
    
    
    
    
</nav>
</header>

            
    <!-- ## Post ## -->
    <div class="post-container">
    <div id="post-card" class="card">
        
        <div class="card-item-container">
            <div class="card-inner-cell">
                <!-- # Post Header Info # -->
                <div class="card-header">
                    
    <h1 class="card-title h3 mb-2">Daily Paper | Aug 1, 2025</h1>




<div class="post-header-info">
    <p class="post-header-info-left text-gray">
        <img class="author-thumb lazyload" data-src="/img/avatar.jpg" src="/img/suka-lazyload.gif" alt="ab's Avatar">
        <span>2025-08-01</span>
        
            <span class="suka-devide-dot"></span>
            <a class="category-link" href="/categories/Daily-Paper/">Daily Paper</a>
        
        
        
    </p>
    <div class="post-header-info-right">
        
            <div class="dropdown dropdown-right">
<a class="dropdown-toggle" tabindex="0">分享本文</a>
<ul class="menu share-menu">
    <!-- Share Weibo -->
    

    <!-- Share Twitter -->
    

    <!-- Share Facebook -->
    

    <!-- Share Google+ -->
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    

    <!-- Share Telegram -->
    

    <!-- QRCode -->
    
    <li class="menu-item">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJQAAACUCAAAAABQV18IAAAB2ElEQVR42u3a227CMBBFUf7/p9vXKs3M2Q4gxZOdl0YU4oVk7Ln49XPD6yVKlChRJ6hXc/37UPF/8gz0HlFjUKeTDqC69xyfQ8YRNQt1HOj4ejUohadxRD0TRe7PcKJEnS161SZbLoTNIirqGahqo6w21LRIfjVKEHV7VBfQf/Lvx7IZUbdGxaJDmLjVoOmH9FbVRdStUd1ETJsp2ciXEllRI1DVYCRwS4NWn41BnqgRqBS4nT5k8T4FfaLmoEhymQquNOnAi6eobVG02dPdk8WSFElEzUF1Cxu97yZw+jKiZqFIEaJrHJHkEycXokagSEC3OlCViMbGuagxKLogpkYjgV/ekEVtg0oJIy2qpS/RPUvULFQ66EDAJFFNPxpRc1HpYA1pMKWDOrjAIWpLFE0crjTCacIgahaKJAy0yN8FdKTIL2oWKgVl1UJLD4GRDV3UTFRXzKCHaughVFzgELUlqit0kQSUbu5tcVfUCNRKIkALs/SwhKiZKNLYXpnsZFNf6jiI2hJFgjF6eJ42j+JEF7U9ihQjLhXC3o0SRI1EpeJsSj7wQTBRj0GlIgZ9T2oaiJqFWilSkGLIpUM9osagSAOoK8ynpCIlpqJmoe50iRIlStSf6xcyTRCvJYbNkgAAAABJRU5ErkJggg==" alt="QRCode">
    </li>
    

</ul>
</div>
        
    </div>
</div>
                </div>
                <div class="card-body">
                    
                        
                        
                            <div id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#table-of-content"><span class="post-toc-number">1.</span> <span class="post-toc-text"> Table of Content</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#recgpt-technical-report"><span class="post-toc-number">2.</span> <span class="post-toc-text"> RecGPT Technical Report</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#rlvmr-reinforcement-learning-with-verifiable-meta-reasoning-rewards-for-robust-long-horizon-agents"><span class="post-toc-number">3.</span> <span class="post-toc-text"> RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#seed-prover-deep-and-broad-reasoning-for-automated-theorem-proving"><span class="post-toc-number">4.</span> <span class="post-toc-text"> Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#where-to-show-demos-in-your-prompt-a-positional-bias-of-in-context-learning"><span class="post-toc-number">5.</span> <span class="post-toc-text"> Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#cot-self-instruct-building-high-quality-synthetic-prompts-for-reasoning-and-non-reasoning-tasks"><span class="post-toc-number">6.</span> <span class="post-toc-text"> CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#unilip-adapting-clip-for-unified-multimodal-understanding-generation-and-editing"><span class="post-toc-number">7.</span> <span class="post-toc-text"> UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#c3-a-bilingual-benchmark-for-spoken-dialogue-models-exploring-challenges-in-complex-conversations"><span class="post-toc-number">8.</span> <span class="post-toc-text"> C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#screencoder-advancing-visual-to-code-generation-for-front-end-automation-via-modular-multimodal-agents"><span class="post-toc-number">9.</span> <span class="post-toc-text"> ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#i-am-big-you-are-little-i-am-right-you-are-wrong"><span class="post-toc-number">10.</span> <span class="post-toc-text"> I Am Big, You Are Little; I Am Right, You Are Wrong</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#alphaearth-foundations-an-embedding-field-model-for-accurate-and-efficient-global-mapping-from-sparse-label-data"><span class="post-toc-number">11.</span> <span class="post-toc-text"> AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data</span></a></li></ol></div>
                        
                    
                    <article id="post-content">
                        <!-- omit in toc -->
<h2 id="table-of-content"><a class="markdownIt-Anchor" href="#table-of-content"></a> Table of Content</h2>
<ul>
<li><a href="#recgpt-technical-report">RecGPT Technical Report</a></li>
<li><a href="#rlvmr-reinforcement-learning-with-verifiable-meta-reasoning-rewards-for-robust-long-horizon-agents">RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents</a></li>
<li><a href="#seed-prover-deep-and-broad-reasoning-for-automated-theorem-proving">Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</a></li>
<li><a href="#where-to-show-demos-in-your-prompt-a-positional-bias-of-in-context-learning">Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning</a></li>
<li><a href="#cot-self-instruct-building-high-quality-synthetic-prompts-for-reasoning-and-non-reasoning-tasks">CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</a></li>
<li><a href="#unilip-adapting-clip-for-unified-multimodal-understanding-generation-and-editing">UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing</a></li>
<li><a href="#c3-a-bilingual-benchmark-for-spoken-dialogue-models-exploring-challenges-in-complex-conversations">C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</a></li>
<li><a href="#screencoder-advancing-visual-to-code-generation-for-front-end-automation-via-modular-multimodal-agents">ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents</a></li>
<li><a href="#i-am-big-you-are-little-i-am-right-you-are-wrong">I Am Big, You Are Little; I Am Right, You Are Wrong</a></li>
<li><a href="#alphaearth-foundations-an-embedding-field-model-for-accurate-and-efficient-global-mapping-from-sparse-label-data">AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data</a></li>
</ul>
<h2 id="recgpt-technical-report"><a class="markdownIt-Anchor" href="#recgpt-technical-report"></a> RecGPT Technical Report</h2>
<p>Author: Alibaba Taobao Team</p>
<p>RecGPT, developed by Taobao, integrates large language models into its recommender system to enable intent-centered personalization. This framework, fully deployed on the Taobao App, increased click-through rate by 6.33%, dwell time by 4.82%, and user-clicked item category diversity by 6.96%, while also mitigating the Matthew effect for merchants.</p>
<p><img src="https://pic1.imgdb.cn/item/688cdf9258cb8da5c8fa8b93.png" alt="" /></p>
<h2 id="rlvmr-reinforcement-learning-with-verifiable-meta-reasoning-rewards-for-robust-long-horizon-agents"><a class="markdownIt-Anchor" href="#rlvmr-reinforcement-learning-with-verifiable-meta-reasoning-rewards-for-robust-long-horizon-agents"></a> RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents</h2>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/Tencent/DigitalHuman/tree/main/RLVMR">https://github.com/Tencent/DigitalHuman/tree/main/RLVMR</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.22844">https://www.alphaxiv.org/abs/2507.22844</a></p>
<p><img src="https://pic1.imgdb.cn/item/688ce1b158cb8da5c8fa8ccc.png" alt="" /></p>
<p>RLVMR, developed by Tencent, trains Large Language Model agents to perform complex, long-horizon tasks by providing dense, verifiable meta-reasoning rewards during reinforcement learning. This approach leads to enhanced task success and generalization while significantly reducing inefficient exploration, such as repetitive and invalid actions, on benchmarks like ALFWorld and ScienceWorld.</p>
<p><img src="https://pic1.imgdb.cn/item/688ce1f458cb8da5c8fa8e74.png" alt="" /><br />
<img src="https://pic1.imgdb.cn/item/688ce23558cb8da5c8fa9044.png" alt="" /></p>
<h2 id="seed-prover-deep-and-broad-reasoning-for-automated-theorem-proving"><a class="markdownIt-Anchor" href="#seed-prover-deep-and-broad-reasoning-for-automated-theorem-proving"></a> Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</h2>
<p>Author: ByteDance Seed AI4Math</p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/ByteDance-Seed/Seed-Prover">https://github.com/ByteDance-Seed/Seed-Prover</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.23726">https://www.alphaxiv.org/abs/2507.23726</a></p>
<p>ByteDance Seed AI4Math’s Seed-Prover and Seed-Geometry are AI systems that successfully proved 5 out of 6 problems in the IMO 2025 competition, establishing new state-of-the-art results across several formal mathematical benchmarks including MiniF2F and PutnamBench. The systems achieve this through lemma-style proving, multi-tiered inference strategies that integrate iterative refinement and broad conjecture generation, and a fast, specialized geometry engine.</p>
<p><img src="https://pic1.imgdb.cn/item/688ce2dc58cb8da5c8fa9451.png" alt="" /></p>
<h2 id="where-to-show-demos-in-your-prompt-a-positional-bias-of-in-context-learning"><a class="markdownIt-Anchor" href="#where-to-show-demos-in-your-prompt-a-positional-bias-of-in-context-learning"></a> Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning</h2>
<p><img src="https://pic1.imgdb.cn/item/688ce33b58cb8da5c8fa96b0.png" alt="" /></p>
<p>This academic paper explores how the <strong>position of demonstrations</strong> (demos) within a Large Language Model’s (LLM) prompt affects its performance, a phenomenon termed <strong>DPP bias</strong>. Researchers evaluated ten open-source LLMs across various NLP tasks, discovering that <strong>placing demos at the beginning of the prompt</strong> generally leads to <strong>higher accuracy and greater prediction stability</strong>. Conversely, demos positioned at the end of the user message can drastically alter predictions without improving correctness. The study highlights that the <strong>optimal demo placement is not universal</strong>, varying significantly with both the <strong>LLM’s size and the specific task</strong>, emphasizing the critical need for <strong>model-aware and task-sensitive prompt design</strong>.</p>
<p><img src="https://pic1.imgdb.cn/item/688ce38758cb8da5c8fa97b5.png" alt="" /></p>
<h2 id="cot-self-instruct-building-high-quality-synthetic-prompts-for-reasoning-and-non-reasoning-tasks"><a class="markdownIt-Anchor" href="#cot-self-instruct-building-high-quality-synthetic-prompts-for-reasoning-and-non-reasoning-tasks"></a> CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</h2>
<p><img src="https://pic1.imgdb.cn/item/688ce55c58cb8da5c8fa9834.png" alt="" /></p>
<p>CoT-Self-Instruct, developed by FAIR at Meta, introduces a method for generating high-quality synthetic data for Large Language Models by combining Chain-of-Thought reasoning for instruction creation with robust, automated filtering mechanisms. This approach enables models trained on the synthetic data to achieve superior performance on both reasoning and general instruction-following benchmarks, often surpassing existing synthetic methods and human-annotated datasets.</p>
<p><img src="https://pic1.imgdb.cn/item/688ce61858cb8da5c8fa9874.png" alt="" /></p>
<h2 id="unilip-adapting-clip-for-unified-multimodal-understanding-generation-and-editing"><a class="markdownIt-Anchor" href="#unilip-adapting-clip-for-unified-multimodal-understanding-generation-and-editing"></a> UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing</h2>
<p><img src="https://pic1.imgdb.cn/item/688cfb1258cb8da5c8fa9ba6.png" alt="" /></p>
<p>n this paper, we propose UniLIP, which <strong>extends CLIP to reconstruction, generation and editing</strong>, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often <strong>require additional diffusion decoders or quantization</strong> to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance. In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dualcondition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLM’s strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of <strong>3.62</strong> on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks.</p>
<p><img src="https://pic1.imgdb.cn/item/688cf98658cb8da5c8fa9b85.png" alt="" /></p>
<h2 id="c3-a-bilingual-benchmark-for-spoken-dialogue-models-exploring-challenges-in-complex-conversations"><a class="markdownIt-Anchor" href="#c3-a-bilingual-benchmark-for-spoken-dialogue-models-exploring-challenges-in-complex-conversations"></a> C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</h2>
<p>Project Link: <a target="_blank" rel="noopener" href="https://step-out.github.io/C3-web/">https://step-out.github.io/C3-web/</a><br />
Github Link: <a target="_blank" rel="noopener" href="https://github.com/step-out/C3">https://github.com/step-out/C3</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.22968">https://www.alphaxiv.org/abs/2507.22968</a></p>
<p><img src="https://pic1.imgdb.cn/item/688cfb9358cb8da5c8fa9bab.png" alt="" /></p>
<p>This document introduces <strong>C3</strong>, a new <strong>bilingual benchmark</strong> designed to assess <strong>Spoken Dialogue Models (SDMs)</strong> in <strong>complex conversational scenarios</strong>. It highlights <strong>five key challenges</strong> in human speech: <strong>phonological ambiguity</strong>, <strong>semantic ambiguity</strong>, <strong>omission</strong>, <strong>coreference</strong>, and <strong>multi-turn interaction</strong>. The paper presents a <strong>dataset of 1,079 instances</strong> in both English and Chinese, evaluated using an <strong>LLM-based method</strong> that strongly correlates with human judgment. Experimental results reveal that <strong>ambiguity, especially semantic ambiguity in Chinese, poses significant difficulties</strong> for SDMs, and that <strong>omission is the most challenging aspect of context-dependency</strong>.</p>
<p><img src="https://pic1.imgdb.cn/item/688cfbb358cb8da5c8fa9bb1.png" alt="" /></p>
<h2 id="screencoder-advancing-visual-to-code-generation-for-front-end-automation-via-modular-multimodal-agents"><a class="markdownIt-Anchor" href="#screencoder-advancing-visual-to-code-generation-for-front-end-automation-via-modular-multimodal-agents"></a> ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents</h2>
<p><img src="https://pic1.imgdb.cn/item/688cfc8758cb8da5c8fa9bc2.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/leigest519/ScreenCoder">https://github.com/leigest519/ScreenCoder</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.22827">https://www.alphaxiv.org/abs/2507.22827</a></p>
<p>The source introduces <strong>ScreenCoder</strong>, a novel framework designed to <strong>automate the conversion of user interface (UI) designs into front-end code</strong>, specifically HTML/CSS. It highlights the limitations of existing methods that primarily rely on text-to-code generation and struggle with visual design nuances. ScreenCoder addresses this by employing a <strong>modular multi-agent system</strong> comprising three stages: <strong>grounding</strong> (detecting and labeling UI components), <strong>planning</strong> (structuring a hierarchical layout), and <strong>generation</strong> (synthesizing code from the structured layout). Furthermore, the paper describes how this framework functions as a <strong>scalable data engine</strong>, generating UI-image/code pairs to <strong>enhance vision-language models (VLMs)</strong> through supervised fine-tuning and reinforcement learning, ultimately achieving state-of-the-art performance in UI-to-code synthesis.</p>
<p><img src="https://pic1.imgdb.cn/item/688cfe5558cb8da5c8fa9be3.png" alt="" /></p>
<h2 id="i-am-big-you-are-little-i-am-right-you-are-wrong"><a class="markdownIt-Anchor" href="#i-am-big-you-are-little-i-am-right-you-are-wrong"></a> I Am Big, You Are Little; I Am Right, You Are Wrong</h2>
<p><img src="https://pic1.imgdb.cn/item/688cff1458cb8da5c8fa9bed.png" alt="" /></p>
<p>Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.23509">https://www.alphaxiv.org/abs/2507.23509</a><br />
Github Link: <a target="_blank" rel="noopener" href="https://github.com/ReX-XAI/ReX">https://github.com/ReX-XAI/ReX</a></p>
<p>This paper presents a study on <strong>minimal sufficient pixel sets (MPSs)</strong>, which are the smallest sets of pixels needed for an image classification model to make its original prediction. The authors <strong>investigate various neural network architectures</strong>, including Inception, ResNet, ConvNext, ViT, and EVA, to understand how different models <strong>process visual information</strong>. They specifically examine whether <strong>MPS size and location vary across models and architectures</strong>, and if <strong>misclassifications correlate with larger MPSs</strong>. The research utilizes <strong>ReX, a causal explainable AI (XAI) tool</strong>, to generate these pixel sets, demonstrating that models like ConvNext and EVA often rely on <strong>fewer, more spatially distinct pixels</strong>.</p>
<p><img src="https://pic1.imgdb.cn/item/688cff7258cb8da5c8fa9bf1.png" alt="" /></p>
<h2 id="alphaearth-foundations-an-embedding-field-model-for-accurate-and-efficient-global-mapping-from-sparse-label-data"><a class="markdownIt-Anchor" href="#alphaearth-foundations-an-embedding-field-model-for-accurate-and-efficient-global-mapping-from-sparse-label-data"></a> AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data</h2>
<p><img src="https://pic1.imgdb.cn/item/688cffc058cb8da5c8fa9c02.png" alt="" /></p>
<p>Paper Link: <a target="_blank" rel="noopener" href="https://papers-pdfs.assets.alphaxiv.org/2507.22291v1.pdf">https://papers-pdfs.assets.alphaxiv.org/2507.22291v1.pdf</a></p>
<p>The paper introduces <strong>AlphaEarth Foundations (AEF)</strong>, a novel embedding field model developed by Google DeepMind and Google, designed for <strong>accurate and efficient global mapping from sparse Earth observation data</strong>. AEF creates a <strong>highly general, geospatial representation</strong> by integrating spatial, temporal, and measurement contexts from various sources like <strong>Sentinel and Landsat imagery, LiDAR, climate data, and even text</strong>. This innovation addresses the challenge of creating high-quality global maps despite the <strong>scarcity of detailed, labeled data</strong>, consistently outperforming existing <strong>featurization approaches</strong> across diverse mapping tasks such as thematic mapping, biophysical variable estimation, and change detection. The authors plan to release a dataset of global, annual, analysis-ready <strong>embedding field layers</strong> from 2017 to 2024, enabling practitioners to leverage this technology without complex <strong>deep learning workflows</strong>.</p>
<p><img src="https://pic1.imgdb.cn/item/688cfff458cb8da5c8fa9c06.png" alt="" /></p>

                    </article>
                    


    <blockquote id="date-expire-notification" class="post-expired-notify">本文最后更新于 <span id="date-expire-num"></span> 天前，文中所描述的信息可能已发生改变</blockquote>
    <script>
    (function() {
        var dateUpdate = Date.parse("2025-08-02");
        var nowDate = new Date();
        var a = nowDate.getTime();
        var b = a - dateUpdate;
        var daysUpdateExpire = Math.floor(b/(24*3600*1000));
        if (daysUpdateExpire >= 120) {
            document.getElementById('date-expire-num').innerHTML = daysUpdateExpire;
        } else {
            document.getElementById('date-expire-notification').style.display = 'none';
        }
    })();
    </script>


<p class="post-footer-info mb-0 pt-0">本文发表于&nbsp;<time datetime="2025-08-01T15:22:35.000Z" itemprop="datePublished">2025-08-01</time>

    , 最后修改于&nbsp;<time datetime="2025-08-01T17:58:30.127Z" itemprop="dateModified">2025-08-02</time>

</p>
<p class="post-footer-info mb-0 pt-2">

<span class="post-categories-list mt-2">

<a class="post-categories-list-item" href='/categories/Daily-Paper/'>Daily Paper</a>

</span>




</p>

                </div>
                <div class="post-nav px-2 bg-gray">
<ul class="pagination">
    <!-- Prev Nav -->
    
        <li class="page-item page-prev">
            <a href="/2025/08/02/The-Road-to-Diffusion-and-Flow-Matching-DDPM/" rel="prev">
                <div class="page-item-title"><i class="icon icon-back" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">The Road to Diffusion and Flow Matching | DDPM</div>
            </a>
        </li>
    

    <!-- Next Nav -->
    
        <li class="page-item page-next">
            <a href="/2025/07/31/Daily-Paper-July-31-2025/" rel="next">
                <div class="page-item-title"><i class="icon icon-forward" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Daily Paper | July 31, 2025</div>
            </a>
        </li>
    
</ul>
</div>

                
                    <!-- # Comment # -->
                    
                        <div class="card-footer post-comment">
                            <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
        this.page.url = 'https://abinzzz.github.io/2025/08/01/Daily-Paper-Aug-1-2025/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://abinzzz.github.io/2025/08/01/Daily-Paper-Aug-1-2025/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>
<script id="disqus-thread-script">
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//robin02.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

                        </div>
                    
                
            </div>
        </div>
    </div>
</div>

            <!-- ### Footer ### -->
            <footer class="text-center">
    <!-- footer copyright -->
    
        <p class="footer-copyright mb-0">Copyright&nbsp;©&nbsp;<span id="copyright-year"></span>
            <a class="footer-copyright-a" href="https://abinzzz.github.io">blog</a>
        </p>

    <!-- footer custom text -->
    <p class="footer-text mb-0">
    
    </p>
    <!-- footer develop info -->
    <p class="footer-develop mb-0">
        
    <!-- Busuanzi User Views -->
    <span id="busuanzi_container_site_uv" hidden>
        <span></span>
        <span id="busuanzi_value_site_uv"></span>
        <span>Viewers</span>
        
            <span>|</span>
        
    </span>




        
        Powered by&nbsp;<!--
         --><a href="https://hexo.io" target="_blank" class="footer-develop-a" rel="external nofollow noopener noreferrer">Hexo</a><span class="footer-develop-divider"></span>Theme&nbsp;-&nbsp;<!--
         --><a href="https://github.com/SukkaW/hexo-theme-suka" target="_blank" class="footer-develop-a" rel="external noopener">Suka</a>
    </p>
</footer>


        <!-- ### Import File ### -->
        <!-- ### Footer JS Import ### -->

<script>

    
window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 50
};

(function() {
    var copyrightNow = new Date().getFullYear();
    var copyrightContent = document.getElementById('copyright-year');
    var copyrightSince = 2023;
    if (copyrightSince === copyrightNow) {
        copyrightContent.textContent = copyrightNow;
    } else {
        copyrightContent.textContent = copyrightSince + ' - ' + copyrightNow;
    }
})();
console.log('\n %c Suka Theme (hexo-theme-suka) | © SukkaW | Verision 1.3.3 %c https://github.com/SukkaW/hexo-theme-suka \n', 'color: #fff; background: #444; padding:5px 0;', 'background: #bbb; padding:5px 0;');

</script>

<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@8.9.0" async></script>
    <script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" async></script>


<!-- Offset -->




<!-- Comment -->

    
        <script id="dsq-count-scr" src="https://robin02.disqus.com/count.js" async></script>

    


<!-- ### Custom Footer ### -->

    </body>

</html>