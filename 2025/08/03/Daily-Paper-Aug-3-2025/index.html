<!DOCTYPE html>

<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <!--
        hexo-theme-suka © SukkaW
        GitHub: https://github.com/SukkaW/hexo-theme-suka
    -->

    <!-- ### Resource Hint ### -->

    <!-- ## DNS Prefetch ## -->
    <meta http-equiv="x-dns-prefetch-control" content="on">

<!-- busuanzi -->

    <link rel="dns-prefetch" href="//busuanzi.ibruce.info">


<!-- comment -->


    <link rel="dns-prefetch" href="//disqus.com">
    <link rel="dns-prefetch" href="//robin02.disqus.com">






<!-- analytics -->







    <!-- ## Preload ## -->
    
    <!-- Busuanzi -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" as="script">







    <!-- ### Meta & Title & Info ### -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <meta name="renderer" content="webkit">

    <!-- Title -->
    <title>Daily Paper | Aug 3, 2025 | blog</title>

    <!-- Favicons -->
    <link rel="icon" type="image&#x2F;ico" href="/img/blog.ico">

    <!-- ### Import File ### -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spectre.css@0.5.3"><style>
    body {
        background-color: #f8f9fa;
    }

    a, a:visited {
        color: blue;
    }

    a:active, a:focus, a:hover {
        color: blue;
        opacity: .75;
    }

    #post-content a,
    #post-content a:hover,
    #post-content a:focus,
    #post-content a:visited {
        color: blue;
        opacity: 1;
    }

    

    .post-entry .card-body a {
        color: red;
    }

    .avatar {
        background: red;
    }

    .navbar-link,
    .navbar-link:visited,
    .timeline .timeline-item .timeline-icon.icon-lg {
        color: red;
    }

    .navbar-link:hover {
        color: red;
        opacity: .8;
    }

    #search-input .btn,
    #disqus_click_btn,
    #disqus-switch-to-direct,
    #disqus-loadmore-button {
        background: red;
        border-color: red;
        color: #fff;
    }

    #post-toc a.post-toc-link,
    #post-toc a.post-toc-link:visited,
    .share-menu.menu .menu-item>a {
        color: red;
    }

    .share-menu.menu .menu-item>a:hover,
    .share-menu.menu .menu-item>a:focus,
    .share-menu.menu .menu-item>a:visited {
        color: #50596c;
        background: #f8f9fa;
        opacity: .85;
    }
</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/style.min.css">








    <!-- Prettify Theme -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css"></noscript>





<script>
/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
!function(t){"use strict";t.loadCSS||(t.loadCSS=function(){});var e=loadCSS.relpreload={};if(e.support=function(){var e;try{e=t.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),e.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.addEventListener?t.removeEventListener("load",a):t.attachEvent&&t.detachEvent("onload",a),t.setAttribute("onload",null),t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},e.poly=function(){if(!e.support())for(var a=t.document.getElementsByTagName("link"),n=0;n<a.length;n++){var o=a[n];"preload"!==o.rel||"style"!==o.getAttribute("as")||o.getAttribute("data-loadcss")||(o.setAttribute("data-loadcss",!0),e.bindMediaToggle(o))}},!e.support()){e.poly();var a=t.setInterval(e.poly,500);t.addEventListener?t.addEventListener("load",function(){e.poly(),t.clearInterval(a)}):t.attachEvent&&t.attachEvent("onload",function(){e.poly(),t.clearInterval(a)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:t.loadCSS=loadCSS}("undefined"!=typeof global?global:this);
</script>

    <!-- ### Site Verification ### -->
    


    <meta name="mobile-web-app-capable" content="yes"><meta name="application-name" content="blog"><meta name="msapplication-starturl" content="https://abinzzz.github.io"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="blog">

    <!-- ### The Open Graph & Twitter Card Protocol ### -->
    <meta property="og:title" content="Daily Paper | Aug 3, 2025 | blog"><meta property="og:site_name" content="blog"><meta property="og:type" content="article"><meta property="og:url" content="https://abinzzz.github.io/2025/08/03/Daily-Paper-Aug-3-2025/"><meta property="og:locale" content="zh-CN"><meta name="description" content="Table of Content  SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Tok - ab - blog"><meta name="keywords" content="blog"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f2f8858cb8da5c8008f65.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f304b58cb8da5c80091ac.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f31b158cb8da5c8009591.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f332058cb8da5c8009944.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f335a58cb8da5c80099de.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f33d258cb8da5c8009b7d.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f348958cb8da5c8009d72.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f365d58cb8da5c800a21c.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f36c058cb8da5c800a323.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f373d58cb8da5c800a456.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f376358cb8da5c800a4b9.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f37bf58cb8da5c800a59e.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f386f58cb8da5c800a74e.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f38a158cb8da5c800a7d6.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688f399c58cb8da5c800aa5d.png"><meta property="article:published_time" content="2025-08-03T09:37:21.000Z"><meta property="article:modified_time" content="2025-08-03T10:32:03.857Z"><meta property="og:updated_time" content="2025-08-03T10:32:03.857Z"><meta property="article:author" content="ab"><meta property="article:tag" content="blog"><meta name="twitter:card" content="summary">

    

    <!-- ### Canonical link ### -->
    <link rel="canonical" href="https://abinzzz.github.io/2025/08/03/Daily-Paper-Aug-3-2025/">

    <meta name="generator" content="Hexo 5.4.2">

    <!-- ### Analytics ### -->
    







    <!-- ### Structured Data ### -->
    



<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://abinzzz.github.io/2025/08/03/Daily-Paper-Aug-3-2025/",
    "@type": "BlogPosting",
    "logo": "https://abinzzz.github.io/img/blog.ico",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://abinzzz.github.io/2025/08/03/Daily-Paper-Aug-3-2025/"
    },
    "headline": "Daily Paper | Aug 3, 2025 | blog",
    
    "image": {
        "@type": "ImageObject",
        "url": "https://abinzzz.github.io/img/blog.ico"
    },
    
    "datePublished": "2025-08-03T09:37:21.000Z",
    "dateModified": "2025-08-03T10:32:03.857Z",
    "author": {
        "@type": "Person",
        "name": "ab",
        "image": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/avatar.jpg"
        },
        "description": "Welcome to my blog!"
    },
    "publisher": {
        "@type": "Organization",
        "name": "blog",
        "logo": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/blog.ico"
        }
    },
    
    "potentialAction": {
        "@type": "SearchAction",
        "target": "https://abinzzz.github.io/search?s={search_term_string}",
        "query-input": "required name=search_term_string"
    },
    
    "keywords": "blog",
    "description": "Table of Content  SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Tok - ab - blog"
}
</script>



    <!-- ### Custom Head ### -->
    
</head>

    <body>
            

            <!-- ### Main content ### -->
            <!-- ## Header ##-->
<header>
    <h1 class="header-title text-center"><a href="/">blog</a></h1>

    <p class="text-center header-slogan">
        
            
                Welcome to my blog!
            
        
    </p>

    <nav class="navbar-section text-center">
    
        <a href="/" class="navbar-link">首页</a>
    
    
        <a href="/archives/" class="navbar-link">归档</a>
    
    
        <a href="/search" class="navbar-link">搜索</a>
    
    
    
    
</nav>
</header>

            
    <!-- ## Post ## -->
    <div class="post-container">
    <div id="post-card" class="card">
        
        <div class="card-item-container">
            <div class="card-inner-cell">
                <!-- # Post Header Info # -->
                <div class="card-header">
                    
    <h1 class="card-title h3 mb-2">Daily Paper | Aug 3, 2025</h1>




<div class="post-header-info">
    <p class="post-header-info-left text-gray">
        <img class="author-thumb lazyload" data-src="/img/avatar.jpg" src="/img/suka-lazyload.gif" alt="ab's Avatar">
        <span>2025-08-03</span>
        
            <span class="suka-devide-dot"></span>
            <a class="category-link" href="/categories/Daily-Paper/">Daily Paper</a>
        
        
        
    </p>
    <div class="post-header-info-right">
        
            <div class="dropdown dropdown-right">
<a class="dropdown-toggle" tabindex="0">分享本文</a>
<ul class="menu share-menu">
    <!-- Share Weibo -->
    

    <!-- Share Twitter -->
    

    <!-- Share Facebook -->
    

    <!-- Share Google+ -->
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    

    <!-- Share Telegram -->
    

    <!-- QRCode -->
    
    <li class="menu-item">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJQAAACUCAAAAABQV18IAAAB30lEQVR42u3a0W6FIBCEYd//pdvbxrIzs9YmsOf35iStymeCwA5eXxseFyhQoEAtUJc4fl1U/D+5R3QOqDGoZae73bC6cQVfXWvbATUKdW9I/VYNKLhrB9RnoZY3uP39DgUFKunoq4bVgOsGYFCzUdVEWU2obpD811UCqO1RakH/5u9r1QyorVE2dDAdt2rUvUh/Sl1AbY1KCs1qMk0mcjfYgpqJSgMLVYimoZksRkCNQjlAVZCqc7uLPlCzUW4ydZ1ePUg8eII6EpUWn65xN1iq80DNQrnJ2IVgyaQeD7qgRqBcQakG1rT4jIsLUCNQatB7EtKrzSMVnICahep0ZNWBkw4eT8igjka5IOLJB1vJRrhdJYA6EpVMoqqhtFB1m5Wg5qJcQZmCVBAXBRygRqDSTaPkRUg2yUHNRKlgQy3O1EMlg2X7oy5Qx6GSIDUNwpJrlpM2qFGoZMHnNoo6AW4UcIAagXLFYxrEJoWsrJBBHYtKNhrdhNwJ++WDghqBSkOxTud1m9vxjgOoY1FPiwH1kU0Snj0a0UEdg0rCiDTIcIh4lQBqLMp94K4KBfcSgPpMlAsx0nPaQSyoo1FJSOE6tAr1X13kgdoe1SkE0vC/U5iCmoXa6QAFChSoH8c3QnJQb4RGasAAAAAASUVORK5CYII=" alt="QRCode">
    </li>
    

</ul>
</div>
        
    </div>
</div>
                </div>
                <div class="card-body">
                    
                        
                        
                            <div id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#table-of-content"><span class="post-toc-number">1.</span> <span class="post-toc-text"> Table of Content</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#simura-towards-general-goal-oriented-agent-via-simulative-reasoning-architecture-with-llm-based-world-model"><span class="post-toc-number">2.</span> <span class="post-toc-text"> SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation"><span class="post-toc-number">3.</span> <span class="post-toc-text"> Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#phi-ground-tech-report-advancing-perception-in-gui-grounding"><span class="post-toc-number">4.</span> <span class="post-toc-text"> Phi-Ground Tech Report: Advancing Perception in GUI Grounding</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#fairreason-balancing-reasoning-and-social-bias-in-mllms"><span class="post-toc-number">5.</span> <span class="post-toc-text"> FairReason: Balancing Reasoning and Social Bias in MLLMs</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#dino-vo-a-feature-based-visual-odometry-leveraging-a-visual-foundation-model"><span class="post-toc-number">6.</span> <span class="post-toc-text"> DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tars-minmax-token-adaptive-preference-strategy-for-hallucination-reduction-in-mllms"><span class="post-toc-number">7.</span> <span class="post-toc-text"> TARS : MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#autotir-autonomous-tools-integrated-reasoning-via-reinforcement-learning"><span class="post-toc-number">8.</span> <span class="post-toc-text"> AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#policy-learning-from-large-vision-language-model-feedback-without-reward-modeling"><span class="post-toc-number">9.</span> <span class="post-toc-text"> Policy Learning from Large Vision-Language Model Feedback Without Reward Modeling</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#first-return-entropy-eliciting-explore"><span class="post-toc-number">10.</span> <span class="post-toc-text"> First Return, Entropy-Eliciting Explore</span></a></li></ol></div>
                        
                    
                    <article id="post-content">
                        <!-- omit in toc -->
<h2 id="table-of-content"><a class="markdownIt-Anchor" href="#table-of-content"></a> Table of Content</h2>
<ul>
<li><a href="#simura-towards-general-goal-oriented-agent-via-simulative-reasoning-architecture-with-llm-based-world-model">SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</a></li>
<li><a href="#mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</a></li>
<li><a href="#phi-ground-tech-report-advancing-perception-in-gui-grounding">Phi-Ground Tech Report: Advancing Perception in GUI Grounding</a></li>
<li><a href="#fairreason-balancing-reasoning-and-social-bias-in-mllms">FairReason: Balancing Reasoning and Social Bias in MLLMs</a></li>
<li><a href="#dino-vo-a-feature-based-visual-odometry-leveraging-a-visual-foundation-model">DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model</a></li>
<li><a href="#tars--minmax-token-adaptive-preference-strategy-for-hallucination-reduction-in-mllms">TARS : MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs</a></li>
<li><a href="#autotir-autonomous-tools-integrated-reasoning-via-reinforcement-learning">AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning</a></li>
<li><a href="#policy-learning-from-large-vision-language-model-feedback-without-reward-modeling">Policy Learning from Large Vision-Language Model Feedback Without Reward Modeling</a></li>
<li><a href="#first-return-entropy-eliciting-explore">First Return, Entropy-Eliciting Explore</a></li>
</ul>
<h2 id="simura-towards-general-goal-oriented-agent-via-simulative-reasoning-architecture-with-llm-based-world-model"><a class="markdownIt-Anchor" href="#simura-towards-general-goal-oriented-agent-via-simulative-reasoning-architecture-with-llm-based-world-model"></a> SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</h2>
<p><img src="https://pic1.imgdb.cn/item/688f2f8858cb8da5c8008f65.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/maitrix-org/llm-reasoners/tree/main/examples/ReasonerAgent-Web">https://github.com/maitrix-org/llm-reasoners/tree/main/examples/ReasonerAgent-Web</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.23773">https://arxiv.org/abs/2507.23773</a></p>
<p>AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SIMURA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, SIMURA overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that SIMURA improves the success of flight search from 0% to 32.2%. World-model-based planning, in particular, shows consistent advantage of up to 124% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make REASONERAGENT-WEB, a web-browsing agent built on SIMURA with pretrained LLMs, available as a research demo for public testing.</p>
<p><img src="https://pic1.imgdb.cn/item/688f304b58cb8da5c80091ac.png" alt="" /></p>
<h2 id="mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation"><a class="markdownIt-Anchor" href="#mixture-of-recursions-learning-dynamic-recursive-depths-for-adaptive-token-level-computation"></a> Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</h2>
<p><img src="https://pic1.imgdb.cn/item/688f31b158cb8da5c8009591.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/raymin0223/mixture_of_recursions">https://github.com/raymin0223/mixture_of_recursions</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.10524">https://www.alphaxiv.org/abs/2507.10524</a></p>
<p>The Mixture-of-Recursions (MoR) framework unifies parameter sharing and adaptive computation within Recursive Transformer architectures, allowing models to dynamically apply shared layers to individual tokens. MoR achieves competitive performance with vanilla Transformers while using 50% fewer parameters, 25% fewer training FLOPs, and boosting inference throughput by up to 2.06x.</p>
<h2 id="phi-ground-tech-report-advancing-perception-in-gui-grounding"><a class="markdownIt-Anchor" href="#phi-ground-tech-report-advancing-perception-in-gui-grounding"></a> Phi-Ground Tech Report: Advancing Perception in GUI Grounding</h2>
<p><img src="https://pic1.imgdb.cn/item/688f332058cb8da5c8009944.png" alt="" /></p>
<p>Project Link: <a target="_blank" rel="noopener" href="https://zhangmiaosen2000.github.io/Phi-Ground/">https://zhangmiaosen2000.github.io/Phi-Ground/</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.23779">https://www.alphaxiv.org/abs/2507.23779</a></p>
<p>The Phi-Ground model family from Microsoft introduces an efficient and high-performing approach to Graphical User Interface (GUI) grounding, achieving state-of-the-art accuracy on five challenging benchmarks for models under 10B parameters by systematically optimizing data, training, and architectural considerations. This work explores practical aspects such as the impact of image tokens and the unexpected effectiveness of Direct Preference Optimization (DPO) for perceptual tasks.</p>
<p><img src="https://pic1.imgdb.cn/item/688f335a58cb8da5c80099de.png" alt="" /></p>
<h2 id="fairreason-balancing-reasoning-and-social-bias-in-mllms"><a class="markdownIt-Anchor" href="#fairreason-balancing-reasoning-and-social-bias-in-mllms"></a> FairReason: Balancing Reasoning and Social Bias in MLLMs</h2>
<p><img src="https://pic1.imgdb.cn/item/688f33d258cb8da5c8009b7d.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/Yutongzhang20080108/FairReason-Balancing-Reasoning-and-Social-Bias-in-MLLMs">https://github.com/Yutongzhang20080108/FairReason-Balancing-Reasoning-and-Social-Bias-in-MLLMs</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.23067">https://www.alphaxiv.org/abs/2507.23067</a></p>
<p>Multimodal Large Language Models (MLLMs) already achieve state-of-the-art results across a wide range of tasks and modalities. To push their reasoning ability further, recent studies explore advanced prompting schemes and post-training fine-tuning. Although these techniques improve logical accuracy, they frequently leave the models’ outputs burdened with pronounced social biases. Clarifying how reasoning gains interact with bias mitigation—and whether the two objectives inherently trade off—therefore remains an open and pressing research problem. Our study begins by benchmarking three bias-mitigation strategies—supervised fine-tuning (SFT), knowledge distillation (KD), and rule-based reinforcement learning (RL)—under identical conditions, establishing their baseline strengths and weaknesses. Building on these results, we vary the proportion of debias-focused and reasoning-centric samples within each paradigm to chart the reasoning-versusbias trade-off. Our sweeps reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement learning cuts stereotype scores by 10% while retaining 88% of the model’s original reasoning accuracy, offering concrete guidance for balancing fairness and capability in MLLMs.</p>
<p><img src="https://pic1.imgdb.cn/item/688f348958cb8da5c8009d72.png" alt="" /></p>
<h2 id="dino-vo-a-feature-based-visual-odometry-leveraging-a-visual-foundation-model"><a class="markdownIt-Anchor" href="#dino-vo-a-feature-based-visual-odometry-leveraging-a-visual-foundation-model"></a> DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model</h2>
<p>affiliation: KAIST</p>
<p>Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2’s coarse features. Furthermore, we complement DINOv2’s robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-toframe VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.</p>
<p><img src="https://pic1.imgdb.cn/item/688f365d58cb8da5c800a21c.png" alt="" /></p>
<h2 id="tars-minmax-token-adaptive-preference-strategy-for-hallucination-reduction-in-mllms"><a class="markdownIt-Anchor" href="#tars-minmax-token-adaptive-preference-strategy-for-hallucination-reduction-in-mllms"></a> TARS : MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs</h2>
<p><img src="https://pic1.imgdb.cn/item/688f36c058cb8da5c800a323.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://kejiazhang-robust.github.io/tars_web/">https://kejiazhang-robust.github.io/tars_web/</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.21584">https://www.alphaxiv.org/abs/2507.21584</a></p>
<p>Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics.</p>
<p><img src="https://pic1.imgdb.cn/item/688f373d58cb8da5c800a456.png" alt="" /></p>
<h2 id="autotir-autonomous-tools-integrated-reasoning-via-reinforcement-learning"><a class="markdownIt-Anchor" href="#autotir-autonomous-tools-integrated-reasoning-via-reinforcement-learning"></a> AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning</h2>
<p><img src="https://pic1.imgdb.cn/item/688f376358cb8da5c800a4b9.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/AutoTIR">https://github.com/weiyifan1023/AutoTIR</a>.<br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.21836">https://www.alphaxiv.org/abs/2507.21836</a></p>
<p>Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/AutoTIR">https://github.com/weiyifan1023/AutoTIR</a>.</p>
<p><img src="https://pic1.imgdb.cn/item/688f37bf58cb8da5c800a59e.png" alt="" /></p>
<h2 id="policy-learning-from-large-vision-language-model-feedback-without-reward-modeling"><a class="markdownIt-Anchor" href="#policy-learning-from-large-vision-language-model-feedback-without-reward-modeling"></a> Policy Learning from Large Vision-Language Model Feedback Without Reward Modeling</h2>
<p>affiliation: KAIST</p>
<p>Offline reinforcement learning (RL) provides a powerful framework for training robotic agents using precollected, suboptimal datasets, eliminating the need for costly, time-consuming, and potentially hazardous online interactions. This is particularly useful in safety-critical real-world applications, where online data collection is expensive and impractical. However, existing offline RL algorithms typically require reward labeled data, which introduces an additional bottleneck: reward function design is itself costly, labor-intensive, and requires significant domain expertise. In this paper, we introduce PLARE, a novel approach that leverages large visionlanguage models (VLMs) to provide guidance signals for agent training. Instead of relying on manually designed reward functions, PLARE queries a VLM for preference labels on pairs of visual trajectory segments based on a language task description. The policy is then trained directly from these preference labels using a supervised contrastive preference learning objective, bypassing the need to learn explicit reward models. Through extensive experiments on robotic manipulation tasks from the MetaWorld, PLARE achieves performance on par with or surpassing existing state-of-the-art VLM-based reward generation methods. Furthermore, we demonstrate the effectiveness of PLARE in real-world manipulation tasks with a physical robot, further validating its practical applicability.</p>
<p><img src="https://pic1.imgdb.cn/item/688f386f58cb8da5c800a74e.png" alt="" /></p>
<h2 id="first-return-entropy-eliciting-explore"><a class="markdownIt-Anchor" href="#first-return-entropy-eliciting-explore"></a> First Return, Entropy-Eliciting Explore</h2>
<p><img src="https://pic1.imgdb.cn/item/688f38a158cb8da5c800a7d6.png" alt="" /></p>
<p>Project Link: <a target="_blank" rel="noopener" href="https://huggingface.co/FR3E-Bytedance">https://huggingface.co/FR3E-Bytedance</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.07017">https://www.alphaxiv.org/abs/2507.07017</a></p>
<p>ByteDance researchers developed First Return, Entropy-Eliciting Explore (FR3E), a value-model-free reinforcement learning framework that enhances LLM reasoning by providing semantically grounded intermediate feedback. It identifies high-uncertainty decision points in reasoning paths using token-level entropy and conducts targeted partial rollouts, leading to more stable training and improved performance on mathematical reasoning benchmarks, with an average accuracy increase of over 3% on Qwen2.5 models.</p>
<p><img src="https://pic1.imgdb.cn/item/688f399c58cb8da5c800aa5d.png" alt="" /></p>

                    </article>
                    


    <blockquote id="date-expire-notification" class="post-expired-notify">本文最后更新于 <span id="date-expire-num"></span> 天前，文中所描述的信息可能已发生改变</blockquote>
    <script>
    (function() {
        var dateUpdate = Date.parse("2025-08-03");
        var nowDate = new Date();
        var a = nowDate.getTime();
        var b = a - dateUpdate;
        var daysUpdateExpire = Math.floor(b/(24*3600*1000));
        if (daysUpdateExpire >= 120) {
            document.getElementById('date-expire-num').innerHTML = daysUpdateExpire;
        } else {
            document.getElementById('date-expire-notification').style.display = 'none';
        }
    })();
    </script>


<p class="post-footer-info mb-0 pt-0">本文发表于&nbsp;<time datetime="2025-08-03T09:37:21.000Z" itemprop="datePublished">2025-08-03</time>

</p>
<p class="post-footer-info mb-0 pt-2">

<span class="post-categories-list mt-2">

<a class="post-categories-list-item" href='/categories/Daily-Paper/'>Daily Paper</a>

</span>




</p>

                </div>
                <div class="post-nav px-2 bg-gray">
<ul class="pagination">
    <!-- Prev Nav -->
    
        <li class="page-item page-prev">
            <a href="/2025/08/05/Daily-Paper-Aug-5-2025/" rel="prev">
                <div class="page-item-title"><i class="icon icon-back" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Daily Paper | Aug 5, 2025</div>
            </a>
        </li>
    

    <!-- Next Nav -->
    
        <li class="page-item page-next">
            <a href="/2025/08/02/Daily-Paper-Aug-2-2025/" rel="next">
                <div class="page-item-title"><i class="icon icon-forward" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Daily Paper | Aug 2, 2025</div>
            </a>
        </li>
    
</ul>
</div>

                
                    <!-- # Comment # -->
                    
                        <div class="card-footer post-comment">
                            <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
        this.page.url = 'https://abinzzz.github.io/2025/08/03/Daily-Paper-Aug-3-2025/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://abinzzz.github.io/2025/08/03/Daily-Paper-Aug-3-2025/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>
<script id="disqus-thread-script">
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//robin02.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

                        </div>
                    
                
            </div>
        </div>
    </div>
</div>

            <!-- ### Footer ### -->
            <footer class="text-center">
    <!-- footer copyright -->
    
        <p class="footer-copyright mb-0">Copyright&nbsp;©&nbsp;<span id="copyright-year"></span>
            <a class="footer-copyright-a" href="https://abinzzz.github.io">blog</a>
        </p>

    <!-- footer custom text -->
    <p class="footer-text mb-0">
    
    </p>
    <!-- footer develop info -->
    <p class="footer-develop mb-0">
        
    <!-- Busuanzi User Views -->
    <span id="busuanzi_container_site_uv" hidden>
        <span></span>
        <span id="busuanzi_value_site_uv"></span>
        <span>Viewers</span>
        
            <span>|</span>
        
    </span>




        
        Powered by&nbsp;<!--
         --><a href="https://hexo.io" target="_blank" class="footer-develop-a" rel="external nofollow noopener noreferrer">Hexo</a><span class="footer-develop-divider"></span>Theme&nbsp;-&nbsp;<!--
         --><a href="https://github.com/SukkaW/hexo-theme-suka" target="_blank" class="footer-develop-a" rel="external noopener">Suka</a>
    </p>
</footer>


        <!-- ### Import File ### -->
        <!-- ### Footer JS Import ### -->

<script>

    
window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 50
};

(function() {
    var copyrightNow = new Date().getFullYear();
    var copyrightContent = document.getElementById('copyright-year');
    var copyrightSince = 2023;
    if (copyrightSince === copyrightNow) {
        copyrightContent.textContent = copyrightNow;
    } else {
        copyrightContent.textContent = copyrightSince + ' - ' + copyrightNow;
    }
})();
console.log('\n %c Suka Theme (hexo-theme-suka) | © SukkaW | Verision 1.3.3 %c https://github.com/SukkaW/hexo-theme-suka \n', 'color: #fff; background: #444; padding:5px 0;', 'background: #bbb; padding:5px 0;');

</script>

<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@8.9.0" async></script>
    <script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" async></script>


<!-- Offset -->




<!-- Comment -->

    
        <script id="dsq-count-scr" src="https://robin02.disqus.com/count.js" async></script>

    


<!-- ### Custom Footer ### -->

    </body>

</html>