<!DOCTYPE html>

<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <!--
        hexo-theme-suka © SukkaW
        GitHub: https://github.com/SukkaW/hexo-theme-suka
    -->

    <!-- ### Resource Hint ### -->

    <!-- ## DNS Prefetch ## -->
    <meta http-equiv="x-dns-prefetch-control" content="on">

<!-- busuanzi -->

    <link rel="dns-prefetch" href="//busuanzi.ibruce.info">


<!-- comment -->


    <link rel="dns-prefetch" href="//disqus.com">
    <link rel="dns-prefetch" href="//robin02.disqus.com">






<!-- analytics -->







    <!-- ## Preload ## -->
    
    <!-- Busuanzi -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" as="script">







    <!-- ### Meta & Title & Info ### -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <meta name="renderer" content="webkit">

    <!-- Title -->
    <title>Daily Paper | July 31, 2025 | blog</title>

    <!-- Favicons -->
    <link rel="icon" type="image&#x2F;ico" href="/img/blog.ico">

    <!-- ### Import File ### -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spectre.css@0.5.3"><style>
    body {
        background-color: #f8f9fa;
    }

    a, a:visited {
        color: blue;
    }

    a:active, a:focus, a:hover {
        color: blue;
        opacity: .75;
    }

    #post-content a,
    #post-content a:hover,
    #post-content a:focus,
    #post-content a:visited {
        color: blue;
        opacity: 1;
    }

    

    .post-entry .card-body a {
        color: red;
    }

    .avatar {
        background: red;
    }

    .navbar-link,
    .navbar-link:visited,
    .timeline .timeline-item .timeline-icon.icon-lg {
        color: red;
    }

    .navbar-link:hover {
        color: red;
        opacity: .8;
    }

    #search-input .btn,
    #disqus_click_btn,
    #disqus-switch-to-direct,
    #disqus-loadmore-button {
        background: red;
        border-color: red;
        color: #fff;
    }

    #post-toc a.post-toc-link,
    #post-toc a.post-toc-link:visited,
    .share-menu.menu .menu-item>a {
        color: red;
    }

    .share-menu.menu .menu-item>a:hover,
    .share-menu.menu .menu-item>a:focus,
    .share-menu.menu .menu-item>a:visited {
        color: #50596c;
        background: #f8f9fa;
        opacity: .85;
    }
</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/style.min.css">








    <!-- Prettify Theme -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css"></noscript>





<script>
/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
!function(t){"use strict";t.loadCSS||(t.loadCSS=function(){});var e=loadCSS.relpreload={};if(e.support=function(){var e;try{e=t.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),e.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.addEventListener?t.removeEventListener("load",a):t.attachEvent&&t.detachEvent("onload",a),t.setAttribute("onload",null),t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},e.poly=function(){if(!e.support())for(var a=t.document.getElementsByTagName("link"),n=0;n<a.length;n++){var o=a[n];"preload"!==o.rel||"style"!==o.getAttribute("as")||o.getAttribute("data-loadcss")||(o.setAttribute("data-loadcss",!0),e.bindMediaToggle(o))}},!e.support()){e.poly();var a=t.setInterval(e.poly,500);t.addEventListener?t.addEventListener("load",function(){e.poly(),t.clearInterval(a)}):t.attachEvent&&t.attachEvent("onload",function(){e.poly(),t.clearInterval(a)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:t.loadCSS=loadCSS}("undefined"!=typeof global?global:this);
</script>

    <!-- ### Site Verification ### -->
    


    <meta name="mobile-web-app-capable" content="yes"><meta name="application-name" content="blog"><meta name="msapplication-starturl" content="https://abinzzz.github.io"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="blog">

    <!-- ### The Open Graph & Twitter Card Protocol ### -->
    <meta property="og:title" content="Daily Paper | July 31, 2025 | blog"><meta property="og:site_name" content="blog"><meta property="og:type" content="article"><meta property="og:url" content="https://abinzzz.github.io/2025/07/31/Daily-Paper-July-31-2025/"><meta property="og:locale" content="zh-CN"><meta name="description" content="Table of Content  MetaCLIP 2: A Worldwide Scaling Recipe X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again HunyuanWorld 1.0: Generating Immersive, Explor - ab - blog"><meta name="keywords" content="blog"><meta property="og:image" content="https://pic1.imgdb.cn/item/688af73958cb8da5c8f38709.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688af78e58cb8da5c8f388d3.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688aff1958cb8da5c8f3bc42.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b01f758cb8da5c8f3d841.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b043c58cb8da5c8f3e3c3.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b037058cb8da5c8f3dffb.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b053a58cb8da5c8f3e8b2.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b05a658cb8da5c8f3eafa.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b086f58cb8da5c8f3fb61.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b06ad58cb8da5c8f3f051.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b084758cb8da5c8f3fa49.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b1eb858cb8da5c8f48499.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b1f3058cb8da5c8f4887d.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b1fbd58cb8da5c8f48e48.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b204958cb8da5c8f491e8.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b220858cb8da5c8f4a445.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b22c858cb8da5c8f4adde.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b258258cb8da5c8f4cbae.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b279a58cb8da5c8f4d916.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b292958cb8da5c8f4e066.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/688b296558cb8da5c8f4e080.png"><meta property="article:published_time" content="2025-07-31T04:24:58.000Z"><meta property="article:modified_time" content="2025-07-31T08:30:12.905Z"><meta property="og:updated_time" content="2025-07-31T08:30:12.905Z"><meta property="article:author" content="ab"><meta property="article:tag" content="blog"><meta name="twitter:card" content="summary">

    

    <!-- ### Canonical link ### -->
    <link rel="canonical" href="https://abinzzz.github.io/2025/07/31/Daily-Paper-July-31-2025/">

    <meta name="generator" content="Hexo 5.4.2">

    <!-- ### Analytics ### -->
    







    <!-- ### Structured Data ### -->
    



<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://abinzzz.github.io/2025/07/31/Daily-Paper-July-31-2025/",
    "@type": "BlogPosting",
    "logo": "https://abinzzz.github.io/img/blog.ico",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://abinzzz.github.io/2025/07/31/Daily-Paper-July-31-2025/"
    },
    "headline": "Daily Paper | July 31, 2025 | blog",
    
    "image": {
        "@type": "ImageObject",
        "url": "https://abinzzz.github.io/img/blog.ico"
    },
    
    "datePublished": "2025-07-31T04:24:58.000Z",
    "dateModified": "2025-07-31T08:30:12.905Z",
    "author": {
        "@type": "Person",
        "name": "ab",
        "image": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/avatar.jpg"
        },
        "description": "Welcome to my blog!"
    },
    "publisher": {
        "@type": "Organization",
        "name": "blog",
        "logo": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/blog.ico"
        }
    },
    
    "potentialAction": {
        "@type": "SearchAction",
        "target": "https://abinzzz.github.io/search?s={search_term_string}",
        "query-input": "required name=search_term_string"
    },
    
    "keywords": "blog",
    "description": "Table of Content  MetaCLIP 2: A Worldwide Scaling Recipe X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again HunyuanWorld 1.0: Generating Immersive, Explor - ab - blog"
}
</script>



    <!-- ### Custom Head ### -->
    
</head>

    <body>
            

            <!-- ### Main content ### -->
            <!-- ## Header ##-->
<header>
    <h1 class="header-title text-center"><a href="/">blog</a></h1>

    <p class="text-center header-slogan">
        
            
                Welcome to my blog!
            
        
    </p>

    <nav class="navbar-section text-center">
    
        <a href="/" class="navbar-link">首页</a>
    
    
        <a href="/archives/" class="navbar-link">归档</a>
    
    
        <a href="/search" class="navbar-link">搜索</a>
    
    
    
    
</nav>
</header>

            
    <!-- ## Post ## -->
    <div class="post-container">
    <div id="post-card" class="card">
        
        <div class="card-item-container">
            <div class="card-inner-cell">
                <!-- # Post Header Info # -->
                <div class="card-header">
                    
    <h1 class="card-title h3 mb-2">Daily Paper | July 31, 2025</h1>




<div class="post-header-info">
    <p class="post-header-info-left text-gray">
        <img class="author-thumb lazyload" data-src="/img/avatar.jpg" src="/img/suka-lazyload.gif" alt="ab's Avatar">
        <span>2025-07-31</span>
        
            <span class="suka-devide-dot"></span>
            <a class="category-link" href="/categories/Daily-Paper/">Daily Paper</a>
        
        
        
    </p>
    <div class="post-header-info-right">
        
            <div class="dropdown dropdown-right">
<a class="dropdown-toggle" tabindex="0">分享本文</a>
<ul class="menu share-menu">
    <!-- Share Weibo -->
    

    <!-- Share Twitter -->
    

    <!-- Share Facebook -->
    

    <!-- Share Google+ -->
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    

    <!-- Share Telegram -->
    

    <!-- QRCode -->
    
    <li class="menu-item">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJQAAACUCAAAAABQV18IAAAB5ElEQVR42u3a22qFMBBGYd//pdurghVn/pV9AB2XN4VtNZ8QY+aw/Vzw2ESJEiXqBLU1x9/5/f/tfzueP/72b8AwjqhZqNNJV6CONzoOmu5TnRc1D9VN8GrCnk3w47nu4UQ9E9UtftUkFyUqTXTyUa5gKw8vahaq/VA2gybkV3YJoi6PooHDu3+/Es2IuhwKJR7ABjAFDh/Puoi6LKpb9OLEDB/qaoxyQRY1BtVtwNINu8RHSp7FRKyoW6KqhW9lY1ctoi8Ho6JujUrJipTAJx9iHMCKGoPqkmKrCY0u6Ix4UWNQK0UgsrFLL0oJFzUKRTb2K5vCVBwQNRe1EjCSZBq5X7koixqBqgamAQNZOMkkFzUX9YlGLfpRRhUHUWNQJACtXpDV4pGoWSgSQKbkPklqpKBX1BxUChBokYg2e7ULrKhxKPLRpNelJC+qYom6LSolWknTIAGlBgtRM1Gk8YEEC+QFQhUHUbdErSQuUoBKHgInOETdEvVKkyAJImjxSNQ8FGlopk2DqZgUXyZRY1DdBE1FyO66lSBX1DwUabxJi2E3+VHSTdRjUCk4oAHIUsVB1FhUKlbTonZq3hE1D5UackjzFmmMFvUMFAkc0kApwVZd/1Y0I+qSqCsdokSJErU7fgGxfzGAjfU+DgAAAABJRU5ErkJggg==" alt="QRCode">
    </li>
    

</ul>
</div>
        
    </div>
</div>
                </div>
                <div class="card-body">
                    
                        
                        
                            <div id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#table-of-content"><span class="post-toc-number">1.</span> <span class="post-toc-text"> Table of Content</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#metaclip-2-a-worldwide-scaling-recipe"><span class="post-toc-number">2.</span> <span class="post-toc-text"> MetaCLIP 2: A Worldwide Scaling Recipe</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#x-omni-reinforcement-learning-makes-discrete-autoregressive-image-generative-models-great-again"><span class="post-toc-number">3.</span> <span class="post-toc-text"> X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#hunyuanworld-10-generating-immersive-explorable-and-interactive-3d-worlds-from-words-or-pixels"><span class="post-toc-number">4.</span> <span class="post-toc-text"> HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#graph-r1-towards-agentic-graphrag-framework-via-end-to-end-reinforcement-learning"><span class="post-toc-number">5.</span> <span class="post-toc-text"> Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#learning-only-with-images-visual-reinforcement-learning-with-reasoning-rendering-and-visual-feedback"><span class="post-toc-number">6.</span> <span class="post-toc-text"> Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#edge-grpo-entropy-driven-grpo-with-guided-error-correction-for-advantage-diversity"><span class="post-toc-number">7.</span> <span class="post-toc-text"> EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#smallthinker-a-family-of-efficient-large-language-models-natively-trained-for-local-deployment"><span class="post-toc-number">8.</span> <span class="post-toc-text"> SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#mixgrpo-unlocking-flow-based-grpo-efficiency-with-mixed-ode-sde"><span class="post-toc-number">9.</span> <span class="post-toc-text"> MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#towards-a-large-physics-benchmark"><span class="post-toc-number">10.</span> <span class="post-toc-text"> Towards a Large Physics Benchmark</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#stepfun-prover-preview-lets-think-and-verify-step-by-step"><span class="post-toc-number">11.</span> <span class="post-toc-text"> StepFun-Prover Preview: Let’s Think and Verify Step by Step</span></a></li></ol></div>
                        
                    
                    <article id="post-content">
                        <!-- omit in toc -->
<h2 id="table-of-content"><a class="markdownIt-Anchor" href="#table-of-content"></a> Table of Content</h2>
<ul>
<li><a href="#metaclip-2-a-worldwide-scaling-recipe">MetaCLIP 2: A Worldwide Scaling Recipe</a></li>
<li><a href="#x-omni-reinforcement-learning-makes-discrete-autoregressive-image-generative-models-great-again">X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again</a></li>
<li><a href="#hunyuanworld-10-generating-immersive-explorable-and-interactive-3d-worlds-from-words-or-pixels">HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels</a></li>
<li><a href="#graph-r1-towards-agentic-graphrag-framework-via-end-to-end-reinforcement-learning">Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning</a></li>
<li><a href="#learning-only-with-images-visual-reinforcement-learning-with-reasoning-rendering-and-visual-feedback">Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback</a></li>
<li><a href="#edge-grpo-entropy-driven-grpo-with-guided-error-correction-for-advantage-diversity">EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity</a></li>
<li><a href="#smallthinker-a-family-of-efficient-large-language-models-natively-trained-for-local-deployment">SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment</a></li>
<li><a href="#mixgrpo-unlocking-flow-based-grpo-efficiency-with-mixed-ode-sde">MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</a></li>
<li><a href="#towards-a-large-physics-benchmark">Towards a Large Physics Benchmark</a></li>
<li><a href="#stepfun-prover-preview-lets-think-and-verify-step-by-step">StepFun-Prover Preview: Let’s Think and Verify Step by Step</a></li>
</ul>
<h2 id="metaclip-2-a-worldwide-scaling-recipe"><a class="markdownIt-Anchor" href="#metaclip-2-a-worldwide-scaling-recipe"></a> MetaCLIP 2: A Worldwide Scaling Recipe</h2>
<p><img src="https://pic1.imgdb.cn/item/688af73958cb8da5c8f38709.png" alt="" /><br />
Alphaxiv Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.22062">https://www.alphaxiv.org/abs/2507.22062</a><br />
Code and Model: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/MetaCLIP">https://github.com/facebookresearch/MetaCLIP</a></p>
<p>Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zeroshot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP’s training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., “curse of multilinguality” that is common in LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, MetaCLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.</p>
<p><img src="https://pic1.imgdb.cn/item/688af78e58cb8da5c8f388d3.png" alt="" /></p>
<h2 id="x-omni-reinforcement-learning-makes-discrete-autoregressive-image-generative-models-great-again"><a class="markdownIt-Anchor" href="#x-omni-reinforcement-learning-makes-discrete-autoregressive-image-generative-models-great-again"></a> X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again</h2>
<p><img src="https://pic1.imgdb.cn/item/688aff1958cb8da5c8f3bc42.png" alt="" /></p>
<p>Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.22058">https://www.alphaxiv.org/abs/2507.22058</a><br />
Github Link: <a target="_blank" rel="noopener" href="https://github.com/X-Omni-Team/X-Omni">https://github.com/X-Omni-Team/X-Omni</a><br />
Project Link: <a target="_blank" rel="noopener" href="https://x-omni-team.github.io/">https://x-omni-team.github.io/</a></p>
<p>Numerous efforts have been made to extend the “next token prediction” paradigm to visual contents, aiming to create a unified approach for both image generation and understanding. Nevertheless, attempts to generate images through autoregressive modeling with discrete tokens have been plagued by issues such as low visual fidelity, distorted outputs, and failure to adhere to complex instructions when rendering intricate details. These shortcomings are likely attributed to cumulative errors during autoregressive inference or information loss incurred during the discretization process. Probably due to this challenge, recent research has increasingly shifted toward jointly training image generation with diffusion objectives and language generation with autoregressive objectives, moving away from unified modeling approaches. In this work, we demonstrate that reinforcement learning can effectively mitigate artifacts and largely enhance the generation quality of a discrete autoregressive modeling method, thereby enabling seamless integration of image and language generation. Our framework comprises a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder for image generation, termed X-Omni. X-Omni achieves state-of-the-art performance in image generation tasks using a 7B language model, producing images with high aesthetic quality while exhibiting strong capabilities in following instructions and rendering long texts.</p>
<p><img src="https://pic1.imgdb.cn/item/688b01f758cb8da5c8f3d841.png" alt="" /></p>
<h2 id="hunyuanworld-10-generating-immersive-explorable-and-interactive-3d-worlds-from-words-or-pixels"><a class="markdownIt-Anchor" href="#hunyuanworld-10-generating-immersive-explorable-and-interactive-3d-worlds-from-words-or-pixels"></a> HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels</h2>
<p><strong>The writing in this aritcle is worth studying.</strong></p>
<p>Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.21809">https://www.alphaxiv.org/abs/2507.21809</a><br />
Project Link: <a target="_blank" rel="noopener" href="https://3d.hunyuan.tencent.com/sceneTo3D">https://3d.hunyuan.tencent.com/sceneTo3D</a><br />
Github Link: <a target="_blank" rel="noopener" href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0">https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0</a></p>
<p>Creating immersive and playable 3D worlds from texts or images remains a fundamental challenge in computer vision and graphics. Existing world generation approaches typically fall into two categories: video-based methods that offer rich diversity but lack 3D consistency and rendering efficiency, and 3D-based methods that provide geometric consistency but struggle with limited training data and memory-inefficient representations. To address these limitations, we present HunyuanWorld 1.0, a novel framework that combines the best of both worlds for generating immersive, explorable, and interactive 3D scenes from text and image conditions. Our approach features three key advantages: 1) 360° immersive experiences via panoramic world proxies; 2) mesh export capabilities for seamless compatibility with existing computer graphics pipelines; 3) disentangled object representations for augmented interactivity. The core of our framework is a semantically layered 3D mesh representation that leverages panoramic images as 360° world proxies for semantic-aware world decomposition and reconstruction, enabling the generation of diverse 3D worlds. Extensive experiments demonstrate that our method achieves state-of-the-art performance in generating coherent, explorable, and interactive 3D worlds while enabling versatile applications in virtual reality, physical simulation, game development, and interactive content creation.</p>
<p><img src="https://pic1.imgdb.cn/item/688b043c58cb8da5c8f3e3c3.png" alt="" /></p>
<p><img src="https://pic1.imgdb.cn/item/688b037058cb8da5c8f3dffb.png" alt="" /></p>
<h2 id="graph-r1-towards-agentic-graphrag-framework-via-end-to-end-reinforcement-learning"><a class="markdownIt-Anchor" href="#graph-r1-towards-agentic-graphrag-framework-via-end-to-end-reinforcement-learning"></a> Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning</h2>
<p><img src="https://pic1.imgdb.cn/item/688b053a58cb8da5c8f3e8b2.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/LHRLAB/Graph-R1">https://github.com/LHRLAB/Graph-R1</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.21892">https://www.alphaxiv.org/abs/2507.21892</a></p>
<p>Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality. Our code is publicly available.</p>
<p><img src="https://pic1.imgdb.cn/item/688b05a658cb8da5c8f3eafa.png" alt="" /></p>
<h2 id="learning-only-with-images-visual-reinforcement-learning-with-reasoning-rendering-and-visual-feedback"><a class="markdownIt-Anchor" href="#learning-only-with-images-visual-reinforcement-learning-with-reasoning-rendering-and-visual-feedback"></a> Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback</h2>
<p><img src="https://pic1.imgdb.cn/item/688b086f58cb8da5c8f3fb61.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/L-OI/RRVF">https://github.com/L-OI/RRVF</a>.<br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.20766">https://www.alphaxiv.org/abs/2507.20766</a></p>
<p>Multimodal Large Language Models (MLLMs) have exhibited impressive performance across various visual tasks. Subsequent investigations into enhancing their visual reasoning abilities have significantly expanded their performance envelope. However, a critical bottleneck in the advancement of MLLMs toward deep visual reasoning is their heavy reliance on curated image-text supervision. To solve this problem, we introduce a novel framework termed “Reasoning-RenderingVisual-Feedback” (RRVF), which enables MLLMs to learn complex visual reasoning from only raw images. This framework builds on the “Asymmetry of Verification” principle to train MLLMs, i.e., verifying the rendered output against a source image is easier than generating it. We demonstrate that this relative ease provides an ideal reward signal for optimization via Reinforcement Learning (RL) training, reducing the reliance on the image-text supervision. Guided by the above principle, RRVF implements a closed-loop iterative process encompassing reasoning, rendering, and visual feedback components, enabling the model to perform selfcorrection through multi-turn interactions and tool invocation, while this pipeline can be optimized by the GRPO algorithm in an end-to-end manner. Extensive experiments on image-to-code generation for data charts and web interfaces show that RRVF substantially outperforms existing opensource MLLMs and surpasses supervised fine-tuning baselines. Our findings demonstrate that systems driven by purely visual feedback present a viable path toward more robust and generalizable reasoning models without requiring explicit supervision. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/L-OI/RRVF">https://github.com/L-OI/RRVF</a>.</p>
<p><img src="https://pic1.imgdb.cn/item/688b06ad58cb8da5c8f3f051.png" alt="" /></p>
<p><img src="https://pic1.imgdb.cn/item/688b084758cb8da5c8f3fa49.png" alt="" /></p>
<h2 id="edge-grpo-entropy-driven-grpo-with-guided-error-correction-for-advantage-diversity"><a class="markdownIt-Anchor" href="#edge-grpo-entropy-driven-grpo-with-guided-error-correction-for-advantage-diversity"></a> EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity</h2>
<p><img src="https://pic1.imgdb.cn/item/688b1eb858cb8da5c8f48499.png" alt="" /></p>
<p>Github Link: <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/EDGE-GRPO">https://github.com/ZhangXJ199/EDGE-GRPO</a><br />
Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.21848">https://www.alphaxiv.org/abs/2507.21848</a></p>
<p>Large Language Models (LLMs) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts Entropy-Driven Advantage and Guided Error Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/EDGE-GRPO">https://github.com/ZhangXJ199/EDGE-GRPO</a>.</p>
<p><img src="https://pic1.imgdb.cn/item/688b1f3058cb8da5c8f4887d.png" alt="" /></p>
<p><img src="https://pic1.imgdb.cn/item/688b1fbd58cb8da5c8f48e48.png" alt="" /></p>
<h2 id="smallthinker-a-family-of-efficient-large-language-models-natively-trained-for-local-deployment"><a class="markdownIt-Anchor" href="#smallthinker-a-family-of-efficient-large-language-models-natively-trained-for-local-deployment"></a> SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment</h2>
<p><img src="https://pic1.imgdb.cn/item/688b204958cb8da5c8f491e8.png" alt="" /></p>
<p>Model Link: <a target="_blank" rel="noopener" href="https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct">https://huggingface.co/PowerInfer/SmallThinker-4BA0.6B-Instruct</a></p>
<p>Model Link: <a target="_blank" rel="noopener" href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct">https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct</a></p>
<p>While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed—not adapted—for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4BA0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively.</p>
<p><img src="https://pic1.imgdb.cn/item/688b220858cb8da5c8f4a445.png" alt="" /></p>
<h2 id="mixgrpo-unlocking-flow-based-grpo-efficiency-with-mixed-ode-sde"><a class="markdownIt-Anchor" href="#mixgrpo-unlocking-flow-based-grpo-efficiency-with-mixed-ode-sde"></a> MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</h2>
<p><strong>The writing for this paper is worth studying.</strong></p>
<p><img src="https://pic1.imgdb.cn/item/688b22c858cb8da5c8f4adde.png" alt="" /></p>
<p>Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.21802">https://www.alphaxiv.org/abs/2507.21802</a><br />
Github Link: <a target="_blank" rel="noopener" href="https://github.com/Tencent-Hunyuan/MixGRPO">https://github.com/Tencent-Hunyuan/MixGRPO</a><br />
Project Link: <a target="_blank" rel="noopener" href="https://tulvgengenr.github.io/MixGRPO-Project-Page/">https://tulvgengenr.github.io/MixGRPO-Project-Page/</a></p>
<p>Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPOguided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%.</p>
<p><img src="https://pic1.imgdb.cn/item/688b258258cb8da5c8f4cbae.png" alt="" /></p>
<h2 id="towards-a-large-physics-benchmark"><a class="markdownIt-Anchor" href="#towards-a-large-physics-benchmark"></a> Towards a Large Physics Benchmark</h2>
<p><img src="https://pic1.imgdb.cn/item/688b279a58cb8da5c8f4d916.png" alt="" /></p>
<p>We introduce a benchmark framework developed by and for the scientific community to evaluate, monitor and steer large language model development in fundamental physics. Building on philosophical concepts of scientific understanding and creativity, we develop a scoring system in which each question is scored by an expert for its correctness, difficulty, and surprise. The questions are of three forms: (i) multiple-choice questions for conceptual understanding, (ii) analytical problems requiring mathematical derivation, and (iii) openended tasks requiring complex problem solving. Our current dataset contains diverse set of examples, including a machine learning challenge to classify high-energy physics events, such as the four top quark signal. To ensure continued relevance, we propose a “living” benchmark, where physicists contribute questions, for instance alongside new publications. We invite contributions via: <a target="_blank" rel="noopener" href="http://www.physicsbenchmarks.org/">http://www.physicsbenchmarks.org/</a>. We hope that this benchmark will enable a targeted AI development that can make a meaningful contribution to fundamental physics research.</p>
<h2 id="stepfun-prover-preview-lets-think-and-verify-step-by-step"><a class="markdownIt-Anchor" href="#stepfun-prover-preview-lets-think-and-verify-step-by-step"></a> StepFun-Prover Preview: Let’s Think and Verify Step by Step</h2>
<p><img src="https://pic1.imgdb.cn/item/688b292958cb8da5c8f4e066.png" alt="" /></p>
<p>Paper Link: <a target="_blank" rel="noopener" href="https://www.alphaxiv.org/abs/2507.20199">https://www.alphaxiv.org/abs/2507.20199</a></p>
<p>We present StepFun-Prover Preview, a large language model designed for formal theorem proving through tool-integrated reasoning. Using a reinforcement learning pipeline that incorporates tool-based interactions, StepFun-Prover can achieve strong performance in generating Lean 4 proofs with minimal sampling. Our approach enables the model to emulate human-like problem-solving strategies by iteratively refining proofs based on real-time environment feedback. On the miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of 70.0%. Beyond advancing benchmark performance, we introduce an end-to-end training framework for developing tool-integrated reasoning models, offering a promising direction for automated theorem proving and Math AI assistant.</p>
<p><img src="https://pic1.imgdb.cn/item/688b296558cb8da5c8f4e080.png" alt="" /></p>

                    </article>
                    


    <blockquote id="date-expire-notification" class="post-expired-notify">本文最后更新于 <span id="date-expire-num"></span> 天前，文中所描述的信息可能已发生改变</blockquote>
    <script>
    (function() {
        var dateUpdate = Date.parse("2025-07-31");
        var nowDate = new Date();
        var a = nowDate.getTime();
        var b = a - dateUpdate;
        var daysUpdateExpire = Math.floor(b/(24*3600*1000));
        if (daysUpdateExpire >= 120) {
            document.getElementById('date-expire-num').innerHTML = daysUpdateExpire;
        } else {
            document.getElementById('date-expire-notification').style.display = 'none';
        }
    })();
    </script>


<p class="post-footer-info mb-0 pt-0">本文发表于&nbsp;<time datetime="2025-07-31T04:24:58.000Z" itemprop="datePublished">2025-07-31</time>

</p>
<p class="post-footer-info mb-0 pt-2">

<span class="post-categories-list mt-2">

<a class="post-categories-list-item" href='/categories/Daily-Paper/'>Daily Paper</a>

</span>




</p>

                </div>
                <div class="post-nav px-2 bg-gray">
<ul class="pagination">
    <!-- Prev Nav -->
    
        <li class="page-item page-prev">
            <a href="/2025/08/01/Daily-Paper-Aug-1-2025/" rel="prev">
                <div class="page-item-title"><i class="icon icon-back" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Daily Paper | Aug 1, 2025</div>
            </a>
        </li>
    

    <!-- Next Nav -->
    
        <li class="page-item page-next">
            <a href="/2025/07/30/Daily-Paper-July-30-2025/" rel="next">
                <div class="page-item-title"><i class="icon icon-forward" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Daily Paper | July 30, 2025</div>
            </a>
        </li>
    
</ul>
</div>

                
                    <!-- # Comment # -->
                    
                        <div class="card-footer post-comment">
                            <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
        this.page.url = 'https://abinzzz.github.io/2025/07/31/Daily-Paper-July-31-2025/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://abinzzz.github.io/2025/07/31/Daily-Paper-July-31-2025/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>
<script id="disqus-thread-script">
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//robin02.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

                        </div>
                    
                
            </div>
        </div>
    </div>
</div>

            <!-- ### Footer ### -->
            <footer class="text-center">
    <!-- footer copyright -->
    
        <p class="footer-copyright mb-0">Copyright&nbsp;©&nbsp;<span id="copyright-year"></span>
            <a class="footer-copyright-a" href="https://abinzzz.github.io">blog</a>
        </p>

    <!-- footer custom text -->
    <p class="footer-text mb-0">
    
    </p>
    <!-- footer develop info -->
    <p class="footer-develop mb-0">
        
    <!-- Busuanzi User Views -->
    <span id="busuanzi_container_site_uv" hidden>
        <span></span>
        <span id="busuanzi_value_site_uv"></span>
        <span>Viewers</span>
        
            <span>|</span>
        
    </span>




        
        Powered by&nbsp;<!--
         --><a href="https://hexo.io" target="_blank" class="footer-develop-a" rel="external nofollow noopener noreferrer">Hexo</a><span class="footer-develop-divider"></span>Theme&nbsp;-&nbsp;<!--
         --><a href="https://github.com/SukkaW/hexo-theme-suka" target="_blank" class="footer-develop-a" rel="external noopener">Suka</a>
    </p>
</footer>


        <!-- ### Import File ### -->
        <!-- ### Footer JS Import ### -->

<script>

    
window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 50
};

(function() {
    var copyrightNow = new Date().getFullYear();
    var copyrightContent = document.getElementById('copyright-year');
    var copyrightSince = 2023;
    if (copyrightSince === copyrightNow) {
        copyrightContent.textContent = copyrightNow;
    } else {
        copyrightContent.textContent = copyrightSince + ' - ' + copyrightNow;
    }
})();
console.log('\n %c Suka Theme (hexo-theme-suka) | © SukkaW | Verision 1.3.3 %c https://github.com/SukkaW/hexo-theme-suka \n', 'color: #fff; background: #444; padding:5px 0;', 'background: #bbb; padding:5px 0;');

</script>

<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@8.9.0" async></script>
    <script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" async></script>


<!-- Offset -->




<!-- Comment -->

    
        <script id="dsq-count-scr" src="https://robin02.disqus.com/count.js" async></script>

    


<!-- ### Custom Footer ### -->

    </body>

</html>