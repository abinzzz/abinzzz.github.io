<!DOCTYPE html>

<html lang="zh-CN">
    <head>
    <meta charset="utf-8">
    <!--
        hexo-theme-suka © SukkaW
        GitHub: https://github.com/SukkaW/hexo-theme-suka
    -->

    <!-- ### Resource Hint ### -->

    <!-- ## DNS Prefetch ## -->
    <meta http-equiv="x-dns-prefetch-control" content="on">

<!-- busuanzi -->

    <link rel="dns-prefetch" href="//busuanzi.ibruce.info">


<!-- comment -->


    <link rel="dns-prefetch" href="//disqus.com">
    <link rel="dns-prefetch" href="//robin02.disqus.com">






<!-- analytics -->







    <!-- ## Preload ## -->
    
    <!-- Busuanzi -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" as="script">







    <!-- ### Meta & Title & Info ### -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, maximum-scale=5, viewport-fit=cover">
    <meta name="renderer" content="webkit">

    <!-- Title -->
    <title>Daily Paper | July 30, 2025 | blog</title>

    <!-- Favicons -->
    <link rel="icon" type="image&#x2F;ico" href="/img/blog.ico">

    <!-- ### Import File ### -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/spectre.css@0.5.3"><style>
    body {
        background-color: #f8f9fa;
    }

    a, a:visited {
        color: blue;
    }

    a:active, a:focus, a:hover {
        color: blue;
        opacity: .75;
    }

    #post-content a,
    #post-content a:hover,
    #post-content a:focus,
    #post-content a:visited {
        color: blue;
        opacity: 1;
    }

    

    .post-entry .card-body a {
        color: red;
    }

    .avatar {
        background: red;
    }

    .navbar-link,
    .navbar-link:visited,
    .timeline .timeline-item .timeline-icon.icon-lg {
        color: red;
    }

    .navbar-link:hover {
        color: red;
        opacity: .8;
    }

    #search-input .btn,
    #disqus_click_btn,
    #disqus-switch-to-direct,
    #disqus-loadmore-button {
        background: red;
        border-color: red;
        color: #fff;
    }

    #post-toc a.post-toc-link,
    #post-toc a.post-toc-link:visited,
    .share-menu.menu .menu-item>a {
        color: red;
    }

    .share-menu.menu .menu-item>a:hover,
    .share-menu.menu .menu-item>a:focus,
    .share-menu.menu .menu-item>a:visited {
        color: #50596c;
        background: #f8f9fa;
        opacity: .85;
    }
</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/style.min.css">








    <!-- Prettify Theme -->
    
    <link rel="preload" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sukkaw/hexo-theme-suka@1.3.0/source/css/highlight/[theme-name].min.css"></noscript>





<script>
/*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
!function(t){"use strict";t.loadCSS||(t.loadCSS=function(){});var e=loadCSS.relpreload={};if(e.support=function(){var e;try{e=t.document.createElement("link").relList.supports("preload")}catch(t){e=!1}return function(){return e}}(),e.bindMediaToggle=function(t){var e=t.media||"all";function a(){t.addEventListener?t.removeEventListener("load",a):t.attachEvent&&t.detachEvent("onload",a),t.setAttribute("onload",null),t.media=e}t.addEventListener?t.addEventListener("load",a):t.attachEvent&&t.attachEvent("onload",a),setTimeout(function(){t.rel="stylesheet",t.media="only x"}),setTimeout(a,3e3)},e.poly=function(){if(!e.support())for(var a=t.document.getElementsByTagName("link"),n=0;n<a.length;n++){var o=a[n];"preload"!==o.rel||"style"!==o.getAttribute("as")||o.getAttribute("data-loadcss")||(o.setAttribute("data-loadcss",!0),e.bindMediaToggle(o))}},!e.support()){e.poly();var a=t.setInterval(e.poly,500);t.addEventListener?t.addEventListener("load",function(){e.poly(),t.clearInterval(a)}):t.attachEvent&&t.attachEvent("onload",function(){e.poly(),t.clearInterval(a)})}"undefined"!=typeof exports?exports.loadCSS=loadCSS:t.loadCSS=loadCSS}("undefined"!=typeof global?global:this);
</script>

    <!-- ### Site Verification ### -->
    


    <meta name="mobile-web-app-capable" content="yes"><meta name="application-name" content="blog"><meta name="msapplication-starturl" content="https://abinzzz.github.io"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="blog">

    <!-- ### The Open Graph & Twitter Card Protocol ### -->
    <meta property="og:title" content="Daily Paper | July 30, 2025 | blog"><meta property="og:site_name" content="blog"><meta property="og:type" content="article"><meta property="og:url" content="https://abinzzz.github.io/2025/07/30/Daily-Paper-July-30-2025/"><meta property="og:locale" content="zh-CN"><meta name="description" content="Table of Content  AGENTIC REINFORCED POLICY OPTIMIZATION KIMI-K2 Flow Matching Policy Gradients Geometric-Mean Policy Optimization ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Sh - ab - blog"><meta name="keywords" content="blog"><meta property="og:image" content="https://pic1.imgdb.cn/item/68897a6d58cb8da5c8ee9424.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68897b5a58cb8da5c8ee968f.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68897ce658cb8da5c8ee9a01.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68897d5758cb8da5c8ee9b96.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68897d5758cb8da5c8ee9b96.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889809558cb8da5c8eeac04.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889816b58cb8da5c8eeaf05.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68899e4a58cb8da5c8ef4591.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68899ead58cb8da5c8ef46c8.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68899f1c58cb8da5c8ef483a.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/68899fe558cb8da5c8ef4b49.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889a03958cb8da5c8ef4c6e.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889a18f58cb8da5c8ef5778.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889a1f958cb8da5c8ef5aeb.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889a37e58cb8da5c8ef6ed6.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889a40e58cb8da5c8ef71cb.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889a63d58cb8da5c8ef8243.png"><meta property="og:image" content="https://pic1.imgdb.cn/item/6889a71058cb8da5c8ef82cb.png"><meta property="article:published_time" content="2025-07-30T01:44:39.000Z"><meta property="article:modified_time" content="2025-07-30T05:02:33.758Z"><meta property="og:updated_time" content="2025-07-30T05:02:33.758Z"><meta property="article:author" content="ab"><meta property="article:tag" content="blog"><meta name="twitter:card" content="summary">

    

    <!-- ### Canonical link ### -->
    <link rel="canonical" href="https://abinzzz.github.io/2025/07/30/Daily-Paper-July-30-2025/">

    <meta name="generator" content="Hexo 5.4.2">

    <!-- ### Analytics ### -->
    







    <!-- ### Structured Data ### -->
    



<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://abinzzz.github.io/2025/07/30/Daily-Paper-July-30-2025/",
    "@type": "BlogPosting",
    "logo": "https://abinzzz.github.io/img/blog.ico",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://abinzzz.github.io/2025/07/30/Daily-Paper-July-30-2025/"
    },
    "headline": "Daily Paper | July 30, 2025 | blog",
    
    "image": {
        "@type": "ImageObject",
        "url": "https://abinzzz.github.io/img/blog.ico"
    },
    
    "datePublished": "2025-07-30T01:44:39.000Z",
    "dateModified": "2025-07-30T05:02:33.758Z",
    "author": {
        "@type": "Person",
        "name": "ab",
        "image": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/avatar.jpg"
        },
        "description": "Welcome to my blog!"
    },
    "publisher": {
        "@type": "Organization",
        "name": "blog",
        "logo": {
            "@type": "ImageObject",
            "url": "https://abinzzz.github.io/img/blog.ico"
        }
    },
    
    "potentialAction": {
        "@type": "SearchAction",
        "target": "https://abinzzz.github.io/search?s={search_term_string}",
        "query-input": "required name=search_term_string"
    },
    
    "keywords": "blog",
    "description": "Table of Content  AGENTIC REINFORCED POLICY OPTIMIZATION KIMI-K2 Flow Matching Policy Gradients Geometric-Mean Policy Optimization ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Sh - ab - blog"
}
</script>



    <!-- ### Custom Head ### -->
    
</head>

    <body>
            

            <!-- ### Main content ### -->
            <!-- ## Header ##-->
<header>
    <h1 class="header-title text-center"><a href="/">blog</a></h1>

    <p class="text-center header-slogan">
        
            
                Welcome to my blog!
            
        
    </p>

    <nav class="navbar-section text-center">
    
        <a href="/" class="navbar-link">首页</a>
    
    
    <a href="/categories/" class="navbar-link">分类</a>
    
        <a href="/archives/" class="navbar-link">归档</a>
    
    
        <a href="/search" class="navbar-link">搜索</a>
    
    
    
    
</nav>
</header>

            
    <!-- ## Post ## -->
    <div class="post-container">
    <div id="post-card" class="card">
        
        <div class="card-item-container">
            <div class="card-inner-cell">
                <!-- # Post Header Info # -->
                <div class="card-header">
                    
    <h1 class="card-title h3 mb-2">Daily Paper | July 30, 2025</h1>




<div class="post-header-info">
    <p class="post-header-info-left text-gray">
        <img class="author-thumb lazyload" data-src="/img/avatar.jpg" src="/img/suka-lazyload.gif" alt="ab's Avatar">
        <span>2025-07-30</span>
        
            <span class="suka-devide-dot"></span>
            <a class="category-link" href="/categories/Daily-Paper/">Daily Paper</a>
        
        
        
    </p>
    <div class="post-header-info-right">
        
            <div class="dropdown dropdown-right">
<a class="dropdown-toggle" tabindex="0">分享本文</a>
<ul class="menu share-menu">
    <!-- Share Weibo -->
    

    <!-- Share Twitter -->
    

    <!-- Share Facebook -->
    

    <!-- Share Google+ -->
    

    <!-- Share LinkedIn -->
    

    <!-- Share QQ -->
    

    <!-- Share Telegram -->
    

    <!-- QRCode -->
    
    <li class="menu-item">
        <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJQAAACUCAAAAABQV18IAAAB1klEQVR42u3azZKDIBBFYd//pWe2Vkr7no4uoHPcZH4S+VKFQF84/ha8DlGiRIm6QB3FVd7g9H9yD/QeUWNQqdHPm55//3x/9ff45USNQl01+m0jVx05tSPqN1FVA7RxUaLoRN0dXEXNR91NlFcLtepncp/HqwRRy6OqBf2br69VM6KWRsXQITRGC4dXUxdRS6PIxJsG0OoBSQWFqHkoUnB+0+BdaHZXjIiag0qBaQpeSUhLF32iZqCqgrKaVDvFQgr5Rc1C0Q+Qh6IaLNEgLGokqpqQ7zp4J+goiwxRo1B00zFtHlUDbSxoRY1APdmYJp03PRyi5qFoIfrmJtJtQSJqFIp0xipUTcFZFbaJmofq3DyFYGkAxamLqK1RNAR7emiiteMgamtUd5OxmpTpgS+8CSlqSxQ5jNUtGsgmE1rkidoSRQbMtOijC8J2ECtqDKqzaZ1CNBL2ipqHomFEOnRDDga2wn1R26HSVYVoqYhNE7GomSh6ICsFGSQoa58KErUtinRwOsHSzaNWECtqS1QnqOgEaa+vEkSNQKVCkh7QaU3IokajSCfuHAzDE7KorVFV0JqKU7KJ+eoiT9TyKDJpkoPOd2EIDWtFzUCtdIkSJUrU6foH1W7wwDHHockAAAAASUVORK5CYII=" alt="QRCode">
    </li>
    

</ul>
</div>
        
    </div>
</div>
                </div>
                <div class="card-body">
                    
                        
                        
                            <div id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#table-of-content"><span class="post-toc-number">1.</span> <span class="post-toc-text"> Table of Content</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#agentic-reinforced-policy-optimization"><span class="post-toc-number">2.</span> <span class="post-toc-text"> AGENTIC REINFORCED POLICY OPTIMIZATION</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#kimi-k2"><span class="post-toc-number">3.</span> <span class="post-toc-text"> KIMI-K2</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#flow-matching-policy-gradients"><span class="post-toc-number">4.</span> <span class="post-toc-text"> Flow Matching Policy Gradients</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#geometric-mean-policy-optimization"><span class="post-toc-number">5.</span> <span class="post-toc-text"> Geometric-Mean Policy Optimization</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#arc-hunyuan-video-7b-structured-video-comprehension-of-real-world-shorts"><span class="post-toc-number">6.</span> <span class="post-toc-text"> ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#driveagent-r1-advancing-vlm-based-autonomous-driving-with-hybrid-thinking-and-active-perception"><span class="post-toc-number">7.</span> <span class="post-toc-text"> DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#security-challenges-in-ai-agent-deployment-insights-from-a-large-scale-public-competition"><span class="post-toc-number">8.</span> <span class="post-toc-text"> Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#rep-mtl-unleashing-the-power-of-representation-level-task-saliency-for-multi-task-learning"><span class="post-toc-number">9.</span> <span class="post-toc-text"> Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#self-guided-masked-autoencoder"><span class="post-toc-number">10.</span> <span class="post-toc-text"> Self-Guided Masked Autoencoder</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#transprune-token-transition-pruning-for-efficient-large-vision-language-model"><span class="post-toc-number">11.</span> <span class="post-toc-text"> TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model</span></a></li></ol></div>
                        
                    
                    <article id="post-content">
                        <!-- omit in toc -->
<h2 id="table-of-content"><a class="markdownIt-Anchor" href="#table-of-content"></a> Table of Content</h2>
<ul>
<li><a href="#agentic-reinforced-policy-optimization">AGENTIC REINFORCED POLICY OPTIMIZATION</a></li>
<li><a href="#kimi-k2">KIMI-K2</a></li>
<li><a href="#flow-matching-policy-gradients">Flow Matching Policy Gradients</a></li>
<li><a href="#geometric-mean-policy-optimization">Geometric-Mean Policy Optimization</a></li>
<li><a href="#arc-hunyuan-video-7b-structured-video-comprehension-of-real-world-shorts">ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts</a></li>
<li><a href="#driveagent-r1-advancing-vlm-based-autonomous-driving-with-hybrid-thinking-and-active-perception">DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception</a></li>
<li><a href="#security-challenges-in-ai-agent-deployment-insights-from-a-large-scale-public-competition">Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition</a></li>
<li><a href="#rep-mtl-unleashing-the-power-of-representation-level-task-saliency-for-multi-task-learning">Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</a></li>
<li><a href="#self-guided-masked-autoencoder">Self-Guided Masked Autoencoder</a></li>
<li><a href="#transprune-token-transition-pruning-for-efficient-large-vision-language-model">TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model</a></li>
</ul>
<h2 id="agentic-reinforced-policy-optimization"><a class="markdownIt-Anchor" href="#agentic-reinforced-policy-optimization"></a> AGENTIC REINFORCED POLICY OPTIMIZATION</h2>
<p><img src="https://pic1.imgdb.cn/item/68897a6d58cb8da5c8ee9424.png" alt="" /></p>
<p>The paper “Agentic Reinforced Policy Optimization (ARPO)” introduces a novel reinforcement learning (RL) algorithm designed to enhance the performance and efficiency of Large Language Models (LLMs) in multi-turn, tool-augmented reasoning tasks. Existing RL methods for LLMs often fall short in balancing intrinsic long-horizon reasoning with proficiency in multi-turn tool interactions, primarily due to their trajectory-level sampling approaches.</p>
<p>ARPO addresses this gap by recognizing that LLMs exhibit high token entropy (i.e., uncertainty) immediately after interacting with external tools. Leveraging this insight, ARPO incorporates an entropy-based adaptive rollout mechanism that dynamically balances global trajectory sampling with step-level sampling. This promotes exploration at points of high uncertainty following tool usage. Furthermore, an advantage attribution estimation mechanism is integrated, allowing LLMs to internalize advantage differences in stepwise tool-use interactions.</p>
<p>Experimental results across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO’s significant superiority over traditional trajectory-level RL algorithms. Crucially, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, presenting a scalable and efficient solution for aligning LLM-based agents with dynamic real-time environments.</p>
<p><img src="https://pic1.imgdb.cn/item/68897b5a58cb8da5c8ee968f.png" alt="" /></p>
<h2 id="kimi-k2"><a class="markdownIt-Anchor" href="#kimi-k2"></a> KIMI-K2</h2>
<p>The source introduces <strong>Kimi K2</strong>, a large language model designed for <strong>agentic intelligence</strong>, emphasizing its ability to autonomously learn and interact. The paper details the <strong>MuonClip optimizer</strong> which ensures stable pre-training on a massive dataset of 15.5 trillion tokens, highlighting its efficiency and stability. Post-training involves <strong>large-scale agentic data synthesis</strong> for tool use and a <strong>reinforcement learning framework</strong> utilizing both verifiable rewards and self-critique. The document also presents <strong>extensive evaluation results</strong> showcasing Kimi K2’s state-of-the-art performance in areas like coding, mathematics, reasoning, and tool use, often surpassing other open-source and proprietary models. Finally, it outlines the model’s <strong>architecture, training infrastructure, and safety evaluations</strong>, noting its limitations and future research directions.</p>
<p><img src="https://pic1.imgdb.cn/item/68897ce658cb8da5c8ee9a01.png" alt="" /></p>
<h2 id="flow-matching-policy-gradients"><a class="markdownIt-Anchor" href="#flow-matching-policy-gradients"></a> Flow Matching Policy Gradients</h2>
<p><img src="https://pic1.imgdb.cn/item/68897d5758cb8da5c8ee9b96.png" alt="" /></p>
<p>Flow Policy Optimization (FPO) is a novel, on-policy reinforcement learning (RL) algorithm designed to train flow-based generative models, including diffusion models, within the policy gradient framework. FPO addresses key limitations of prior approaches by reformulating policy optimization as maximizing an advantage-weighted ratio derived from the conditional flow matching (CFM) loss. This method sidesteps the need for computationally expensive exact likelihood calculations, a common hurdle for flow-based models in RL. FPO is sampler-agnostic, meaning it is compatible with various diffusion or flow integration methods at both training and inference times, unlike previous diffusion-based RL techniques that bind training to specific sampling procedures. Empirical validation across diverse continuous control tasks, including GridWorld, MuJoCo Playground, and high-dimensional humanoid control, demonstrates that FPO can effectively train diffusion-style policies from scratch. Notably, FPO-trained policies can capture multimodal action distributions and achieve superior performance compared to traditional Gaussian policies, especially in under-conditioned scenarios.</p>
<hr />
<p>On-policy methods learn about the same policy that is used to generate the data. Think of it as learning by doing and only from your own experiences generated by your current way of doing things. Off-policy methods, conversely, learn about a different policy than the one generating the data. This allows them to learn from past experiences (even from an older or different behavior policy) and potentially from observing others.</p>
<h2 id="geometric-mean-policy-optimization"><a class="markdownIt-Anchor" href="#geometric-mean-policy-optimization"></a> Geometric-Mean Policy Optimization</h2>
<p><img src="https://pic1.imgdb.cn/item/68897d5758cb8da5c8ee9b96.png" alt="" /></p>
<p>This document introduces <strong>Geometric-Mean Policy Optimization (GMPO)</strong>, a new approach designed to enhance the <strong>stability and performance of large language models (LLMs)</strong> during reinforcement learning, particularly for reasoning tasks. It addresses issues found in previous methods like <strong>Group Relative Policy Optimization (GRPO)</strong>, which often suffer from <strong>unstable policy updates due to sensitivity to outlier rewards</strong>. GMPO achieves this by <strong>optimizing the geometric mean of token-level rewards</strong>, which inherently handles outliers more effectively, leading to <strong>more stable training and improved exploration capabilities</strong>. The paper provides <strong>theoretical justifications and experimental results</strong>, showcasing GMPO’s <strong>superior performance on various mathematical and multimodal reasoning benchmarks</strong> compared to existing methods.</p>
<p>Geometric-Mean Policy Optimization (GMPO) enhances large language model fine-tuning by replacing the arithmetic mean with a geometric mean in the policy optimization objective, which stabilizes training and improves exploration. GMPO leads to up to 4.1% higher Pass@1 accuracy on mathematical benchmarks and 1.4% on multimodal tasks compared to its predecessor, GRPO.</p>
<p><img src="https://pic1.imgdb.cn/item/6889809558cb8da5c8eeac04.png" alt="" /></p>
<h2 id="arc-hunyuan-video-7b-structured-video-comprehension-of-real-world-shorts"><a class="markdownIt-Anchor" href="#arc-hunyuan-video-7b-structured-video-comprehension-of-real-world-shorts"></a> ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts</h2>
<p><img src="https://pic1.imgdb.cn/item/6889816b58cb8da5c8eeaf05.png" alt="" /></p>
<p>The document introduces <strong>ARC-Hunyuan-Video-7B</strong>, a novel <strong>multimodal model</strong> designed for <strong>structured comprehension of real-world short videos</strong>, particularly those from platforms like WeChat Channel and TikTok. Unlike previous models, it processes <strong>visual, audio, and textual signals</strong> end-to-end to address challenges posed by the fast-paced, information-dense nature of user-generated content. The model excels at <strong>multi-granularity timestamped video captioning, summarization, open-ended question answering, temporal video grounding, and video reasoning</strong>. Its development involved a comprehensive training regimen, including <strong>pre-training, instruction fine-tuning, cold start, and reinforcement learning</strong>, leveraging a high-quality, automatically annotated dataset. The paper presents <strong>qualitative and quantitative evaluations</strong> demonstrating the model’s superior performance in understanding the <strong>chronological flow, thematic nuances, and creative intent</strong> of videos, with <strong>real-world deployment</strong> showing improved user engagement.</p>
<p><img src="https://pic1.imgdb.cn/item/68899e4a58cb8da5c8ef4591.png" alt="" /></p>
<h2 id="driveagent-r1-advancing-vlm-based-autonomous-driving-with-hybrid-thinking-and-active-perception"><a class="markdownIt-Anchor" href="#driveagent-r1-advancing-vlm-based-autonomous-driving-with-hybrid-thinking-and-active-perception"></a> DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception</h2>
<p><img src="https://pic1.imgdb.cn/item/68899ead58cb8da5c8ef46c8.png" alt="" /></p>
<p>The research introduces <strong>DriveAgent-R1</strong>, an advanced autonomous driving agent designed to address the limitations of current <strong>Vision-Language Models (VLMs)</strong> in complex driving scenarios. It features a <strong>Hybrid-Thinking framework</strong> that adaptively switches between efficient text-based reasoning and in-depth, tool-based reasoning for enhanced decision-making. The agent also incorporates an <strong>Active Perception mechanism</strong> with a <strong>Vision Toolkit</strong> to proactively gather crucial visual information and resolve uncertainties, mirroring human driver behavior. A novel <strong>three-stage progressive reinforcement learning strategy</strong> trains the agent to master these capabilities, enabling it to achieve state-of-the-art performance by grounding its decisions in actively perceived visual evidence. This approach aims to create safer and more intelligent autonomous systems by balancing efficiency with reliability.</p>
<p><img src="https://pic1.imgdb.cn/item/68899f1c58cb8da5c8ef483a.png" alt="" /></p>
<h2 id="security-challenges-in-ai-agent-deployment-insights-from-a-large-scale-public-competition"><a class="markdownIt-Anchor" href="#security-challenges-in-ai-agent-deployment-insights-from-a-large-scale-public-competition"></a> Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition</h2>
<p><img src="https://pic1.imgdb.cn/item/68899fe558cb8da5c8ef4b49.png" alt="" /></p>
<p>This academic paper details a <strong>large-scale public red-teaming competition</strong> designed to evaluate the <strong>security vulnerabilities of AI agents</strong> powered by Large Language Models (LLMs). The study involved 22 frontier AI agents across 44 realistic deployment scenarios, where participants submitted <strong>1.8 million prompt-injection attacks</strong>, with over 60,000 successfully causing policy violations such as unauthorized data access or illicit financial actions. The researchers developed the <strong>Agent Red Teaming (ART) benchmark</strong> from these attacks, demonstrating that nearly all agents exhibit policy violations for most behaviors within 10–100 queries due to <strong>high attack transferability and universality</strong> across models and tasks. A crucial finding is the <strong>lack of correlation between an agent’s robustness and its model size, capability, or inference-time compute</strong>, emphasizing the urgent need for new defense mechanisms. The paper concludes by releasing the ART benchmark to support more rigorous security assessments and drive progress toward safer AI agent deployment.</p>
<p><img src="https://pic1.imgdb.cn/item/6889a03958cb8da5c8ef4c6e.png" alt="" /></p>
<h2 id="rep-mtl-unleashing-the-power-of-representation-level-task-saliency-for-multi-task-learning"><a class="markdownIt-Anchor" href="#rep-mtl-unleashing-the-power-of-representation-level-task-saliency-for-multi-task-learning"></a> Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</h2>
<p><img src="https://pic1.imgdb.cn/item/6889a18f58cb8da5c8ef5778.png" alt="" /></p>
<p>This paper introduces <strong>Rep-MTL</strong>, a novel approach to multi-task learning (MTL) that aims to enhance performance by <strong>addressing negative transfer and promoting inter-task complementarity</strong> directly within the <strong>shared representation space</strong>. Unlike conventional multi-task optimization (MTO) techniques that primarily focus on optimizer-centric loss scaling and gradient manipulation, Rep-MTL utilizes <strong>representation-level task saliency</strong>. This method, through its <strong>Task-specific Saliency Regulation (TSR)</strong> and <strong>Cross-task Saliency Alignment (CSA)</strong> modules, <strong>preserves individual task learning patterns</strong> and <strong>facilitates beneficial information sharing</strong> without altering optimizers or network architectures. Empirical results across various benchmarks demonstrate Rep-MTL’s <strong>consistent performance gains and efficiency</strong>, even when paired with basic weighting policies.</p>
<p><img src="https://pic1.imgdb.cn/item/6889a1f958cb8da5c8ef5aeb.png" alt="" /></p>
<h2 id="self-guided-masked-autoencoder"><a class="markdownIt-Anchor" href="#self-guided-masked-autoencoder"></a> Self-Guided Masked Autoencoder</h2>
<p><img src="https://pic1.imgdb.cn/item/6889a37e58cb8da5c8ef6ed6.png" alt="" /></p>
<p>This collection of sources centers on an <strong>in-depth analysis and proposed improvement</strong> for <strong>Masked Autoencoders (MAE)</strong>, a self-supervised learning approach used in computer vision. The authors <strong>uncover that MAE inherently learns pattern-based patch clustering</strong> from early stages of pre-training. Building on this understanding, they introduce a <strong>“self-guided masked autoencoder”</strong> that generates <strong>informed masks internally</strong> by leveraging its progress in patch clustering, unlike the original MAE’s random masking. This novel approach <strong>significantly boosts MAE’s learning process</strong> without requiring external models or additional information, a benefit verified through comprehensive experiments on various downstream tasks like image classification, object detection, and semantic segmentation.</p>
<p><img src="https://pic1.imgdb.cn/item/6889a40e58cb8da5c8ef71cb.png" alt="" /></p>
<h2 id="transprune-token-transition-pruning-for-efficient-large-vision-language-model"><a class="markdownIt-Anchor" href="#transprune-token-transition-pruning-for-efficient-large-vision-language-model"></a> TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model</h2>
<p><img src="https://pic1.imgdb.cn/item/6889a63d58cb8da5c8ef8243.png" alt="" /></p>
<p>TransPrune introduces a method for Large Vision-Language Models that prunes less important visual tokens based on their representation transitions within transformer layers. This approach reduces inference TFLOPs of LLaVA-v1.5-7B by nearly 60% without performance degradation, and for LLaVA-Next-7B by 60% with minimal accuracy loss, addressing limitations of prior attention-based pruning.</p>
<p><img src="https://pic1.imgdb.cn/item/6889a71058cb8da5c8ef82cb.png" alt="" /></p>

                    </article>
                    


    <blockquote id="date-expire-notification" class="post-expired-notify">本文最后更新于 <span id="date-expire-num"></span> 天前，文中所描述的信息可能已发生改变</blockquote>
    <script>
    (function() {
        var dateUpdate = Date.parse("2025-07-30");
        var nowDate = new Date();
        var a = nowDate.getTime();
        var b = a - dateUpdate;
        var daysUpdateExpire = Math.floor(b/(24*3600*1000));
        if (daysUpdateExpire >= 120) {
            document.getElementById('date-expire-num').innerHTML = daysUpdateExpire;
        } else {
            document.getElementById('date-expire-notification').style.display = 'none';
        }
    })();
    </script>


<p class="post-footer-info mb-0 pt-0">本文发表于&nbsp;<time datetime="2025-07-30T01:44:39.000Z" itemprop="datePublished">2025-07-30</time>

</p>
<p class="post-footer-info mb-0 pt-2">

<span class="post-categories-list mt-2">

<a class="post-categories-list-item" href='/categories/Daily-Paper/'>Daily Paper</a>

</span>




</p>

                </div>
                <div class="post-nav px-2 bg-gray">
<ul class="pagination">
    <!-- Prev Nav -->
    
        <li class="page-item page-prev">
            <a href="/2025/07/31/Daily-Paper-July-31-2025/" rel="prev">
                <div class="page-item-title"><i class="icon icon-back" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Daily Paper | July 31, 2025</div>
            </a>
        </li>
    

    <!-- Next Nav -->
    
        <li class="page-item page-next">
            <a href="/2025/07/29/Daily-Paper-July-29-2025/" rel="next">
                <div class="page-item-title"><i class="icon icon-forward" aria-hidden="true"></i></div>
                <div class="page-item-subtitle">Daily Paper | July 29, 2025</div>
            </a>
        </li>
    
</ul>
</div>

                
                    <!-- # Comment # -->
                    
                        <div class="card-footer post-comment">
                            <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
        this.page.url = 'https://abinzzz.github.io/2025/07/30/Daily-Paper-July-30-2025/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://abinzzz.github.io/2025/07/30/Daily-Paper-July-30-2025/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>
<script id="disqus-thread-script">
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//robin02.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

                        </div>
                    
                
            </div>
        </div>
    </div>
</div>

            <!-- ### Footer ### -->
            <footer class="text-center">
    <!-- footer copyright -->
    
        <p class="footer-copyright mb-0">Copyright&nbsp;©&nbsp;<span id="copyright-year"></span>
            <a class="footer-copyright-a" href="https://abinzzz.github.io">blog</a>
        </p>

    <!-- footer custom text -->
    <p class="footer-text mb-0">
    
    </p>
    <!-- footer develop info -->
    <p class="footer-develop mb-0">
        
    <!-- Busuanzi User Views -->
    <span id="busuanzi_container_site_uv" hidden>
        <span></span>
        <span id="busuanzi_value_site_uv"></span>
        <span>Viewers</span>
        
            <span>|</span>
        
    </span>




        
        Powered by&nbsp;<!--
         --><a href="https://hexo.io" target="_blank" class="footer-develop-a" rel="external nofollow noopener noreferrer">Hexo</a><span class="footer-develop-divider"></span>Theme&nbsp;-&nbsp;<!--
         --><a href="https://github.com/SukkaW/hexo-theme-suka" target="_blank" class="footer-develop-a" rel="external noopener">Suka</a>
    </p>
</footer>


        <!-- ### Import File ### -->
        <!-- ### Footer JS Import ### -->

<script>

    
window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 50
};

(function() {
    var copyrightNow = new Date().getFullYear();
    var copyrightContent = document.getElementById('copyright-year');
    var copyrightSince = 2023;
    if (copyrightSince === copyrightNow) {
        copyrightContent.textContent = copyrightNow;
    } else {
        copyrightContent.textContent = copyrightSince + ' - ' + copyrightNow;
    }
})();
console.log('\n %c Suka Theme (hexo-theme-suka) | © SukkaW | Verision 1.3.3 %c https://github.com/SukkaW/hexo-theme-suka \n', 'color: #fff; background: #444; padding:5px 0;', 'background: #bbb; padding:5px 0;');

(function() {
    'use strict';
    
    // 等待 DOM 加载完成
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', initTocCollapse);
    } else {
        initTocCollapse();
    }
    
    function initTocCollapse() {
        const tocContainer = document.getElementById('post-toc');
        if (!tocContainer) {
            return;
        }
        
        // 找到所有有子目录的 level-1 和 level-2 项
        const tocItems = tocContainer.querySelectorAll('.post-toc-item.post-toc-level-1, .post-toc-item.post-toc-level-2');
        
        tocItems.forEach(function(item) {
            // 检查是否有子目录（包含 post-toc-child 的 ol）
            const hasChildren = item.querySelector('ol.post-toc-child') !== null;
            
            if (hasChildren) {
                // 添加标记类，用于 CSS 显示图标
                item.classList.add('toc-has-children');
                
                const link = item.querySelector('.post-toc-link');
                if (link) {
                    // 阻止默认的锚点跳转，改为展开/折叠
                    link.addEventListener('click', function(e) {
                        // 如果用户按住 Ctrl/Cmd 或中键点击，则跳转
                        if (e.ctrlKey || e.metaKey || e.button === 1) {
                            return; // 允许默认行为（跳转）
                        }
                        
                        e.preventDefault();
                        e.stopPropagation();
                        
                        // 切换展开/折叠状态
                        item.classList.toggle('toc-expanded');
                    });
                    
                    // 允许通过双击链接文本跳转
                    link.addEventListener('dblclick', function(e) {
                        const href = link.getAttribute('href');
                        if (href) {
                            window.location.hash = href;
                        }
                    });
                    
                    // 允许通过右键菜单跳转
                    link.addEventListener('contextmenu', function(e) {
                        // 允许默认的右键菜单
                        return true;
                    });
                }
            }
        });
    }
})();
        

</script>

<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@8.9.0" async></script>
    <script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@2.3/bsz.pure.mini.js" async></script>


<!-- Offset -->




<!-- Comment -->

    
        <script id="dsq-count-scr" src="https://robin02.disqus.com/count.js" async></script>

    


<!-- ### Custom Footer ### -->

    </body>

</html>